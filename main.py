"""
ç”¨æˆ·è¡Œä¸ºåˆ†ææ™ºèƒ½ä½“å¹³å°ä¸»å…¥å£
åŸºäºCrewAIå’ŒStreamlitçš„å¤šæ™ºèƒ½ä½“åä½œåˆ†æç³»ç»Ÿ
"""

import streamlit as st
import sys
import os
import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import Optional, Dict, Any
import time
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# æ—¶é—´ç²’åº¦å’Œå‘¨æœŸçš„ä¸­è‹±æ–‡æ˜ å°„
TIME_GRANULARITY_MAPPING = {
    "æ—¥": "daily",
    "å‘¨": "weekly",
    "æœˆ": "monthly",
    "day": "daily",
    "week": "weekly",
    "month": "monthly"
}

COHORT_PERIOD_MAPPING = {
    "æ—¥": "daily",
    "å‘¨": "weekly",
    "æœˆ": "monthly",
    "æ—¥ç•™å­˜": "daily",
    "å‘¨ç•™å­˜": "weekly",
    "æœˆç•™å­˜": "monthly"
}

def translate_time_granularity(chinese_term: str) -> str:
    """å°†ä¸­æ–‡æ—¶é—´ç²’åº¦è½¬æ¢ä¸ºè‹±æ–‡"""
    return TIME_GRANULARITY_MAPPING.get(chinese_term, chinese_term)

def translate_cohort_period(chinese_term: str) -> str:
    """å°†ä¸­æ–‡é˜Ÿåˆ—å‘¨æœŸè½¬æ¢ä¸ºè‹±æ–‡"""
    return COHORT_PERIOD_MAPPING.get(chinese_term, chinese_term)

from config.settings import settings, validate_config
from utils.logger import setup_logger
from config.llm_provider_manager import get_provider_manager
from tools.ga4_data_parser import GA4DataParser
from tools.data_validator import DataValidator
from tools.data_storage_manager import DataStorageManager
from engines.event_analysis_engine import EventAnalysisEngine
from engines.retention_analysis_engine import RetentionAnalysisEngine
from engines.conversion_analysis_engine import ConversionAnalysisEngine
from engines.user_segmentation_engine import UserSegmentationEngine
from engines.path_analysis_engine import PathAnalysisEngine
from visualization.chart_generator import ChartGenerator
from visualization.advanced_visualizer import AdvancedVisualizer
from utils.report_exporter import ReportExporter
from utils.config_manager import config_manager
from system.integration_manager import IntegrationManager, WorkflowConfig

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title=settings.app_title,
    page_icon=settings.app_icon,
    layout="wide",
    initial_sidebar_state="expanded"
)


def main():
    """ä¸»åº”ç”¨å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger()
    
    # éªŒè¯é…ç½®
    if not validate_config():
        st.error("âš ï¸ ç³»ç»Ÿé…ç½®ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶ä¸­çš„APIé…ç½®")
        st.stop()
    
    # æ£€æŸ¥æä¾›å•†å¥åº·çŠ¶æ€
    check_provider_health()
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    initialize_session_state()
    
    # åº”ç”¨æ ‡é¢˜
    st.title(f"{settings.app_icon} {settings.app_title}")
    st.markdown("---")
    
    # ä¾§è¾¹æ å¯¼èˆª
    with st.sidebar:
        st.header("ğŸš€ åŠŸèƒ½å¯¼èˆª")
        
        # æ˜¾ç¤ºæ•°æ®çŠ¶æ€
        show_data_status()
        
        page = st.selectbox(
            "é€‰æ‹©åŠŸèƒ½æ¨¡å—",
            [
                "ğŸ“ æ•°æ®ä¸Šä¼ ",
                "ğŸš€ æ™ºèƒ½åˆ†æ",
                "ğŸ“Š äº‹ä»¶åˆ†æ", 
                "ğŸ“ˆ ç•™å­˜åˆ†æ",
                "ğŸ”„ è½¬åŒ–åˆ†æ",
                "ğŸ‘¥ ç”¨æˆ·åˆ†ç¾¤",
                "ğŸ›¤ï¸ è·¯å¾„åˆ†æ",
                "ğŸ“‹ ç»¼åˆæŠ¥å‘Š",
                "âš™ï¸ ç³»ç»Ÿè®¾ç½®"
            ]
        )
    
    # ä¸»å†…å®¹åŒºåŸŸ
    if page == "ğŸ“ æ•°æ®ä¸Šä¼ ":
        show_data_upload_page()
        
    elif page == "ğŸš€ æ™ºèƒ½åˆ†æ":
        if not st.session_state.data_loaded:
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ GA4æ•°æ®æ–‡ä»¶")
        else:
            show_intelligent_analysis_page()
        
    elif page == "ğŸ“Š äº‹ä»¶åˆ†æ":
        if not st.session_state.data_loaded:
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ GA4æ•°æ®æ–‡ä»¶")
        else:
            show_event_analysis_page()
        
    elif page == "ğŸ“ˆ ç•™å­˜åˆ†æ":
        if not st.session_state.data_loaded:
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ GA4æ•°æ®æ–‡ä»¶")
        else:
            show_retention_analysis_page()
        
    elif page == "ğŸ”„ è½¬åŒ–åˆ†æ":
        if not st.session_state.data_loaded:
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ GA4æ•°æ®æ–‡ä»¶")
        else:
            show_conversion_analysis_page()
        
    elif page == "ğŸ‘¥ ç”¨æˆ·åˆ†ç¾¤":
        if not st.session_state.data_loaded:
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ GA4æ•°æ®æ–‡ä»¶")
        else:
            show_user_segmentation_page()
        
    elif page == "ğŸ›¤ï¸ è·¯å¾„åˆ†æ":
        if not st.session_state.data_loaded:
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ GA4æ•°æ®æ–‡ä»¶")
        else:
            show_path_analysis_page()
        
    elif page == "ğŸ“‹ ç»¼åˆæŠ¥å‘Š":
        if not st.session_state.data_loaded:
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ GA4æ•°æ®æ–‡ä»¶")
        else:
            show_comprehensive_report_page()
        
    elif page == "âš™ï¸ ç³»ç»Ÿè®¾ç½®":
        st.header("ç³»ç»Ÿé…ç½®")
        show_system_settings()
    
    # é¡µè„š
    st.markdown("---")
    st.markdown(
        "ğŸ’¡ **æç¤º**: è¯·å…ˆä¸Šä¼ GA4 NDJSONæ•°æ®æ–‡ä»¶ï¼Œç„¶åé€‰æ‹©ç›¸åº”çš„åˆ†æåŠŸèƒ½"
    )


def check_provider_health():
    """æ£€æŸ¥æä¾›å•†å¥åº·çŠ¶æ€"""
    if 'provider_health_checked' not in st.session_state:
        try:
            # å¿«é€Ÿæµ‹è¯• LLM æ˜¯å¦å¯ç”¨ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„å¥åº·æ£€æŸ¥
            with st.spinner("ğŸ”§ æ£€æŸ¥LLMæä¾›å•†çŠ¶æ€..."):
                from config.crew_config import get_llm
                llm = get_llm()
                # ç®€å•æµ‹è¯•è°ƒç”¨
                response = llm.invoke("Test")
                
                st.session_state.provider_health_checked = True
                st.session_state.healthy_providers = ["volcano"]  # å‡è®¾ volcano å¯ç”¨
                st.success("âœ… LLM æä¾›å•†æ£€æŸ¥å®Œæˆ")
                
        except Exception as e:
            # ä¸é˜»æ­¢åº”ç”¨å¯åŠ¨ï¼Œåªæ˜¾ç¤ºè­¦å‘Š
            st.warning(f"âš ï¸ LLM æä¾›å•†æ£€æŸ¥å¤±è´¥: {e}")
            st.info("ğŸ’¡ åº”ç”¨ä»å¯ä½¿ç”¨ï¼Œä½†æ™ºèƒ½åˆ†æåŠŸèƒ½å¯èƒ½å—é™")
            st.info("è¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ API é…ç½®")
            
            st.session_state.provider_health_checked = True
            st.session_state.healthy_providers = []


def initialize_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    if 'data_loaded' not in st.session_state:
        st.session_state.data_loaded = False
    if 'raw_data' not in st.session_state:
        st.session_state.raw_data = None
    if 'processed_data' not in st.session_state:
        st.session_state.processed_data = None
    if 'data_summary' not in st.session_state:
        st.session_state.data_summary = None
    if 'validation_report' not in st.session_state:
        st.session_state.validation_report = None
    if 'storage_manager' not in st.session_state:
        st.session_state.storage_manager = DataStorageManager()
    if 'integration_manager' not in st.session_state:
        # åˆå§‹åŒ–ç³»ç»Ÿé›†æˆç®¡ç†å™¨
        config = WorkflowConfig(
            enable_parallel_processing=True,
            max_workers=4,
            memory_limit_gb=8.0,
            timeout_minutes=30,
            enable_caching=True,
            cache_ttl_hours=24,
            enable_monitoring=True,
            auto_cleanup=True
        )
        st.session_state.integration_manager = IntegrationManager(config)
        # ç¡®ä¿é›†æˆç®¡ç†å™¨ä½¿ç”¨ç›¸åŒçš„å­˜å‚¨ç®¡ç†å™¨å¹¶é‡æ–°åˆå§‹åŒ–å¼•æ“
        st.session_state.integration_manager.storage_manager = st.session_state.storage_manager
        
        # é‡æ–°åˆå§‹åŒ–åˆ†æå¼•æ“ä»¥ä½¿ç”¨æ­£ç¡®çš„å­˜å‚¨ç®¡ç†å™¨
        from engines.event_analysis_engine import EventAnalysisEngine
        from engines.retention_analysis_engine import RetentionAnalysisEngine
        from engines.conversion_analysis_engine import ConversionAnalysisEngine
        
        st.session_state.integration_manager.event_engine = EventAnalysisEngine(st.session_state.storage_manager)
        st.session_state.integration_manager.retention_engine = RetentionAnalysisEngine(st.session_state.storage_manager)
        st.session_state.integration_manager.conversion_engine = ConversionAnalysisEngine(st.session_state.storage_manager)
    if 'workflow_results' not in st.session_state:
        st.session_state.workflow_results = None


def show_data_status():
    """æ˜¾ç¤ºæ•°æ®çŠ¶æ€"""
    st.markdown("### ğŸ“Š æ•°æ®çŠ¶æ€")
    
    if st.session_state.data_loaded:
        st.success("âœ… æ•°æ®å·²åŠ è½½")
        if st.session_state.data_summary:
            summary = st.session_state.data_summary
            st.write(f"ğŸ“… **æ—¶é—´èŒƒå›´**: {summary.get('date_range', {}).get('start', 'N/A')} - {summary.get('date_range', {}).get('end', 'N/A')}")
            st.write(f"ğŸ‘¥ **ç”¨æˆ·æ•°**: {summary.get('unique_users', 0):,}")
            st.write(f"ğŸ“ **äº‹ä»¶æ•°**: {summary.get('total_events', 0):,}")
            st.write(f"ğŸ¯ **äº‹ä»¶ç±»å‹**: {len(summary.get('event_types', {}))}")
        
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ•°æ®", type="secondary"):
            clear_session_data()
            st.rerun()
    else:
        st.info("â³ æš‚æ— æ•°æ®")


def clear_session_data():
    """æ¸…é™¤ä¼šè¯æ•°æ®"""
    st.session_state.data_loaded = False
    st.session_state.raw_data = None
    st.session_state.processed_data = None
    st.session_state.data_summary = None
    st.session_state.validation_report = None
    st.session_state.storage_manager = DataStorageManager()


def get_mime_type(format_type: str) -> str:
    """è·å–MIMEç±»å‹"""
    mime_types = {
        'json': 'application/json',
        'pdf': 'application/pdf',
        'excel': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    }
    return mime_types.get(format_type, 'application/octet-stream')


def show_data_upload_page():
    """æ˜¾ç¤ºæ•°æ®ä¸Šä¼ é¡µé¢"""
    st.header("ğŸ“ GA4æ•°æ®ä¸Šä¼ ä¸å¤„ç†")
    
    # æ–‡ä»¶ä¸Šä¼ è¯´æ˜
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜", expanded=False):
        st.markdown("""
        ### æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        - **GA4 NDJSONæ ¼å¼**: ä»Google Analytics 4å¯¼å‡ºçš„NDJSONæ–‡ä»¶
        - **æ–‡ä»¶å¤§å°é™åˆ¶**: æœ€å¤§ {max_size}MB
        
        ### æ•°æ®è¦æ±‚
        - æ–‡ä»¶å¿…é¡»åŒ…å«å®Œæ•´çš„äº‹ä»¶æ•°æ®ç»“æ„
        - æ”¯æŒçš„äº‹ä»¶ç±»å‹: page_view, sign_up, login, purchase, searchç­‰
        - å¿…éœ€å­—æ®µ: event_date, event_timestamp, event_name, user_pseudo_idç­‰
        
        ### å¤„ç†æµç¨‹
        1. ä¸Šä¼ æ–‡ä»¶å¹¶éªŒè¯æ ¼å¼
        2. è§£æäº‹ä»¶æ•°æ®å’Œç”¨æˆ·ä¿¡æ¯
        3. æ•°æ®è´¨é‡æ£€æŸ¥å’Œæ¸…æ´—
        4. ç”Ÿæˆæ•°æ®æ‘˜è¦æŠ¥å‘Š
        """.format(max_size=settings.max_file_size_mb))
    
    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    st.subheader("ğŸ“¤ æ–‡ä»¶ä¸Šä¼ ")
    
    uploaded_file = st.file_uploader(
        "é€‰æ‹©GA4 NDJSONæ–‡ä»¶",
        type=['ndjson', 'json', 'jsonl'],
        help=f"æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .ndjson, .json, .jsonl (æœ€å¤§{settings.max_file_size_mb}MB)"
    )
    
    if uploaded_file is not None:
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size_mb = uploaded_file.size / (1024 * 1024)
        
        if file_size_mb > settings.max_file_size_mb:
            st.error(f"âŒ æ–‡ä»¶å¤§å° ({file_size_mb:.1f}MB) è¶…è¿‡é™åˆ¶ ({settings.max_file_size_mb}MB)")
            return
        
        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        st.info(f"ğŸ“„ **æ–‡ä»¶å**: {uploaded_file.name}")
        st.info(f"ğŸ“ **æ–‡ä»¶å¤§å°**: {file_size_mb:.2f}MB")
        
        # å¤„ç†æŒ‰é’®
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            if st.button("ğŸš€ å¼€å§‹å¤„ç†", type="primary"):
                process_uploaded_file(uploaded_file)
        
        with col2:
            if st.button("ğŸ” é¢„è§ˆæ–‡ä»¶"):
                preview_file(uploaded_file)
    
    # æ˜¾ç¤ºå¤„ç†ç»“æœ
    if st.session_state.data_loaded:
        show_processing_results()


def preview_file(uploaded_file):
    """é¢„è§ˆä¸Šä¼ çš„æ–‡ä»¶"""
    st.subheader("ğŸ” æ–‡ä»¶é¢„è§ˆ")
    
    try:
        # è¯»å–å‰å‡ è¡Œè¿›è¡Œé¢„è§ˆ
        content = uploaded_file.read().decode('utf-8')
        lines = content.split('\n')[:5]  # åªæ˜¾ç¤ºå‰5è¡Œ
        
        st.write(f"**æ–‡ä»¶æ€»è¡Œæ•°**: ~{len(content.split())}")
        st.write("**å‰5è¡Œå†…å®¹**:")
        
        for i, line in enumerate(lines, 1):
            if line.strip():
                try:
                    # å°è¯•è§£æJSONå¹¶æ ¼å¼åŒ–æ˜¾ç¤º
                    json_data = json.loads(line)
                    st.code(f"ç¬¬{i}è¡Œ: {json.dumps(json_data, indent=2, ensure_ascii=False)[:500]}...")
                except json.JSONDecodeError:
                    st.code(f"ç¬¬{i}è¡Œ: {line[:500]}...")
        
        # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        uploaded_file.seek(0)
        
    except Exception as e:
        st.error(f"âŒ æ–‡ä»¶é¢„è§ˆå¤±è´¥: {str(e)}")


def process_uploaded_file(uploaded_file):
    """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶"""
    progress_container = st.container()
    
    with progress_container:
        st.subheader("âš™ï¸ æ•°æ®å¤„ç†è¿›åº¦")
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # æ­¥éª¤1: ä¿å­˜æ–‡ä»¶
            status_text.text("ğŸ“ æ­£åœ¨ä¿å­˜æ–‡ä»¶...")
            progress_bar.progress(10)
            
            # ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
            upload_dir = Path("data/uploads")
            upload_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            file_path = upload_dir / uploaded_file.name
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # æ­¥éª¤2: è§£ææ•°æ®
            status_text.text("ğŸ” æ­£åœ¨è§£æGA4æ•°æ®...")
            progress_bar.progress(30)
            
            parser = GA4DataParser()
            raw_data = parser.parse_ndjson(str(file_path))
            
            # æ­¥éª¤3: æ•°æ®éªŒè¯
            status_text.text("âœ… æ­£åœ¨éªŒè¯æ•°æ®è´¨é‡...")
            progress_bar.progress(50)
            
            validator = DataValidator()
            validation_report = validator.validate_dataframe(raw_data)
            
            # æ­¥éª¤4: æ•°æ®å¤„ç†
            status_text.text("âš™ï¸ æ­£åœ¨å¤„ç†å’Œæ¸…æ´—æ•°æ®...")
            progress_bar.progress(70)
            
            # æå–äº‹ä»¶æ•°æ®
            events_data = parser.extract_events(raw_data)
            user_data = parser.extract_user_properties(raw_data)
            session_data = parser.extract_sessions(raw_data)
            
            # å¤„ç†äº‹ä»¶æ•°æ® - å¦‚æœæ˜¯å­—å…¸ï¼Œåˆå¹¶æ‰€æœ‰äº‹ä»¶ç±»å‹
            if isinstance(events_data, dict):
                # åˆå¹¶æ‰€æœ‰äº‹ä»¶ç±»å‹çš„æ•°æ®
                all_events_list = []
                for event_type, event_df in events_data.items():
                    if not event_df.empty:
                        all_events_list.append(event_df)
                
                if all_events_list:
                    combined_events = pd.concat(all_events_list, ignore_index=True)
                    st.success(f"âœ… åˆå¹¶äº† {len(events_data)} ç§äº‹ä»¶ç±»å‹ï¼Œæ€»è®¡ {len(combined_events)} ä¸ªäº‹ä»¶")
                else:
                    combined_events = pd.DataFrame()
                    st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„äº‹ä»¶æ•°æ®")
            else:
                combined_events = events_data
            
            # æ­¥éª¤5: å­˜å‚¨æ•°æ®
            status_text.text("ğŸ’¾ æ­£åœ¨å­˜å‚¨å¤„ç†ç»“æœ...")
            progress_bar.progress(90)
            
            # å­˜å‚¨åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.raw_data = raw_data
            st.session_state.processed_data = {
                'events': combined_events,
                'users': user_data,
                'sessions': session_data
            }
            
            # å­˜å‚¨åˆ°æ•°æ®ç®¡ç†å™¨
            storage_manager = st.session_state.storage_manager
            storage_manager.store_events(combined_events)
            storage_manager.store_users(user_data)
            storage_manager.store_sessions(session_data)
            
            # åˆ·æ–°é›†æˆç®¡ç†å™¨çš„å­˜å‚¨ç®¡ç†å™¨
            if 'integration_manager' in st.session_state:
                st.session_state.integration_manager.refresh_storage_manager(storage_manager)
            
            # ç”Ÿæˆæ•°æ®æ‘˜è¦
            data_summary = parser.validate_data_quality(raw_data)
            st.session_state.data_summary = data_summary
            st.session_state.validation_report = validation_report
            
            # å®Œæˆ
            status_text.text("âœ… æ•°æ®å¤„ç†å®Œæˆ!")
            progress_bar.progress(100)
            
            st.session_state.data_loaded = True
            
            # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
            st.success("ğŸ‰ æ•°æ®å¤„ç†æˆåŠŸå®Œæˆ!")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if file_path.exists():
                file_path.unlink()
            
            time.sleep(1)
            st.rerun()
            
        except Exception as e:
            st.error(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
            progress_bar.progress(0)
            status_text.text("âŒ å¤„ç†å¤±è´¥")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if 'file_path' in locals() and file_path.exists():
                file_path.unlink()


def show_processing_results():
    """æ˜¾ç¤ºå¤„ç†ç»“æœ"""
    st.markdown("---")
    st.subheader("ğŸ“Š æ•°æ®å¤„ç†ç»“æœ")
    
    if st.session_state.data_summary:
        summary = st.session_state.data_summary
        
        # åŸºç¡€ç»Ÿè®¡
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "æ€»äº‹ä»¶æ•°",
                f"{summary.get('total_events', 0):,}",
                help="æ•°æ®é›†ä¸­çš„æ€»äº‹ä»¶æ•°é‡"
            )
        
        with col2:
            st.metric(
                "ç‹¬ç«‹ç”¨æˆ·æ•°",
                f"{summary.get('unique_users', 0):,}",
                help="æ•°æ®é›†ä¸­çš„ç‹¬ç«‹ç”¨æˆ·æ•°é‡"
            )
        
        with col3:
            date_range = summary.get('date_range', {})
            days = (pd.to_datetime(date_range.get('end', '')) - pd.to_datetime(date_range.get('start', ''))).days
            st.metric(
                "æ•°æ®å¤©æ•°",
                f"{days}å¤©",
                help="æ•°æ®è¦†ç›–çš„æ—¶é—´èŒƒå›´"
            )
        
        with col4:
            st.metric(
                "äº‹ä»¶ç±»å‹æ•°",
                len(summary.get('event_types', {})),
                help="æ•°æ®ä¸­åŒ…å«çš„ä¸åŒäº‹ä»¶ç±»å‹æ•°é‡"
            )
        
        # è¯¦ç»†ä¿¡æ¯
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**ğŸ“… æ—¶é—´èŒƒå›´**")
            date_range = summary.get('date_range', {})
            st.write(f"- å¼€å§‹æ—¶é—´: {date_range.get('start', 'N/A')}")
            st.write(f"- ç»“æŸæ—¶é—´: {date_range.get('end', 'N/A')}")
            
            st.write("**ğŸ¯ äº‹ä»¶ç±»å‹åˆ†å¸ƒ**")
            event_types = summary.get('event_types', {})
            for event_type, count in list(event_types.items())[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                st.write(f"- {event_type}: {count:,}")
            if len(event_types) > 10:
                st.write(f"- ... è¿˜æœ‰ {len(event_types) - 10} ç§äº‹ä»¶ç±»å‹")
        
        with col2:
            st.write("**ğŸ“± å¹³å°åˆ†å¸ƒ**")
            platforms = summary.get('platforms', {})
            for platform, count in platforms.items():
                st.write(f"- {platform}: {count:,}")
            
            # æ•°æ®è´¨é‡é—®é¢˜
            if summary.get('data_issues'):
                st.write("**âš ï¸ æ•°æ®è´¨é‡é—®é¢˜**")
                for issue in summary.get('data_issues', []):
                    st.warning(f"- {issue}")
    
    # éªŒè¯æŠ¥å‘Š
    if st.session_state.validation_report:
        with st.expander("ğŸ” è¯¦ç»†éªŒè¯æŠ¥å‘Š", expanded=False):
            validation_report = st.session_state.validation_report
            
            if validation_report.get('validation_passed'):
                st.success("âœ… æ•°æ®éªŒè¯é€šè¿‡")
            else:
                st.error("âŒ æ•°æ®éªŒè¯å‘ç°é—®é¢˜")
            
            # é”™è¯¯ä¿¡æ¯
            if validation_report.get('errors'):
                st.write("**é”™è¯¯ä¿¡æ¯:**")
                for error in validation_report['errors']:
                    st.error(f"- {error}")
            
            # è­¦å‘Šä¿¡æ¯
            if validation_report.get('warnings'):
                st.write("**è­¦å‘Šä¿¡æ¯:**")
                for warning in validation_report['warnings']:
                    st.warning(f"- {warning}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = validation_report.get('statistics', {})
            if stats:
                st.write("**ç»Ÿè®¡ä¿¡æ¯:**")
                st.json(stats)


def show_system_settings():
    """æ˜¾ç¤ºç³»ç»Ÿè®¾ç½®é¡µé¢"""
    st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®ä¸é…ç½®")
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ”§ ç³»ç»Ÿé…ç½®", "ğŸ“Š åˆ†æå‚æ•°", "ğŸ“¥ å¯¼å‡ºè®¾ç½®", "ğŸ”„ é…ç½®ç®¡ç†"])
    
    with tab1:
        show_system_config_tab()
    
    with tab2:
        show_analysis_config_tab()
    
    with tab3:
        show_export_config_tab()
    
    with tab4:
        show_config_management_tab()


def show_system_config_tab():
    """æ˜¾ç¤ºç³»ç»Ÿé…ç½®æ ‡ç­¾é¡µ"""
    st.subheader("ğŸ”§ ç³»ç»Ÿé…ç½®")
    
    # è·å–å½“å‰é…ç½®
    system_config = config_manager.get_system_config()
    
    # APIé…ç½®
    st.write("### ğŸ”‘ APIé…ç½®")
    
    # LLMæä¾›å•†é€‰æ‹©
    st.write("#### ğŸ¤– LLMæä¾›å•†é…ç½®")
    col1, col2 = st.columns(2)
    
    with col1:
        default_provider = st.selectbox(
            "é»˜è®¤LLMæä¾›å•†",
            options=["google", "volcano"],
            index=0 if system_config.get('api_settings', {}).get('default_llm_provider', 'google') == 'google' else 1,
            help="é€‰æ‹©é»˜è®¤ä½¿ç”¨çš„LLMæä¾›å•†"
        )
        
        enabled_providers = st.multiselect(
            "å¯ç”¨çš„æä¾›å•†",
            options=["google", "volcano"],
            default=system_config.get('api_settings', {}).get('enabled_providers', ['google', 'volcano']),
            help="é€‰æ‹©è¦å¯ç”¨çš„LLMæä¾›å•†"
        )
    
    with col2:
        enable_fallback = st.checkbox(
            "å¯ç”¨æä¾›å•†å›é€€",
            value=system_config.get('api_settings', {}).get('enable_fallback', True),
            help="å½“ä¸»æä¾›å•†ä¸å¯ç”¨æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æä¾›å•†"
        )
        
        if enable_fallback:
            fallback_order = st.multiselect(
                "å›é€€é¡ºåº",
                options=["google", "volcano"],
                default=system_config.get('api_settings', {}).get('fallback_order', ['google', 'volcano']),
                help="æä¾›å•†å›é€€çš„ä¼˜å…ˆçº§é¡ºåº"
            )
        else:
            fallback_order = []
    
    # Google APIé…ç½®
    st.write("#### ğŸ”µ Google Gemini APIé…ç½®")
    col1, col2 = st.columns(2)
    
    with col1:
        current_google_key = system_config.get('api_settings', {}).get('google_api_key', '')
        new_google_key = st.text_input(
            "Google APIå¯†é’¥",
            value=current_google_key[:20] + "..." if len(current_google_key) > 20 else current_google_key,
            type="password",
            help="ç”¨äºè®¿é—®Google Gemini APIçš„å¯†é’¥"
        )
        
        google_model = st.selectbox(
            "Googleæ¨¡å‹",
            options=["gemini-2.5-pro", "gemini-1.5-pro", "gemini-1.5-flash"],
            index=0 if system_config.get('api_settings', {}).get('llm_model') == "gemini-2.5-pro" else 0
        )
    
    with col2:
        google_temperature = st.slider(
            "Googleæ¨¡å‹æ¸©åº¦",
            min_value=0.0,
            max_value=2.0,
            value=system_config.get('api_settings', {}).get('llm_temperature', 0.1),
            step=0.1,
            help="æ§åˆ¶Googleæ¨¡å‹è¾“å‡ºçš„éšæœºæ€§"
        )
        
        google_max_tokens = st.number_input(
            "Googleæœ€å¤§Tokenæ•°",
            min_value=1000,
            max_value=8000,
            value=system_config.get('api_settings', {}).get('llm_max_tokens', 4000),
            step=500
        )
    
    # Volcano APIé…ç½®
    st.write("#### ğŸŒ‹ Volcano ARK APIé…ç½®")
    col1, col2 = st.columns(2)
    
    with col1:
        current_ark_key = system_config.get('api_settings', {}).get('ark_api_key', '')
        new_ark_key = st.text_input(
            "Volcano ARK APIå¯†é’¥",
            value=current_ark_key[:20] + "..." if len(current_ark_key) > 20 else current_ark_key,
            type="password",
            help="ç”¨äºè®¿é—®Volcano ARK APIçš„å¯†é’¥"
        )
        
        ark_base_url = st.text_input(
            "ARK APIåŸºç¡€URL",
            value=system_config.get('api_settings', {}).get('ark_base_url', 'https://ark.cn-beijing.volces.com/api/v3'),
            help="Volcano ARK APIçš„åŸºç¡€URL"
        )
    
    with col2:
        ark_model = st.text_input(
            "Volcanoæ¨¡å‹åç§°",
            value=system_config.get('api_settings', {}).get('ark_model', 'doubao-seed-1-6-250615'),
            help="Volcano Doubaoæ¨¡å‹åç§°"
        )
        
        ark_temperature = st.slider(
            "Volcanoæ¨¡å‹æ¸©åº¦",
            min_value=0.0,
            max_value=2.0,
            value=system_config.get('api_settings', {}).get('ark_temperature', 0.1),
            step=0.1,
            help="æ§åˆ¶Volcanoæ¨¡å‹è¾“å‡ºçš„éšæœºæ€§"
        )
    
    # å¤šæ¨¡æ€é…ç½®
    st.write("#### ğŸ–¼ï¸ å¤šæ¨¡æ€é…ç½®")
    col1, col2 = st.columns(2)
    
    with col1:
        enable_multimodal = st.checkbox(
            "å¯ç”¨å¤šæ¨¡æ€æ”¯æŒ",
            value=system_config.get('api_settings', {}).get('enable_multimodal', True),
            help="å¯ç”¨å›¾ç‰‡å’Œå¤šåª’ä½“å†…å®¹åˆ†æ"
        )
        
        max_image_size = st.number_input(
            "æœ€å¤§å›¾ç‰‡å¤§å° (MB)",
            min_value=1,
            max_value=50,
            value=system_config.get('api_settings', {}).get('max_image_size_mb', 10),
            step=1
        )
    
    with col2:
        if enable_multimodal:
            supported_formats = st.multiselect(
                "æ”¯æŒçš„å›¾ç‰‡æ ¼å¼",
                options=["jpg", "jpeg", "png", "gif", "webp", "bmp"],
                default=system_config.get('api_settings', {}).get('supported_image_formats', ['jpg', 'jpeg', 'png', 'gif', 'webp']),
                help="é€‰æ‹©æ”¯æŒçš„å›¾ç‰‡æ ¼å¼"
            )
            
            image_timeout = st.number_input(
                "å›¾ç‰‡åˆ†æè¶…æ—¶ (ç§’)",
                min_value=10,
                max_value=300,
                value=system_config.get('api_settings', {}).get('image_analysis_timeout', 60),
                step=10
            )
        else:
            supported_formats = []
            image_timeout = 60
    
    # æ•°æ®å¤„ç†é…ç½®
    st.write("### ğŸ’¾ æ•°æ®å¤„ç†é…ç½®")
    col1, col2 = st.columns(2)
    
    with col1:
        max_file_size = st.number_input(
            "æœ€å¤§æ–‡ä»¶å¤§å° (MB)",
            min_value=10,
            max_value=500,
            value=system_config.get('data_processing', {}).get('max_file_size_mb', 100),
            step=10
        )
        
        chunk_size = st.number_input(
            "æ•°æ®å¤„ç†å—å¤§å°",
            min_value=1000,
            max_value=50000,
            value=system_config.get('data_processing', {}).get('chunk_size', 10000),
            step=1000
        )
    
    with col2:
        memory_limit = st.number_input(
            "å†…å­˜é™åˆ¶ (GB)",
            min_value=1,
            max_value=16,
            value=system_config.get('data_processing', {}).get('memory_limit_gb', 4),
            step=1
        )
        
        cleanup_temp = st.checkbox(
            "è‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶",
            value=system_config.get('data_processing', {}).get('cleanup_temp_files', True)
        )
    
    # ç•Œé¢é…ç½®
    st.write("### ğŸ¨ ç•Œé¢é…ç½®")
    col1, col2 = st.columns(2)
    
    with col1:
        ui_theme = st.selectbox(
            "ç•Œé¢ä¸»é¢˜",
            options=["light", "dark"],
            index=0 if system_config.get('ui_settings', {}).get('theme') == "light" else 1
        )
        
        language = st.selectbox(
            "ç•Œé¢è¯­è¨€",
            options=["zh-CN", "en-US"],
            index=0 if system_config.get('ui_settings', {}).get('language') == "zh-CN" else 1
        )
    
    with col2:
        page_size = st.number_input(
            "é¡µé¢å¤§å°",
            min_value=10,
            max_value=100,
            value=system_config.get('ui_settings', {}).get('page_size', 20),
            step=5
        )
        
        show_debug = st.checkbox(
            "æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯",
            value=system_config.get('ui_settings', {}).get('show_debug_info', False)
        )
    
    # ä¿å­˜é…ç½®æŒ‰é’®
    if st.button("ğŸ’¾ ä¿å­˜ç³»ç»Ÿé…ç½®", type="primary"):
        try:
            # æ„å»ºAPIé…ç½®å­—å…¸
            api_config = {
                # æä¾›å•†é…ç½®
                'default_llm_provider': default_provider,
                'enabled_providers': enabled_providers,
                'enable_fallback': enable_fallback,
                'fallback_order': fallback_order,
                
                # Googleé…ç½®
                'llm_model': google_model,
                'llm_temperature': google_temperature,
                'llm_max_tokens': google_max_tokens,
                
                # Volcanoé…ç½®
                'ark_base_url': ark_base_url,
                'ark_model': ark_model,
                'ark_temperature': ark_temperature,
                
                # å¤šæ¨¡æ€é…ç½®
                'enable_multimodal': enable_multimodal,
                'max_image_size_mb': max_image_size,
                'supported_image_formats': supported_formats,
                'image_analysis_timeout': image_timeout
            }
            
            # æ›´æ–°APIå¯†é’¥ï¼ˆå¦‚æœæœ‰å˜åŒ–ï¼‰
            if new_google_key and new_google_key != current_google_key[:20] + "...":
                api_config['google_api_key'] = new_google_key
            
            if new_ark_key and new_ark_key != current_ark_key[:20] + "...":
                api_config['ark_api_key'] = new_ark_key
            
            # ä¿å­˜APIé…ç½®
            config_manager.update_system_config('api_settings', api_config)
            
            # æ›´æ–°æ•°æ®å¤„ç†é…ç½®
            config_manager.update_system_config('data_processing', {
                'max_file_size_mb': max_file_size,
                'chunk_size': chunk_size,
                'memory_limit_gb': memory_limit,
                'cleanup_temp_files': cleanup_temp
            })
            
            # æ›´æ–°ç•Œé¢é…ç½®
            config_manager.update_system_config('ui_settings', {
                'theme': ui_theme,
                'language': language,
                'page_size': page_size,
                'show_debug_info': show_debug
            })
            
            st.success("âœ… ç³»ç»Ÿé…ç½®ä¿å­˜æˆåŠŸ!")
            st.rerun()
            
        except Exception as e:
            st.error(f"âŒ é…ç½®ä¿å­˜å¤±è´¥: {str(e)}")


def show_analysis_config_tab():
    """æ˜¾ç¤ºåˆ†æå‚æ•°é…ç½®æ ‡ç­¾é¡µ"""
    st.subheader("ğŸ“Š åˆ†æå‚æ•°é…ç½®")
    
    # è·å–å½“å‰åˆ†æé…ç½®
    analysis_config = config_manager.get_analysis_config()
    
    # äº‹ä»¶åˆ†æé…ç½®
    with st.expander("ğŸ“ˆ äº‹ä»¶åˆ†æé…ç½®", expanded=True):
        col1, col2 = st.columns(2)
        
        with col1:
            event_granularity = st.selectbox(
                "æ—¶é—´ç²’åº¦",
                options=["day", "week", "month"],
                index=["day", "week", "month"].index(analysis_config.get('event_analysis', {}).get('time_granularity', 'day'))
            )
            
            top_events_limit = st.number_input(
                "çƒ­é—¨äº‹ä»¶æ•°é‡é™åˆ¶",
                min_value=5,
                max_value=50,
                value=analysis_config.get('event_analysis', {}).get('top_events_limit', 10),
                step=5
            )
        
        with col2:
            trend_days = st.number_input(
                "è¶‹åŠ¿åˆ†æå¤©æ•°",
                min_value=7,
                max_value=90,
                value=analysis_config.get('event_analysis', {}).get('trend_analysis_days', 30),
                step=7
            )
            
            correlation_threshold = st.slider(
                "å…³è”æ€§é˜ˆå€¼",
                min_value=0.1,
                max_value=1.0,
                value=analysis_config.get('event_analysis', {}).get('correlation_threshold', 0.5),
                step=0.1
            )
    
    # ç•™å­˜åˆ†æé…ç½®
    with st.expander("ğŸ“ˆ ç•™å­˜åˆ†æé…ç½®", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            retention_periods_str = st.text_input(
                "ç•™å­˜åˆ†æå‘¨æœŸ (å¤©ï¼Œé€—å·åˆ†éš”)",
                value=",".join(map(str, analysis_config.get('retention_analysis', {}).get('retention_periods', [1, 7, 14, 30])))
            )
            
            cohort_type = st.selectbox(
                "é˜Ÿåˆ—ç±»å‹",
                options=["daily", "weekly", "monthly"],
                index=["daily", "weekly", "monthly"].index(analysis_config.get('retention_analysis', {}).get('cohort_type', 'weekly'))
            )
        
        with col2:
            min_cohort_size = st.number_input(
                "æœ€å°é˜Ÿåˆ—å¤§å°",
                min_value=10,
                max_value=1000,
                value=analysis_config.get('retention_analysis', {}).get('min_cohort_size', 100),
                step=10
            )
            
            analysis_window = st.number_input(
                "åˆ†æçª—å£ (å¤©)",
                min_value=30,
                max_value=365,
                value=analysis_config.get('retention_analysis', {}).get('analysis_window_days', 90),
                step=30
            )
    
    # è½¬åŒ–åˆ†æé…ç½®
    with st.expander("ğŸ”„ è½¬åŒ–åˆ†æé…ç½®", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            conversion_window = st.number_input(
                "è½¬åŒ–çª—å£ (å°æ—¶)",
                min_value=1,
                max_value=168,
                value=analysis_config.get('conversion_analysis', {}).get('conversion_window_hours', 24),
                step=1
            )
            
            min_funnel_users = st.number_input(
                "æœ€å°æ¼æ–—ç”¨æˆ·æ•°",
                min_value=10,
                max_value=500,
                value=analysis_config.get('conversion_analysis', {}).get('min_funnel_users', 50),
                step=10
            )
        
        with col2:
            attribution_model = st.selectbox(
                "å½’å› æ¨¡å‹",
                options=["first_touch", "last_touch", "linear"],
                index=["first_touch", "last_touch", "linear"].index(analysis_config.get('conversion_analysis', {}).get('attribution_model', 'first_touch'))
            )
    
    # ç”¨æˆ·åˆ†ç¾¤é…ç½®
    with st.expander("ğŸ‘¥ ç”¨æˆ·åˆ†ç¾¤é…ç½®", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            clustering_method = st.selectbox(
                "èšç±»ç®—æ³•",
                options=["kmeans", "dbscan"],
                index=["kmeans", "dbscan"].index(analysis_config.get('user_segmentation', {}).get('clustering_method', 'kmeans'))
            )
            
            n_clusters = st.slider(
                "èšç±»æ•°é‡",
                min_value=2,
                max_value=15,
                value=analysis_config.get('user_segmentation', {}).get('n_clusters', 5),
                step=1
            )
        
        with col2:
            min_cluster_size = st.number_input(
                "æœ€å°èšç±»å¤§å°",
                min_value=10,
                max_value=500,
                value=analysis_config.get('user_segmentation', {}).get('min_cluster_size', 100),
                step=10
            )
            
            feature_types = st.multiselect(
                "ç‰¹å¾ç±»å‹",
                options=["behavioral", "demographic", "temporal"],
                default=analysis_config.get('user_segmentation', {}).get('feature_types', ['behavioral', 'demographic'])
            )
    
    # è·¯å¾„åˆ†æé…ç½®
    with st.expander("ğŸ›¤ï¸ è·¯å¾„åˆ†æé…ç½®", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            min_path_support = st.slider(
                "æœ€å°è·¯å¾„æ”¯æŒåº¦",
                min_value=0.001,
                max_value=0.1,
                value=analysis_config.get('path_analysis', {}).get('min_path_support', 0.01),
                step=0.001,
                format="%.3f"
            )
            
            max_path_length = st.number_input(
                "æœ€å¤§è·¯å¾„é•¿åº¦",
                min_value=3,
                max_value=20,
                value=analysis_config.get('path_analysis', {}).get('max_path_length', 10),
                step=1
            )
        
        with col2:
            session_timeout = st.number_input(
                "ä¼šè¯è¶…æ—¶ (åˆ†é’Ÿ)",
                min_value=5,
                max_value=120,
                value=analysis_config.get('path_analysis', {}).get('session_timeout_minutes', 30),
                step=5
            )
            
            include_bounce = st.checkbox(
                "åŒ…å«è·³å‡ºä¼šè¯",
                value=analysis_config.get('path_analysis', {}).get('include_bounce_sessions', False)
            )
    
    # ä¿å­˜åˆ†æé…ç½®
    if st.button("ğŸ’¾ ä¿å­˜åˆ†æé…ç½®", type="primary"):
        try:
            # è§£æç•™å­˜å‘¨æœŸ
            retention_periods = [int(x.strip()) for x in retention_periods_str.split(',') if x.strip().isdigit()]
            
            # æ›´æ–°äº‹ä»¶åˆ†æé…ç½®
            config_manager.update_analysis_config('event_analysis', {
                'time_granularity': event_granularity,
                'top_events_limit': top_events_limit,
                'trend_analysis_days': trend_days,
                'correlation_threshold': correlation_threshold
            })
            
            # æ›´æ–°ç•™å­˜åˆ†æé…ç½®
            config_manager.update_analysis_config('retention_analysis', {
                'retention_periods': retention_periods,
                'cohort_type': cohort_type,
                'min_cohort_size': min_cohort_size,
                'analysis_window_days': analysis_window
            })
            
            # æ›´æ–°è½¬åŒ–åˆ†æé…ç½®
            config_manager.update_analysis_config('conversion_analysis', {
                'conversion_window_hours': conversion_window,
                'min_funnel_users': min_funnel_users,
                'attribution_model': attribution_model
            })
            
            # æ›´æ–°ç”¨æˆ·åˆ†ç¾¤é…ç½®
            config_manager.update_analysis_config('user_segmentation', {
                'clustering_method': clustering_method,
                'n_clusters': n_clusters,
                'min_cluster_size': min_cluster_size,
                'feature_types': feature_types
            })
            
            # æ›´æ–°è·¯å¾„åˆ†æé…ç½®
            config_manager.update_analysis_config('path_analysis', {
                'min_path_support': min_path_support,
                'max_path_length': max_path_length,
                'session_timeout_minutes': session_timeout,
                'include_bounce_sessions': include_bounce
            })
            
            st.success("âœ… åˆ†æé…ç½®ä¿å­˜æˆåŠŸ!")
            
        except Exception as e:
            st.error(f"âŒ é…ç½®ä¿å­˜å¤±è´¥: {str(e)}")


def show_export_config_tab():
    """æ˜¾ç¤ºå¯¼å‡ºè®¾ç½®æ ‡ç­¾é¡µ"""
    st.subheader("ğŸ“¥ å¯¼å‡ºè®¾ç½®")
    
    # è·å–å½“å‰å¯¼å‡ºé…ç½®
    export_config = config_manager.get_system_config('export_settings')
    
    col1, col2 = st.columns(2)
    
    with col1:
        default_format = st.selectbox(
            "é»˜è®¤å¯¼å‡ºæ ¼å¼",
            options=["json", "pdf", "excel"],
            index=["json", "pdf", "excel"].index(export_config.get('default_format', 'json'))
        )
        
        include_raw_data = st.checkbox(
            "é»˜è®¤åŒ…å«åŸå§‹æ•°æ®",
            value=export_config.get('include_raw_data', False),
            help="åœ¨å¯¼å‡ºæŠ¥å‘Šæ—¶é»˜è®¤åŒ…å«åŸå§‹æ•°æ®"
        )
        
        compress_exports = st.checkbox(
            "å‹ç¼©å¯¼å‡ºæ–‡ä»¶",
            value=export_config.get('compress_exports', True),
            help="å¯¹å¤§å‹å¯¼å‡ºæ–‡ä»¶è¿›è¡Œå‹ç¼©"
        )
    
    with col2:
        export_dir = st.text_input(
            "å¯¼å‡ºç›®å½•",
            value=export_config.get('export_dir', 'reports'),
            help="æŠ¥å‘Šæ–‡ä»¶çš„é»˜è®¤ä¿å­˜ç›®å½•"
        )
        
        filename_template = st.text_input(
            "æ–‡ä»¶åæ¨¡æ¿",
            value=export_config.get('filename_template', 'analysis_report_{timestamp}'),
            help="ä½¿ç”¨{timestamp}ä½œä¸ºæ—¶é—´æˆ³å ä½ç¬¦"
        )
    
    # å¯¼å‡ºæ ¼å¼æ”¯æŒçŠ¶æ€
    st.write("### ğŸ“‹ å¯¼å‡ºæ ¼å¼æ”¯æŒçŠ¶æ€")
    exporter = ReportExporter()
    supported_formats = exporter.get_supported_formats()
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.write("**JSON**")
        st.success("âœ… æ”¯æŒ" if 'json' in supported_formats else "âŒ ä¸æ”¯æŒ")
    
    with col2:
        st.write("**PDF**")
        st.success("âœ… æ”¯æŒ" if 'pdf' in supported_formats else "âŒ ä¸æ”¯æŒ")
        if 'pdf' not in supported_formats:
            st.info("éœ€è¦å®‰è£… reportlab åº“")
    
    with col3:
        st.write("**Excel**")
        st.success("âœ… æ”¯æŒ" if 'excel' in supported_formats else "âŒ ä¸æ”¯æŒ")
        if 'excel' not in supported_formats:
            st.info("éœ€è¦å®‰è£… openpyxl åº“")
    
    # ä¿å­˜å¯¼å‡ºé…ç½®
    if st.button("ğŸ’¾ ä¿å­˜å¯¼å‡ºé…ç½®", type="primary"):
        try:
            config_manager.update_system_config('export_settings', {
                'default_format': default_format,
                'include_raw_data': include_raw_data,
                'compress_exports': compress_exports,
                'export_dir': export_dir,
                'filename_template': filename_template
            })
            
            st.success("âœ… å¯¼å‡ºé…ç½®ä¿å­˜æˆåŠŸ!")
            
        except Exception as e:
            st.error(f"âŒ é…ç½®ä¿å­˜å¤±è´¥: {str(e)}")


def show_config_management_tab():
    """æ˜¾ç¤ºé…ç½®ç®¡ç†æ ‡ç­¾é¡µ"""
    st.subheader("ğŸ”„ é…ç½®ç®¡ç†")
    
    # é…ç½®çŠ¶æ€æ¦‚è§ˆ
    st.write("### ğŸ“Š é…ç½®çŠ¶æ€æ¦‚è§ˆ")
    config_summary = config_manager.get_config_summary()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**åˆ†æé…ç½®**")
        analysis_summary = config_summary.get('analysis_config', {})
        st.write(f"- äº‹ä»¶åˆ†æ: {'âœ…' if analysis_summary.get('event_analysis_enabled') else 'âŒ'}")
        st.write(f"- ç•™å­˜å‘¨æœŸæ•°: {analysis_summary.get('retention_periods', 0)}")
        st.write(f"- èšç±»æ–¹æ³•: {analysis_summary.get('clustering_method', 'N/A')}")
        st.write(f"- æ¼æ–—æ­¥éª¤: {analysis_summary.get('funnel_steps_configured', 0)}")
    
    with col2:
        st.write("**ç³»ç»Ÿé…ç½®**")
        system_summary = config_summary.get('system_config', {})
        st.write(f"- APIé…ç½®: {'âœ…' if system_summary.get('api_configured') else 'âŒ'}")
        st.write(f"- æ–‡ä»¶å¤§å°é™åˆ¶: {system_summary.get('max_file_size_mb', 0)}MB")
        st.write(f"- é»˜è®¤å¯¼å‡ºæ ¼å¼: {system_summary.get('default_export_format', 'N/A')}")
        st.write(f"- ç•Œé¢ä¸»é¢˜: {system_summary.get('ui_theme', 'N/A')}")
    
    # é…ç½®éªŒè¯
    st.write("### âœ… é…ç½®éªŒè¯")
    if st.button("ğŸ” éªŒè¯é…ç½®", type="secondary"):
        validation_result = config_manager.validate_config()
        
        if validation_result['valid']:
            st.success("âœ… é…ç½®éªŒè¯é€šè¿‡!")
        else:
            st.error("âŒ é…ç½®éªŒè¯å¤±è´¥!")
            
            if validation_result['errors']:
                st.write("**é”™è¯¯:**")
                for error in validation_result['errors']:
                    st.error(f"â€¢ {error}")
        
        if validation_result['warnings']:
            st.write("**è­¦å‘Š:**")
            for warning in validation_result['warnings']:
                st.warning(f"â€¢ {warning}")
    
    # é…ç½®å¯¼å…¥å¯¼å‡º
    st.write("### ğŸ“ é…ç½®å¯¼å…¥å¯¼å‡º")
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**å¯¼å‡ºé…ç½®**")
        if st.button("ğŸ“¤ å¯¼å‡ºé…ç½®æ–‡ä»¶"):
            try:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                export_path = f"config/config_backup_{timestamp}.json"
                
                if config_manager.export_config(export_path):
                    st.success(f"âœ… é…ç½®å¯¼å‡ºæˆåŠŸ: {export_path}")
                    
                    # æä¾›ä¸‹è½½
                    with open(export_path, 'r', encoding='utf-8') as f:
                        config_data = f.read()
                    
                    st.download_button(
                        label="â¬‡ï¸ ä¸‹è½½é…ç½®æ–‡ä»¶",
                        data=config_data,
                        file_name=f"config_backup_{timestamp}.json",
                        mime="application/json"
                    )
                else:
                    st.error("âŒ é…ç½®å¯¼å‡ºå¤±è´¥")
                    
            except Exception as e:
                st.error(f"âŒ å¯¼å‡ºè¿‡ç¨‹å‡ºé”™: {str(e)}")
    
    with col2:
        st.write("**å¯¼å…¥é…ç½®**")
        uploaded_config = st.file_uploader(
            "é€‰æ‹©é…ç½®æ–‡ä»¶",
            type=['json'],
            help="ä¸Šä¼ ä¹‹å‰å¯¼å‡ºçš„é…ç½®æ–‡ä»¶"
        )
        
        if uploaded_config is not None:
            if st.button("ğŸ“¥ å¯¼å…¥é…ç½®", type="primary"):
                try:
                    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                    import tempfile
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as tmp_file:
                        tmp_file.write(uploaded_config.read().decode('utf-8'))
                        tmp_path = tmp_file.name
                    
                    if config_manager.import_config(tmp_path):
                        st.success("âœ… é…ç½®å¯¼å…¥æˆåŠŸ!")
                        st.info("è¯·åˆ·æ–°é¡µé¢ä»¥åº”ç”¨æ–°é…ç½®")
                    else:
                        st.error("âŒ é…ç½®å¯¼å…¥å¤±è´¥")
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    os.unlink(tmp_path)
                    
                except Exception as e:
                    st.error(f"âŒ å¯¼å…¥è¿‡ç¨‹å‡ºé”™: {str(e)}")
    
    # é‡ç½®é…ç½®
    st.write("### ğŸ”„ é‡ç½®é…ç½®")
    st.warning("âš ï¸ é‡ç½®æ“ä½œå°†æ¢å¤é»˜è®¤é…ç½®ï¼Œæ— æ³•æ’¤é”€!")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ”„ é‡ç½®åˆ†æé…ç½®", type="secondary"):
            if config_manager.reset_analysis_config():
                st.success("âœ… åˆ†æé…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼")
                st.rerun()
            else:
                st.error("âŒ é‡ç½®åˆ†æé…ç½®å¤±è´¥")
    
    with col2:
        if st.button("ğŸ”„ é‡ç½®ç³»ç»Ÿé…ç½®", type="secondary"):
            if config_manager.reset_system_config():
                st.success("âœ… ç³»ç»Ÿé…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼")
                st.rerun()
            else:
                st.error("âŒ é‡ç½®ç³»ç»Ÿé…ç½®å¤±è´¥")


def show_event_analysis_page():
    """æ˜¾ç¤ºäº‹ä»¶åˆ†æç»“æœé¡µé¢"""
    st.header("ğŸ“Š äº‹ä»¶åˆ†æ")
    
    # åˆå§‹åŒ–åˆ†æå¼•æ“å’Œå¯è§†åŒ–ç»„ä»¶
    if 'event_engine' not in st.session_state:
        st.session_state.event_engine = EventAnalysisEngine()
    if 'chart_generator' not in st.session_state:
        st.session_state.chart_generator = ChartGenerator()
    
    # åˆ†ææ§åˆ¶é¢æ¿
    with st.expander("ğŸ”§ åˆ†æé…ç½®", expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            date_range = st.date_input(
                "é€‰æ‹©åˆ†ææ—¶é—´èŒƒå›´",
                value=(datetime.now() - timedelta(days=30), datetime.now()),
                help="é€‰æ‹©è¦åˆ†æçš„æ—¶é—´èŒƒå›´"
            )
        
        with col2:
            event_filter = st.multiselect(
                "ç­›é€‰äº‹ä»¶ç±»å‹",
                options=list(st.session_state.data_summary.get('event_types', {}).keys()),
                default=list(st.session_state.data_summary.get('event_types', {}).keys())[:5],
                help="é€‰æ‹©è¦åˆ†æçš„äº‹ä»¶ç±»å‹"
            )
        
        with col3:
            analysis_granularity = st.selectbox(
                "åˆ†æç²’åº¦",
                options=["æ—¥", "å‘¨", "æœˆ"],
                index=0,
                help="é€‰æ‹©æ—¶é—´åˆ†æçš„ç²’åº¦"
            )
    
    # æ‰§è¡Œåˆ†ææŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹äº‹ä»¶åˆ†æ", type="primary"):
        with st.spinner("æ­£åœ¨è¿›è¡Œäº‹ä»¶åˆ†æ..."):
            try:
                # è·å–æ•°æ®
                raw_data = st.session_state.raw_data
                
                # åº”ç”¨ç­›é€‰æ¡ä»¶
                if event_filter:
                    filtered_data = raw_data[raw_data['event_name'].isin(event_filter)]
                else:
                    filtered_data = raw_data
                
                # æ‰§è¡Œåˆ†æ
                engine = st.session_state.event_engine
                
                # äº‹ä»¶é¢‘æ¬¡åˆ†æ
                frequency_results = engine.analyze_event_frequency(filtered_data)
                
                # äº‹ä»¶è¶‹åŠ¿åˆ†æ - è½¬æ¢ä¸­æ–‡ç²’åº¦ä¸ºè‹±æ–‡
                english_granularity = translate_time_granularity(analysis_granularity)
                trend_results = engine.analyze_event_trends(filtered_data, time_granularity=english_granularity)
                
                # å­˜å‚¨åˆ†æç»“æœ
                st.session_state.event_analysis_results = {
                    'frequency': frequency_results,
                    'trends': trend_results,
                    'filtered_data': filtered_data
                }
                
                st.success("âœ… äº‹ä»¶åˆ†æå®Œæˆ!")
                
            except Exception as e:
                st.error(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    if 'event_analysis_results' in st.session_state:
        results = st.session_state.event_analysis_results
        chart_gen = st.session_state.chart_generator
        
        st.markdown("---")
        st.subheader("ğŸ“ˆ åˆ†æç»“æœ")
        
        # å…³é”®æŒ‡æ ‡æ¦‚è§ˆ
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            total_events = len(results['filtered_data'])
            st.metric("æ€»äº‹ä»¶æ•°", f"{total_events:,}")
        
        with col2:
            unique_users = results['filtered_data']['user_pseudo_id'].nunique()
            st.metric("æ´»è·ƒç”¨æˆ·æ•°", f"{unique_users:,}")
        
        with col3:
            avg_events_per_user = total_events / unique_users if unique_users > 0 else 0
            st.metric("äººå‡äº‹ä»¶æ•°", f"{avg_events_per_user:.1f}")
        
        with col4:
            event_types = results['filtered_data']['event_name'].nunique()
            st.metric("äº‹ä»¶ç±»å‹æ•°", f"{event_types}")
        
        # äº‹ä»¶æ—¶é—´çº¿å›¾è¡¨
        st.subheader("ğŸ“Š äº‹ä»¶æ—¶é—´çº¿")
        try:
            timeline_chart = chart_gen.create_event_timeline(results['filtered_data'])
            st.plotly_chart(timeline_chart, use_container_width=True)
        except Exception as e:
            st.error(f"æ—¶é—´çº¿å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}")
        
        # äº‹ä»¶é¢‘æ¬¡åˆ†å¸ƒ
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ¯ äº‹ä»¶é¢‘æ¬¡åˆ†å¸ƒ")
            event_counts = results['filtered_data']['event_name'].value_counts()
            st.bar_chart(event_counts)
        
        with col2:
            st.subheader("ğŸ‘¥ ç”¨æˆ·æ´»è·ƒåº¦åˆ†å¸ƒ")
            user_activity = results['filtered_data'].groupby('user_pseudo_id').size()

            # åˆ›å»ºç”¨æˆ·æ´»è·ƒåº¦åˆ†å¸ƒç›´æ–¹å›¾
            activity_df = pd.DataFrame({
                'user_id': user_activity.index,
                'event_count': user_activity.values
            })

            fig = px.histogram(
                activity_df,
                x='event_count',
                nbins=20,
                title="ç”¨æˆ·äº‹ä»¶æ•°é‡åˆ†å¸ƒ",
                labels={'event_count': 'äº‹ä»¶æ•°é‡', 'count': 'ç”¨æˆ·æ•°é‡'}
            )
            fig.update_layout(
                xaxis_title="äº‹ä»¶æ•°é‡",
                yaxis_title="ç”¨æˆ·æ•°é‡",
                showlegend=False
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # è¯¦ç»†æ•°æ®è¡¨
        with st.expander("ğŸ“‹ è¯¦ç»†æ•°æ®", expanded=False):
            st.dataframe(
                results['filtered_data'][['event_date', 'event_name', 'user_pseudo_id', 'platform']].head(1000),
                use_container_width=True
            )


def show_retention_analysis_page():
    """æ˜¾ç¤ºç•™å­˜åˆ†æç»“æœé¡µé¢"""
    st.header("ğŸ“ˆ ç”¨æˆ·ç•™å­˜åˆ†æ")
    
    # åˆå§‹åŒ–åˆ†æå¼•æ“
    if 'retention_engine' not in st.session_state:
        st.session_state.retention_engine = RetentionAnalysisEngine()
    if 'chart_generator' not in st.session_state:
        st.session_state.chart_generator = ChartGenerator()
    
    # åˆ†æé…ç½®
    with st.expander("ğŸ”§ ç•™å­˜åˆ†æé…ç½®", expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            retention_type = st.selectbox(
                "ç•™å­˜ç±»å‹",
                options=["æ—¥ç•™å­˜", "å‘¨ç•™å­˜", "æœˆç•™å­˜"],
                index=0
            )
        
        with col2:
            cohort_period = st.selectbox(
                "é˜Ÿåˆ—å‘¨æœŸ",
                options=["æ—¥", "å‘¨", "æœˆ"],
                index=1
            )
        
        with col3:
            analysis_periods = st.slider(
                "åˆ†æå‘¨æœŸæ•°",
                min_value=7,
                max_value=30,
                value=14,
                help="åˆ†æå¤šå°‘ä¸ªå‘¨æœŸçš„ç•™å­˜æƒ…å†µ"
            )
    
    # æ‰§è¡Œç•™å­˜åˆ†æ
    if st.button("ğŸš€ å¼€å§‹ç•™å­˜åˆ†æ", type="primary"):
        with st.spinner("æ­£åœ¨è¿›è¡Œç•™å­˜åˆ†æ..."):
            try:
                raw_data = st.session_state.raw_data
                engine = st.session_state.retention_engine
                
                # æ‰§è¡Œç•™å­˜åˆ†æ - è½¬æ¢ä¸­æ–‡ç±»å‹ä¸ºè‹±æ–‡
                english_retention_type = translate_cohort_period(retention_type)
                english_cohort_period = translate_cohort_period(cohort_period)

                # æ‰§è¡Œå®Œæ•´çš„ç•™å­˜åˆ†æï¼Œè·å–åŒ…å«é˜Ÿåˆ—æ•°æ®çš„ç»“æœ
                if english_retention_type == "daily":
                    retention_results = engine.calculate_retention_rates(
                        events=raw_data,
                        analysis_type='daily',
                        max_periods=analysis_periods
                    )
                elif english_retention_type == "weekly":
                    retention_results = engine.calculate_retention_rates(
                        events=raw_data,
                        analysis_type='weekly',
                        max_periods=analysis_periods
                    )
                elif english_retention_type == "monthly":
                    retention_results = engine.calculate_retention_rates(
                        events=raw_data,
                        analysis_type='monthly',
                        max_periods=analysis_periods
                    )
                else:
                    # é»˜è®¤ä½¿ç”¨æœˆåº¦åˆ†æ
                    retention_results = engine.calculate_retention_rates(
                        events=raw_data,
                        analysis_type='monthly',
                        max_periods=analysis_periods
                    )

                # ä»ç•™å­˜åˆ†æç»“æœä¸­æå–é˜Ÿåˆ—æ•°æ®å¹¶è½¬æ¢ä¸ºçƒ­åŠ›å›¾æ‰€éœ€æ ¼å¼
                cohort_viz_data = []
                if retention_results and hasattr(retention_results, 'cohorts'):
                    for cohort in retention_results.cohorts:
                        cohort_period = cohort.cohort_period
                        retention_rates = cohort.retention_rates

                        for period_num, rate in enumerate(retention_rates):
                            cohort_viz_data.append({
                                'cohort_group': cohort_period,
                                'period_number': period_num,
                                'retention_rate': rate
                            })

                # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®
                if not cohort_viz_data:
                    cohort_viz_data = [
                        {'cohort_group': '2024-01', 'period_number': 0, 'retention_rate': 1.0},
                        {'cohort_group': '2024-01', 'period_number': 1, 'retention_rate': 0.7},
                        {'cohort_group': '2024-01', 'period_number': 2, 'retention_rate': 0.5},
                        {'cohort_group': '2024-02', 'period_number': 0, 'retention_rate': 1.0},
                        {'cohort_group': '2024-02', 'period_number': 1, 'retention_rate': 0.6},
                        {'cohort_group': '2024-02', 'period_number': 2, 'retention_rate': 0.4}
                    ]

                # è½¬æ¢ä¸ºDataFrame
                cohort_data = pd.DataFrame(cohort_viz_data)

                st.session_state.retention_results = {
                    'retention_data': retention_results,
                    'cohort_data': cohort_data,
                    'cohorts': retention_results.cohorts if retention_results and hasattr(retention_results, 'cohorts') else [],
                    'overall_retention_rates': retention_results.overall_retention_rates if retention_results and hasattr(retention_results, 'overall_retention_rates') else {}
                }
                
                st.success("âœ… ç•™å­˜åˆ†æå®Œæˆ!")
                
            except Exception as e:
                st.error(f"âŒ ç•™å­˜åˆ†æå¤±è´¥: {str(e)}")
    
    # æ˜¾ç¤ºç•™å­˜åˆ†æç»“æœ
    if 'retention_results' in st.session_state:
        results = st.session_state.retention_results
        chart_gen = st.session_state.chart_generator
        
        st.markdown("---")
        st.subheader("ğŸ“Š ç•™å­˜åˆ†æç»“æœ")
        
        # ç•™å­˜çƒ­åŠ›å›¾
        if 'cohort_data' in results and results['cohort_data'] is not None:
            # æ£€æŸ¥æ˜¯å¦ä¸ºDataFrameä¸”ä¸ä¸ºç©ºï¼Œæˆ–è€…æ˜¯å¦ä¸ºéç©ºå­—å…¸/åˆ—è¡¨
            cohort_data = results['cohort_data']
            is_valid_data = False

            if isinstance(cohort_data, pd.DataFrame) and not cohort_data.empty:
                is_valid_data = True
            elif isinstance(cohort_data, (dict, list)) and len(cohort_data) > 0:
                is_valid_data = True

            if is_valid_data:
                st.subheader("ğŸ”¥ ç•™å­˜çƒ­åŠ›å›¾")
                try:
                    # ä½¿ç”¨å¤„ç†è¿‡çš„cohort_dataè€Œä¸æ˜¯åŸå§‹çš„results['cohort_data']
                    heatmap_chart = chart_gen.create_retention_heatmap(cohort_data)
                    st.plotly_chart(heatmap_chart, use_container_width=True)
                except Exception as e:
                    st.error(f"çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")
                    # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                    st.info(f"æ•°æ®ç±»å‹: {type(cohort_data)}")
                    if isinstance(cohort_data, pd.DataFrame):
                        st.info(f"DataFrameå½¢çŠ¶: {cohort_data.shape}")
                        st.info(f"DataFrameåˆ—: {list(cohort_data.columns)}")
                    elif isinstance(cohort_data, (dict, list)):
                        st.info(f"æ•°æ®é•¿åº¦: {len(cohort_data)}")
                        if len(cohort_data) > 0:
                            st.info(f"æ•°æ®ç¤ºä¾‹: {str(cohort_data)[:200]}...")
        
        # ç•™å­˜æ›²çº¿
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“ˆ æ•´ä½“ç•™å­˜æ›²çº¿")

            # å°è¯•ä»å¤šä¸ªå¯èƒ½çš„æ•°æ®æºè·å–ç•™å­˜æ•°æ®
            retention_curve_data = None

            # æ£€æŸ¥ä¸åŒçš„æ•°æ®ç»“æ„
            if 'overall_retention_rates' in results and results['overall_retention_rates']:
                # ä»æ•´ä½“ç•™å­˜ç‡åˆ›å»ºæ›²çº¿æ•°æ®
                overall_rates = results['overall_retention_rates']
                if isinstance(overall_rates, dict):
                    curve_data = []
                    for k, v in overall_rates.items():
                        # å¤„ç†ä¸åŒç±»å‹çš„é”®
                        if isinstance(k, str) and k.startswith('period_'):
                            # å­—ç¬¦ä¸²é”®ï¼Œå¦‚ 'period_0', 'period_1'
                            period_num = int(k.replace('period_', ''))
                            curve_data.append({'period': period_num, 'retention_rate': v})
                        elif isinstance(k, int):
                            # æ•´æ•°é”®ï¼Œç›´æ¥ä½¿ç”¨
                            curve_data.append({'period': k, 'retention_rate': v})
                        elif isinstance(k, str) and k.isdigit():
                            # æ•°å­—å­—ç¬¦ä¸²é”®ï¼Œå¦‚ '0', '1'
                            curve_data.append({'period': int(k), 'retention_rate': v})

                    if curve_data:
                        retention_curve_data = pd.DataFrame(curve_data)
            elif 'cohorts' in results and results['cohorts']:
                # ä»é˜Ÿåˆ—æ•°æ®è®¡ç®—å¹³å‡ç•™å­˜ç‡
                cohorts = results['cohorts']
                if isinstance(cohorts, list) and len(cohorts) > 0:
                    # è®¡ç®—æ‰€æœ‰é˜Ÿåˆ—çš„å¹³å‡ç•™å­˜ç‡
                    # å®‰å…¨åœ°è·å–æ‰€æœ‰é˜Ÿåˆ—çš„ç•™å­˜ç‡æ•°æ®
                    cohort_retention_data = []
                    for cohort in cohorts:
                        if hasattr(cohort, 'retention_rates') and cohort.retention_rates:
                            cohort_retention_data.append(cohort.retention_rates)
                        else:
                            cohort_retention_data.append([])

                    if cohort_retention_data:
                        max_periods = max(len(rates) for rates in cohort_retention_data) if cohort_retention_data else 0
                        avg_rates = []
                        for period in range(max_periods):
                            period_rates = [
                                rates[period]
                                for rates in cohort_retention_data
                                if len(rates) > period
                            ]
                            if period_rates:
                                avg_rates.append({
                                    'period': period,
                                    'retention_rate': sum(period_rates) / len(period_rates)
                                })
                        retention_curve_data = pd.DataFrame(avg_rates)
            elif 'retention_data' in results and results['retention_data'] is not None:
                # åŸæœ‰çš„æ•°æ®ç»“æ„
                retention_data = results['retention_data']
                try:
                    if isinstance(retention_data, pd.DataFrame):
                        retention_curve_data = retention_data
                    elif isinstance(retention_data, (list, dict)):
                        retention_curve_data = pd.DataFrame(retention_data)
                except Exception:
                    pass

            # æ˜¾ç¤ºç•™å­˜æ›²çº¿
            if retention_curve_data is not None and not retention_curve_data.empty:
                if 'period' in retention_curve_data.columns and 'retention_rate' in retention_curve_data.columns:
                    st.line_chart(retention_curve_data.set_index('period')['retention_rate'])
                else:
                    st.info("ç•™å­˜æ•°æ®æ ¼å¼ä¸å®Œæ•´ï¼Œæ— æ³•æ˜¾ç¤ºæ›²çº¿å›¾")
            else:
                st.info("æš‚æ— ç•™å­˜æ›²çº¿æ•°æ®")
        
        with col2:
            st.subheader("ğŸ“Š ç•™å­˜ç‡åˆ†å¸ƒ")

            # å°è¯•ä»å¤šä¸ªå¯èƒ½çš„æ•°æ®æºè·å–åˆ†å¸ƒæ•°æ®
            distribution_data = None

            # æ£€æŸ¥ä¸åŒçš„æ•°æ®ç»“æ„
            if 'cohorts' in results and results['cohorts']:
                # ä»é˜Ÿåˆ—æ•°æ®åˆ›å»ºåˆ†å¸ƒæ•°æ®
                cohorts = results['cohorts']
                if isinstance(cohorts, list) and len(cohorts) > 0:
                    # åˆ›å»ºåŒ…å«æ‰€æœ‰é˜Ÿåˆ—å’Œæ—¶æœŸçš„æ•°æ®
                    dist_data = []
                    for cohort in cohorts:
                        # å®‰å…¨åœ°è®¿é—®CohortDataå¯¹è±¡çš„å±æ€§
                        if hasattr(cohort, 'cohort_period'):
                            cohort_period = cohort.cohort_period
                        else:
                            cohort_period = 'Unknown'

                        if hasattr(cohort, 'retention_rates'):
                            retention_rates = cohort.retention_rates
                        else:
                            retention_rates = []

                        for period_num, rate in enumerate(retention_rates):
                            dist_data.append({
                                'cohort_group': cohort_period,
                                'period_number': period_num,
                                'retention_rate': rate
                            })

                    if dist_data:
                        distribution_data = pd.DataFrame(dist_data)
            elif 'cohort_data' in results and results['cohort_data'] is not None:
                # åŸæœ‰çš„æ•°æ®ç»“æ„
                cohort_data = results['cohort_data']
                try:
                    if isinstance(cohort_data, pd.DataFrame) and not cohort_data.empty:
                        distribution_data = cohort_data
                    elif isinstance(cohort_data, (dict, list)) and len(cohort_data) > 0:
                        distribution_data = pd.DataFrame(cohort_data)
                except Exception:
                    pass

            # æ˜¾ç¤ºç•™å­˜ç‡åˆ†å¸ƒ
            if distribution_data is not None and not distribution_data.empty:
                if 'period_number' in distribution_data.columns and 'retention_rate' in distribution_data.columns:
                    # è®¡ç®—æ¯ä¸ªæ—¶æœŸçš„å¹³å‡ç•™å­˜ç‡
                    avg_retention = distribution_data.groupby('period_number')['retention_rate'].mean()
                    st.bar_chart(avg_retention)
                elif 'period' in distribution_data.columns and 'retention_rate' in distribution_data.columns:
                    # å¤‡ç”¨åˆ—å
                    avg_retention = distribution_data.groupby('period')['retention_rate'].mean()
                    st.bar_chart(avg_retention)
                else:
                    st.info("ç•™å­˜æ•°æ®æ ¼å¼ä¸å®Œæ•´ï¼Œæ— æ³•æ˜¾ç¤ºåˆ†å¸ƒå›¾")
            else:
                st.info("æš‚æ— ç•™å­˜ç‡åˆ†å¸ƒæ•°æ®")


def show_conversion_analysis_page():
    """æ˜¾ç¤ºè½¬åŒ–åˆ†æç»“æœé¡µé¢"""
    st.header("ğŸ”„ è½¬åŒ–æ¼æ–—åˆ†æ")
    
    # åˆå§‹åŒ–åˆ†æå¼•æ“
    if 'conversion_engine' not in st.session_state:
        st.session_state.conversion_engine = ConversionAnalysisEngine()
    if 'chart_generator' not in st.session_state:
        st.session_state.chart_generator = ChartGenerator()
    
    # æ¼æ–—é…ç½®
    with st.expander("ğŸ”§ è½¬åŒ–æ¼æ–—é…ç½®", expanded=True):
        st.write("**å®šä¹‰è½¬åŒ–æ¼æ–—æ­¥éª¤**")
        
        available_events = list(st.session_state.data_summary.get('event_types', {}).keys())
        
        funnel_steps = []
        for i in range(5):  # æœ€å¤š5ä¸ªæ­¥éª¤
            col1, col2 = st.columns([3, 1])
            with col1:
                step_event = st.selectbox(
                    f"æ­¥éª¤ {i+1}",
                    options=[""] + available_events,
                    key=f"funnel_step_{i}",
                    help=f"é€‰æ‹©æ¼æ–—ç¬¬{i+1}æ­¥çš„äº‹ä»¶"
                )
            with col2:
                if step_event:
                    funnel_steps.append(step_event)
                    st.success("âœ“")
                else:
                    break
        
        if len(funnel_steps) < 2:
            st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©2ä¸ªæ­¥éª¤æ¥æ„å»ºè½¬åŒ–æ¼æ–—")
    
    # æ‰§è¡Œè½¬åŒ–åˆ†æ
    if st.button("ğŸš€ å¼€å§‹è½¬åŒ–åˆ†æ", type="primary") and len(funnel_steps) >= 2:
        with st.spinner("æ­£åœ¨è¿›è¡Œè½¬åŒ–åˆ†æ..."):
            try:
                raw_data = st.session_state.raw_data
                engine = st.session_state.conversion_engine
                
                # æ„å»ºè½¬åŒ–æ¼æ–—
                funnel_result = engine.build_conversion_funnel(raw_data, funnel_steps)
                
                # è¯†åˆ«æµå¤±ç‚¹
                bottlenecks = engine.identify_drop_off_points(raw_data, funnel_steps)
                
                st.session_state.conversion_results = {
                    'funnel_data': funnel_result,
                    'bottlenecks': bottlenecks,
                    'funnel_steps': funnel_steps
                }
                
                st.success("âœ… è½¬åŒ–åˆ†æå®Œæˆ!")
                
            except Exception as e:
                st.error(f"âŒ è½¬åŒ–åˆ†æå¤±è´¥: {str(e)}")
    
    # æ˜¾ç¤ºè½¬åŒ–åˆ†æç»“æœ
    if 'conversion_results' in st.session_state:
        results = st.session_state.conversion_results
        chart_gen = st.session_state.chart_generator
        
        st.markdown("---")
        st.subheader("ğŸ“Š è½¬åŒ–åˆ†æç»“æœ")
        
        # è½¬åŒ–æ¼æ–—å›¾
        if 'funnel_data' in results:
            st.subheader("ğŸ”„ è½¬åŒ–æ¼æ–—")
            try:
                funnel_chart = chart_gen.create_funnel_chart(results['funnel_data'])
                st.plotly_chart(funnel_chart, use_container_width=True)
            except Exception as e:
                st.error(f"æ¼æ–—å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")
        
        # è½¬åŒ–æŒ‡æ ‡
        col1, col2, col3 = st.columns(3)
        
        if 'funnel_data' in results and results['funnel_data'] is not None:
            funnel_data = results['funnel_data']

            # æ£€æŸ¥æ•°æ®ç±»å‹å¹¶å¤„ç†
            if isinstance(funnel_data, pd.DataFrame) and not funnel_data.empty:
                is_valid_funnel = True
            elif isinstance(funnel_data, (dict, list)) and len(funnel_data) > 0:
                # å°è¯•è½¬æ¢ä¸ºDataFrame
                try:
                    funnel_data = pd.DataFrame(funnel_data)
                    is_valid_funnel = not funnel_data.empty
                except:
                    is_valid_funnel = False
            else:
                is_valid_funnel = False

            if is_valid_funnel:
                funnel_df = funnel_data  # Use the processed data

                with col1:
                    overall_conversion = funnel_df.iloc[-1]['user_count'] / funnel_df.iloc[0]['user_count'] * 100
                    st.metric("æ•´ä½“è½¬åŒ–ç‡", f"{overall_conversion:.1f}%")

                with col2:
                    total_users = funnel_df.iloc[0]['user_count']
                    st.metric("æ¼æ–—å…¥å£ç”¨æˆ·", f"{total_users:,}")

                with col3:
                    converted_users = funnel_df.iloc[-1]['user_count']
                    st.metric("æœ€ç»ˆè½¬åŒ–ç”¨æˆ·", f"{converted_users:,}")
        
        # ç“¶é¢ˆåˆ†æ
        if 'bottlenecks' in results and results['bottlenecks'] is not None:
            st.subheader("ğŸš¨ è½¬åŒ–ç“¶é¢ˆåˆ†æ")
            bottlenecks_data = results['bottlenecks']

            # å®‰å…¨åœ°å¤„ç†ç“¶é¢ˆæ•°æ®
            try:
                if isinstance(bottlenecks_data, list):
                    for bottleneck in bottlenecks_data:
                        if isinstance(bottleneck, dict):
                            step = bottleneck.get('step', 'æœªçŸ¥æ­¥éª¤')
                            drop_rate = bottleneck.get('drop_rate', 0)
                            st.warning(f"**{step}**: æµå¤±ç‡ {drop_rate:.1f}%")
                        else:
                            st.warning(f"ç“¶é¢ˆä¿¡æ¯: {str(bottleneck)}")
                elif isinstance(bottlenecks_data, dict):
                    # å¦‚æœæ˜¯å•ä¸ªç“¶é¢ˆå­—å…¸
                    step = bottlenecks_data.get('step', 'æœªçŸ¥æ­¥éª¤')
                    drop_rate = bottlenecks_data.get('drop_rate', 0)
                    st.warning(f"**{step}**: æµå¤±ç‡ {drop_rate:.1f}%")
                else:
                    st.info(f"ç“¶é¢ˆæ•°æ®æ ¼å¼: {type(bottlenecks_data).__name__}")
                    st.write(str(bottlenecks_data))
            except Exception as e:
                st.error(f"ç“¶é¢ˆåˆ†ææ˜¾ç¤ºå¤±è´¥: {str(e)}")
                st.info("è¯·æ£€æŸ¥ç“¶é¢ˆæ•°æ®æ ¼å¼")


def show_user_segmentation_page():
    """æ˜¾ç¤ºç”¨æˆ·åˆ†ç¾¤ç»“æœé¡µé¢"""
    st.header("ğŸ‘¥ æ™ºèƒ½ç”¨æˆ·åˆ†ç¾¤")
    
    # åˆå§‹åŒ–åˆ†æå¼•æ“
    if 'segmentation_engine' not in st.session_state:
        st.session_state.segmentation_engine = UserSegmentationEngine()
    if 'advanced_visualizer' not in st.session_state:
        st.session_state.advanced_visualizer = AdvancedVisualizer()
    
    # åˆ†ç¾¤é…ç½®
    with st.expander("ğŸ”§ åˆ†ç¾¤é…ç½®", expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            clustering_method = st.selectbox(
                "èšç±»ç®—æ³•",
                options=["K-Means", "DBSCAN"],
                index=0
            )
        
        with col2:
            n_clusters = st.slider(
                "åˆ†ç¾¤æ•°é‡",
                min_value=2,
                max_value=10,
                value=4,
                help="K-Meansç®—æ³•çš„åˆ†ç¾¤æ•°é‡"
            )
        
        with col3:
            feature_types = st.multiselect(
                "ç‰¹å¾ç±»å‹",
                options=["è¡Œä¸ºç‰¹å¾", "è®¾å¤‡ç‰¹å¾", "åœ°ç†ç‰¹å¾", "æ—¶é—´ç‰¹å¾"],
                default=["è¡Œä¸ºç‰¹å¾", "è®¾å¤‡ç‰¹å¾"]
            )
    
    # æ‰§è¡Œç”¨æˆ·åˆ†ç¾¤
    if st.button("ğŸš€ å¼€å§‹ç”¨æˆ·åˆ†ç¾¤", type="primary"):
        with st.spinner("æ­£åœ¨è¿›è¡Œç”¨æˆ·åˆ†ç¾¤åˆ†æ..."):
            try:
                raw_data = st.session_state.raw_data
                engine = st.session_state.segmentation_engine

                # ç¡®ä¿raw_dataæ˜¯DataFrameæ ¼å¼
                if isinstance(raw_data, list):
                    events_df = pd.DataFrame(raw_data)
                elif isinstance(raw_data, pd.DataFrame):
                    events_df = raw_data
                else:
                    st.error("âŒ æ•°æ®æ ¼å¼ä¸æ”¯æŒ")
                    st.stop()

                # æå–ç”¨æˆ·ç‰¹å¾ - ä½¿ç”¨æ­£ç¡®çš„å‚æ•°
                user_features = engine.extract_user_features(events=events_df)

                if not user_features:
                    st.warning("âš ï¸ æ— æ³•æå–ç”¨æˆ·ç‰¹å¾ï¼Œå¯èƒ½æ˜¯æ•°æ®æ ¼å¼é—®é¢˜")
                    st.info("è¯·æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«å¿…è¦çš„å­—æ®µï¼ˆå¦‚user_pseudo_id, event_nameç­‰ï¼‰")
                    st.stop()

                # æ‰§è¡Œèšç±» - ä¼ å…¥ç”¨æˆ·ç‰¹å¾
                segmentation_result = engine.create_user_segments(
                    user_features=user_features,
                    method=clustering_method.lower().replace('-', ''),
                    n_clusters=n_clusters
                )

                st.session_state.segmentation_results = {
                    'segments': segmentation_result,
                    'user_features': user_features
                }

                st.success("âœ… ç”¨æˆ·åˆ†ç¾¤å®Œæˆ!")

            except Exception as e:
                st.error(f"âŒ ç”¨æˆ·åˆ†ç¾¤å¤±è´¥: {str(e)}")
                st.error(f"é”™è¯¯è¯¦æƒ…: {type(e).__name__}")
                import traceback
                st.text(traceback.format_exc())
    
    # æ˜¾ç¤ºåˆ†ç¾¤ç»“æœ
    if 'segmentation_results' in st.session_state:
        results = st.session_state.segmentation_results
        visualizer = st.session_state.advanced_visualizer
        
        st.markdown("---")
        st.subheader("ğŸ“Š ç”¨æˆ·åˆ†ç¾¤ç»“æœ")
        
        # åˆ†ç¾¤æ¦‚è§ˆ
        if 'segments' in results:
            segmentation_result = results['segments']

            # å®‰å…¨åœ°å¤„ç†SegmentationResultå¯¹è±¡
            if hasattr(segmentation_result, 'segments'):
                # å¦‚æœæ˜¯SegmentationResultå¯¹è±¡ï¼Œè·å–segmentsåˆ—è¡¨
                segments = segmentation_result.segments
            elif isinstance(segmentation_result, list):
                # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
                segments = segmentation_result
            else:
                # å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢
                segments = []
                st.warning(f"æœªçŸ¥çš„åˆ†ç¾¤æ•°æ®ç±»å‹: {type(segmentation_result)}")

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("åˆ†ç¾¤æ•°é‡", len(segments))
            with col2:
                # å®‰å…¨åœ°è®¡ç®—æ€»ç”¨æˆ·æ•°
                total_users = 0
                for seg in segments:
                    if hasattr(seg, 'user_count'):
                        # å¦‚æœæ˜¯UserSegmentå¯¹è±¡
                        total_users += seg.user_count
                    elif isinstance(seg, dict) and 'user_ids' in seg:
                        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
                        total_users += len(seg.get('user_ids', []))
                    elif isinstance(seg, dict) and 'user_count' in seg:
                        # å¦‚æœå­—å…¸ä¸­æœ‰user_count
                        total_users += seg.get('user_count', 0)

                st.metric("æ€»ç”¨æˆ·æ•°", f"{total_users:,}")
            with col3:
                avg_size = total_users / len(segments) if segments else 0
                st.metric("å¹³å‡åˆ†ç¾¤å¤§å°", f"{avg_size:.0f}")
        
        # åˆ†ç¾¤æ•£ç‚¹å›¾
        if 'user_features' in results and results['user_features'] is not None:
            st.subheader("ğŸ¯ ç”¨æˆ·åˆ†ç¾¤å¯è§†åŒ–")
            try:
                # åˆ›å»ºç¤ºä¾‹æ•£ç‚¹å›¾æ•°æ®
                scatter_data = pd.DataFrame({
                    'user_id': [f'user_{i}' for i in range(100)],
                    'segment': [f'åˆ†ç¾¤{i%4+1}' for i in range(100)],
                    'x_feature': np.random.normal(0, 1, 100),
                    'y_feature': np.random.normal(0, 1, 100)
                })
                
                scatter_chart = visualizer.create_user_segmentation_scatter(scatter_data)
                st.plotly_chart(scatter_chart, use_container_width=True)
            except Exception as e:
                st.error(f"æ•£ç‚¹å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")
        
        # åˆ†ç¾¤ç‰¹å¾é›·è¾¾å›¾
        st.subheader("ğŸ•¸ï¸ åˆ†ç¾¤ç‰¹å¾å¯¹æ¯”")
        try:
            # åˆ›å»ºç¤ºä¾‹é›·è¾¾å›¾æ•°æ®
            radar_data = pd.DataFrame({
                'segment': ['åˆ†ç¾¤1', 'åˆ†ç¾¤1', 'åˆ†ç¾¤1', 'åˆ†ç¾¤2', 'åˆ†ç¾¤2', 'åˆ†ç¾¤2'],
                'feature_name': ['æ´»è·ƒåº¦', 'è½¬åŒ–ç‡', 'ç•™å­˜ç‡', 'æ´»è·ƒåº¦', 'è½¬åŒ–ç‡', 'ç•™å­˜ç‡'],
                'feature_value': [0.8, 0.3, 0.6, 0.5, 0.7, 0.4]
            })
            
            radar_chart = visualizer.create_feature_radar_chart(radar_data)
            st.plotly_chart(radar_chart, use_container_width=True)
        except Exception as e:
            st.error(f"é›·è¾¾å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")


def show_path_analysis_page():
    """æ˜¾ç¤ºè·¯å¾„åˆ†æç»“æœé¡µé¢"""
    st.header("ğŸ›¤ï¸ ç”¨æˆ·è¡Œä¸ºè·¯å¾„åˆ†æ")
    
    # åˆå§‹åŒ–åˆ†æå¼•æ“
    if 'path_engine' not in st.session_state:
        st.session_state.path_engine = PathAnalysisEngine()
    if 'advanced_visualizer' not in st.session_state:
        st.session_state.advanced_visualizer = AdvancedVisualizer()
    
    # è·¯å¾„åˆ†æé…ç½®
    with st.expander("ğŸ”§ è·¯å¾„åˆ†æé…ç½®", expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            session_timeout = st.slider(
                "ä¼šè¯è¶…æ—¶(åˆ†é’Ÿ)",
                min_value=10,
                max_value=120,
                value=30,
                help="å®šä¹‰ä¼šè¯çš„è¶…æ—¶æ—¶é—´"
            )
        
        with col2:
            min_path_length = st.slider(
                "æœ€å°è·¯å¾„é•¿åº¦",
                min_value=2,
                max_value=10,
                value=3,
                help="åˆ†æçš„æœ€å°è·¯å¾„é•¿åº¦"
            )
        
        with col3:
            top_paths = st.slider(
                "æ˜¾ç¤ºè·¯å¾„æ•°é‡",
                min_value=5,
                max_value=20,
                value=10,
                help="æ˜¾ç¤ºæœ€å¸¸è§çš„è·¯å¾„æ•°é‡"
            )
    
    # æ‰§è¡Œè·¯å¾„åˆ†æ
    if st.button("ğŸš€ å¼€å§‹è·¯å¾„åˆ†æ", type="primary"):
        with st.spinner("æ­£åœ¨è¿›è¡Œè·¯å¾„åˆ†æ..."):
            try:
                raw_data = st.session_state.raw_data
                engine = st.session_state.path_engine
                
                # è®¾ç½®ä¼šè¯è¶…æ—¶æ—¶é—´
                engine.session_timeout_minutes = session_timeout

                # ç¡®ä¿raw_dataæ˜¯DataFrameæ ¼å¼
                if isinstance(raw_data, list):
                    events_df = pd.DataFrame(raw_data)
                elif isinstance(raw_data, pd.DataFrame):
                    events_df = raw_data
                else:
                    st.error("âŒ æ•°æ®æ ¼å¼ä¸æ”¯æŒ")
                    st.stop()

                # é‡æ„ç”¨æˆ·ä¼šè¯
                sessions = engine.reconstruct_user_sessions(events=events_df)
                
                # è¯†åˆ«è·¯å¾„æ¨¡å¼
                path_analysis_result = engine.identify_path_patterns(
                    sessions=sessions,
                    min_length=min_path_length,
                    max_length=10  # è®¾ç½®æœ€å¤§è·¯å¾„é•¿åº¦
                )

                # æå–å¸¸è§è·¯å¾„ï¼ˆé™åˆ¶æ•°é‡ï¼‰
                common_paths = path_analysis_result.common_patterns[:top_paths] if path_analysis_result.common_patterns else []
                
                st.session_state.path_results = {
                    'sessions': sessions,
                    'common_paths': common_paths
                }
                
                st.success("âœ… è·¯å¾„åˆ†æå®Œæˆ!")
                
            except Exception as e:
                st.error(f"âŒ è·¯å¾„åˆ†æå¤±è´¥: {str(e)}")
    
    # æ˜¾ç¤ºè·¯å¾„åˆ†æç»“æœ
    if 'path_results' in st.session_state:
        results = st.session_state.path_results
        visualizer = st.session_state.advanced_visualizer
        
        st.markdown("---")
        st.subheader("ğŸ“Š è·¯å¾„åˆ†æç»“æœ")
        
        # è·¯å¾„ç»Ÿè®¡
        if 'sessions' in results:
            sessions = results['sessions']
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ€»ä¼šè¯æ•°", f"{len(sessions):,}")
            with col2:
                # å®‰å…¨åœ°å¤„ç†UserSessionå¯¹è±¡çš„path_sequenceå±æ€§
                path_lengths = []
                for s in sessions:
                    if hasattr(s, 'path_sequence') and s.path_sequence:
                        path_lengths.append(len(s.path_sequence))
                    else:
                        path_lengths.append(0)
                avg_length = np.mean(path_lengths) if path_lengths else 0
                st.metric("å¹³å‡è·¯å¾„é•¿åº¦", f"{avg_length:.1f}")
            with col3:
                # å®‰å…¨åœ°å¤„ç†UserSessionå¯¹è±¡çš„path_sequenceå±æ€§
                unique_paths_set = set()
                for s in sessions:
                    if hasattr(s, 'path_sequence') and s.path_sequence:
                        unique_paths_set.add(tuple(s.path_sequence))
                unique_paths = len(unique_paths_set)
                st.metric("å”¯ä¸€è·¯å¾„æ•°", f"{unique_paths:,}")
        
        # ç”¨æˆ·è¡Œä¸ºæµç¨‹å›¾
        st.subheader("ğŸŒŠ ç”¨æˆ·è¡Œä¸ºæµç¨‹")
        try:
            # åˆ›å»ºç¤ºä¾‹æµç¨‹æ•°æ®
            flow_data = pd.DataFrame({
                'source': ['é¦–é¡µ', 'é¦–é¡µ', 'äº§å“é¡µ', 'äº§å“é¡µ', 'è´­ç‰©è½¦'],
                'target': ['äº§å“é¡µ', 'æœç´¢é¡µ', 'è´­ç‰©è½¦', 'è¯¦æƒ…é¡µ', 'ç»“ç®—é¡µ'],
                'value': [100, 50, 80, 30, 60]
            })
            
            flow_chart = visualizer.create_user_behavior_flow(flow_data)
            st.plotly_chart(flow_chart, use_container_width=True)
        except Exception as e:
            st.error(f"æµç¨‹å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")
        
        # å¸¸è§è·¯å¾„åˆ—è¡¨
        if 'common_paths' in results and results['common_paths'] is not None:
            st.subheader("ğŸ” æœ€å¸¸è§è·¯å¾„")
            common_paths_data = results['common_paths']

            # å®‰å…¨åœ°åˆ›å»ºDataFrame
            try:
                if isinstance(common_paths_data, pd.DataFrame):
                    paths_df = common_paths_data
                elif isinstance(common_paths_data, (list, dict)):
                    paths_df = pd.DataFrame(common_paths_data)
                else:
                    paths_df = pd.DataFrame()

                if not paths_df.empty:
                    st.dataframe(paths_df, use_container_width=True)
                else:
                    st.info("æš‚æ— å¸¸è§è·¯å¾„æ•°æ®")

            except Exception as e:
                st.error(f"è·¯å¾„æ•°æ®æ˜¾ç¤ºå¤±è´¥: {str(e)}")
                st.info("è¯·æ£€æŸ¥è·¯å¾„æ•°æ®æ ¼å¼")


def show_comprehensive_report_page():
    """æ˜¾ç¤ºç»¼åˆåˆ†ææŠ¥å‘Šé¡µé¢"""
    st.header("ğŸ“‹ ç»¼åˆåˆ†ææŠ¥å‘Š")
    
    # æŠ¥å‘Šé…ç½®
    with st.expander("ğŸ“Š æŠ¥å‘Šé…ç½®", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            report_sections = st.multiselect(
                "é€‰æ‹©æŠ¥å‘Šç« èŠ‚",
                options=["æ•°æ®æ¦‚è§ˆ", "äº‹ä»¶åˆ†æ", "ç•™å­˜åˆ†æ", "è½¬åŒ–åˆ†æ", "ç”¨æˆ·åˆ†ç¾¤", "è·¯å¾„åˆ†æ"],
                default=["æ•°æ®æ¦‚è§ˆ", "äº‹ä»¶åˆ†æ", "ç•™å­˜åˆ†æ"]
            )
        
        with col2:
            export_format = st.selectbox(
                "å¯¼å‡ºæ ¼å¼",
                options=["HTML", "PDF", "JSON"],
                index=0
            )
    
    # ç”ŸæˆæŠ¥å‘Š
    if st.button("ğŸ“ ç”Ÿæˆç»¼åˆæŠ¥å‘Š", type="primary"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆç»¼åˆæŠ¥å‘Š..."):
            try:
                # æ±‡æ€»æ‰€æœ‰åˆ†æç»“æœ
                report_data = {
                    'data_summary': st.session_state.get('data_summary', {}),
                    'event_analysis': st.session_state.get('event_analysis_results', {}),
                    'retention_analysis': st.session_state.get('retention_results', {}),
                    'conversion_analysis': st.session_state.get('conversion_results', {}),
                    'segmentation_analysis': st.session_state.get('segmentation_results', {}),
                    'path_analysis': st.session_state.get('path_results', {})
                }
                
                st.session_state.comprehensive_report = report_data
                st.success("âœ… ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
                
            except Exception as e:
                st.error(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
    
    # æ˜¾ç¤ºç»¼åˆæŠ¥å‘Š
    if 'comprehensive_report' in st.session_state:
        report = st.session_state.comprehensive_report
        
        st.markdown("---")
        
        # æ•°æ®æ¦‚è§ˆ
        if "æ•°æ®æ¦‚è§ˆ" in report_sections and 'data_summary' in report:
            st.subheader("ğŸ“Š æ•°æ®æ¦‚è§ˆ")
            summary = report['data_summary']
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æ€»äº‹ä»¶æ•°", f"{summary.get('total_events', 0):,}")
            with col2:
                st.metric("ç‹¬ç«‹ç”¨æˆ·æ•°", f"{summary.get('unique_users', 0):,}")
            with col3:
                st.metric("äº‹ä»¶ç±»å‹æ•°", len(summary.get('event_types', {})))
            with col4:
                date_range = summary.get('date_range', {})
                days = (pd.to_datetime(date_range.get('end', '')) - pd.to_datetime(date_range.get('start', ''))).days
                st.metric("æ•°æ®å¤©æ•°", f"{days}å¤©")
        
        # å…³é”®æ´å¯Ÿ
        st.subheader("ğŸ’¡ å…³é”®æ´å¯Ÿ")
        insights = [
            "ç”¨æˆ·æ´»è·ƒåº¦åœ¨å·¥ä½œæ—¥è¾ƒé«˜ï¼Œå‘¨æœ«æœ‰æ‰€ä¸‹é™",
            "ç§»åŠ¨ç«¯ç”¨æˆ·å æ¯”é€æ¸å¢åŠ ï¼Œéœ€è¦ä¼˜åŒ–ç§»åŠ¨ä½“éªŒ",
            "æ–°ç”¨æˆ·ç•™å­˜ç‡åœ¨ç¬¬7å¤©å‡ºç°æ˜æ˜¾ä¸‹é™",
            "è´­ä¹°è½¬åŒ–æ¼æ–—åœ¨è´­ç‰©è½¦ç¯èŠ‚æµå¤±ç‡æœ€é«˜",
            "é«˜ä»·å€¼ç”¨æˆ·ç¾¤ä½“ä¸»è¦é›†ä¸­åœ¨25-35å²å¹´é¾„æ®µ"
        ]
        
        for insight in insights:
            st.info(f"â€¢ {insight}")
        
        # è¡ŒåŠ¨å»ºè®®
        st.subheader("ğŸ¯ è¡ŒåŠ¨å»ºè®®")
        recommendations = [
            "**ä¼˜åŒ–ç§»åŠ¨ç«¯ä½“éªŒ**: é’ˆå¯¹ç§»åŠ¨ç«¯ç”¨æˆ·å¢é•¿è¶‹åŠ¿ï¼Œä¼˜åŒ–ç§»åŠ¨ç«¯ç•Œé¢å’Œäº¤äº’",
            "**æ”¹å–„æ–°ç”¨æˆ·å¼•å¯¼**: åŠ å¼ºæ–°ç”¨æˆ·ç¬¬ä¸€å‘¨çš„å¼•å¯¼å’Œæ¿€åŠ±æœºåˆ¶",
            "**ä¼˜åŒ–è´­ç‰©è½¦æµç¨‹**: ç®€åŒ–è´­ç‰©è½¦åˆ°ç»“ç®—çš„æµç¨‹ï¼Œå‡å°‘ç”¨æˆ·æµå¤±",
            "**ç²¾å‡†è¥é”€**: é’ˆå¯¹é«˜ä»·å€¼ç”¨æˆ·ç¾¤ä½“åˆ¶å®šä¸ªæ€§åŒ–è¥é”€ç­–ç•¥",
            "**æå‡å‘¨æœ«æ´»è·ƒåº¦**: æ¨å‡ºå‘¨æœ«ä¸“å±æ´»åŠ¨å’Œä¼˜æƒ æ¥æå‡ç”¨æˆ·æ´»è·ƒåº¦"
        ]
        
        for rec in recommendations:
            st.success(rec)
        
        # å¯¼å‡ºæŠ¥å‘Š
        st.markdown("---")
        st.subheader("ğŸ“¥ æŠ¥å‘Šå¯¼å‡º")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            export_format_final = st.selectbox(
                "é€‰æ‹©å¯¼å‡ºæ ¼å¼",
                options=["JSON", "PDF", "Excel"],
                index=0,
                key="final_export_format"
            )
        
        with col2:
            include_raw_data = st.checkbox(
                "åŒ…å«åŸå§‹æ•°æ®",
                value=False,
                help="æ˜¯å¦åœ¨å¯¼å‡ºä¸­åŒ…å«åŸå§‹åˆ†ææ•°æ®"
            )
        
        with col3:
            custom_filename = st.text_input(
                "è‡ªå®šä¹‰æ–‡ä»¶å",
                value="",
                placeholder="ç•™ç©ºä½¿ç”¨é»˜è®¤æ–‡ä»¶å"
            )
        
        if st.button(f"ğŸ“¥ å¯¼å‡º{export_format_final}æŠ¥å‘Š", type="primary"):
            with st.spinner(f"æ­£åœ¨å¯¼å‡º{export_format_final}æ ¼å¼æŠ¥å‘Š..."):
                try:
                    # åˆå§‹åŒ–æŠ¥å‘Šå¯¼å‡ºå™¨
                    exporter = ReportExporter()
                    
                    # å‡†å¤‡æŠ¥å‘Šæ•°æ®
                    export_data = {
                        'metadata': {
                            'generated_at': datetime.now().isoformat(),
                            'report_sections': report_sections,
                            'data_summary': report.get('data_summary', {})
                        },
                        'executive_summary': {
                            'key_metrics': {
                                'total_events': report.get('data_summary', {}).get('total_events', 0),
                                'unique_users': report.get('data_summary', {}).get('unique_users', 0),
                                'event_types': len(report.get('data_summary', {}).get('event_types', {}))
                            },
                            'key_insights': insights,
                            'recommendations': recommendations
                        },
                        'detailed_analysis': {}
                    }
                    
                    # æ·»åŠ é€‰ä¸­çš„åˆ†æç»“æœ
                    if "äº‹ä»¶åˆ†æ" in report_sections and 'event_analysis' in report:
                        export_data['detailed_analysis']['event_analysis'] = report['event_analysis']
                    
                    if "ç•™å­˜åˆ†æ" in report_sections and 'retention_analysis' in report:
                        export_data['detailed_analysis']['retention_analysis'] = report['retention_analysis']
                    
                    if "è½¬åŒ–åˆ†æ" in report_sections and 'conversion_analysis' in report:
                        export_data['detailed_analysis']['conversion_analysis'] = report['conversion_analysis']
                    
                    if "ç”¨æˆ·åˆ†ç¾¤" in report_sections and 'segmentation_analysis' in report:
                        export_data['detailed_analysis']['user_segmentation'] = report['segmentation_analysis']
                    
                    if "è·¯å¾„åˆ†æ" in report_sections and 'path_analysis' in report:
                        export_data['detailed_analysis']['path_analysis'] = report['path_analysis']
                    
                    # å¦‚æœé€‰æ‹©åŒ…å«åŸå§‹æ•°æ®
                    if include_raw_data and st.session_state.get('raw_data') is not None:
                        # åªåŒ…å«æ•°æ®æ‘˜è¦ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§
                        raw_data = st.session_state.raw_data
                        export_data['raw_data_summary'] = {
                            'total_records': len(raw_data),
                            'columns': list(raw_data.columns),
                            'sample_records': raw_data.head(5).to_dict('records')
                        }
                    
                    # ç”Ÿæˆæ–‡ä»¶å
                    if custom_filename:
                        filename = f"{custom_filename}.{export_format_final.lower()}"
                    else:
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"comprehensive_report_{timestamp}.{export_format_final.lower()}"
                    
                    output_path = f"reports/{filename}"
                    
                    # æ‰§è¡Œå¯¼å‡º
                    result = exporter.export_report(export_data, export_format_final.lower(), output_path)
                    
                    if result['status'] == 'success':
                        st.success(f"âœ… æŠ¥å‘Šå¯¼å‡ºæˆåŠŸ!")
                        st.info(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {result['file_path']}")
                        st.info(f"ğŸ“ æ–‡ä»¶å¤§å°: {result['file_size'] / 1024:.1f} KB")
                        
                        # æä¾›ä¸‹è½½é“¾æ¥
                        try:
                            with open(result['file_path'], 'rb') as f:
                                file_data = f.read()
                            
                            st.download_button(
                                label=f"â¬‡ï¸ ä¸‹è½½{export_format_final}æŠ¥å‘Š",
                                data=file_data,
                                file_name=filename,
                                mime=get_mime_type(export_format_final.lower())
                            )
                        except Exception as e:
                            st.warning(f"ä¸‹è½½æŒ‰é’®åˆ›å»ºå¤±è´¥: {str(e)}")
                    else:
                        st.error(f"âŒ æŠ¥å‘Šå¯¼å‡ºå¤±è´¥: {result['message']}")
                        
                except Exception as e:
                    st.error(f"âŒ å¯¼å‡ºè¿‡ç¨‹å‡ºé”™: {str(e)}")
    



def show_intelligent_analysis_page():
    """æ˜¾ç¤ºæ™ºèƒ½åˆ†æé¡µé¢"""
    st.header("ğŸš€ æ™ºèƒ½åˆ†æ - ä¸€é”®å®Œæ•´åˆ†æ")
    
    st.markdown("""
    ### åŠŸèƒ½è¯´æ˜
    æ™ºèƒ½åˆ†æåŠŸèƒ½ä½¿ç”¨ç³»ç»Ÿé›†æˆç®¡ç†å™¨ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“åä½œå®Œæˆå®Œæ•´çš„GA4æ•°æ®åˆ†æå·¥ä½œæµç¨‹ã€‚
    åŒ…æ‹¬äº‹ä»¶åˆ†æã€ç•™å­˜åˆ†æã€è½¬åŒ–åˆ†æã€ç”¨æˆ·åˆ†ç¾¤å’Œè·¯å¾„åˆ†æç­‰æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ã€‚
    """)
    
    # åˆ†æé…ç½®
    st.subheader("ğŸ“‹ åˆ†æé…ç½®")
    
    col1, col2 = st.columns(2)
    
    with col1:
        analysis_types = st.multiselect(
            "é€‰æ‹©åˆ†æç±»å‹",
            options=[
                'event_analysis',
                'retention_analysis', 
                'conversion_analysis',
                'user_segmentation',
                'path_analysis'
            ],
            default=[
                'event_analysis',
                'retention_analysis', 
                'conversion_analysis'
            ],
            format_func=lambda x: {
                'event_analysis': 'ğŸ“Š äº‹ä»¶åˆ†æ',
                'retention_analysis': 'ğŸ“ˆ ç•™å­˜åˆ†æ',
                'conversion_analysis': 'ğŸ”„ è½¬åŒ–åˆ†æ',
                'user_segmentation': 'ğŸ‘¥ ç”¨æˆ·åˆ†ç¾¤',
                'path_analysis': 'ğŸ›¤ï¸ è·¯å¾„åˆ†æ'
            }.get(x, x)
        )
        
        enable_parallel = st.checkbox(
            "å¯ç”¨å¹¶è¡Œå¤„ç†",
            value=True,
            help="å¹¶è¡Œæ‰§è¡Œå¤šä¸ªåˆ†æä»»åŠ¡ä»¥æé«˜æ€§èƒ½"
        )
    
    with col2:
        max_workers = st.slider(
            "æœ€å¤§å¹¶è¡Œä»»åŠ¡æ•°",
            min_value=1,
            max_value=8,
            value=4,
            disabled=not enable_parallel
        )
        
        enable_caching = st.checkbox(
            "å¯ç”¨æ™ºèƒ½ç¼“å­˜",
            value=True,
            help="ç¼“å­˜åˆ†æç»“æœä»¥åŠ é€Ÿåç»­æ‰§è¡Œ"
        )
    
    # æ‰§è¡Œåˆ†æ
    st.subheader("ğŸ¯ æ‰§è¡Œåˆ†æ")
    
    if not analysis_types:
        st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©ä¸€ç§åˆ†æç±»å‹")
        return
    
    col1, col2, col3 = st.columns([1, 1, 2])
    
    with col1:
        if st.button("ğŸš€ å¼€å§‹æ™ºèƒ½åˆ†æ", type="primary"):
            execute_intelligent_analysis(analysis_types, enable_parallel, max_workers, enable_caching)
    
    with col2:
        if st.button("ğŸ“Š æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€"):
            show_system_status()
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    if st.session_state.workflow_results:
        show_workflow_results()


def execute_intelligent_analysis(analysis_types, enable_parallel, max_workers, enable_caching):
    """æ‰§è¡Œæ™ºèƒ½åˆ†æ"""
    
    # æ›´æ–°é›†æˆç®¡ç†å™¨é…ç½®
    integration_manager = st.session_state.integration_manager
    integration_manager.config.enable_parallel_processing = enable_parallel
    integration_manager.config.max_workers = max_workers
    integration_manager.config.enable_caching = enable_caching
    
    # åˆ›å»ºè¿›åº¦å®¹å™¨
    progress_container = st.container()
    
    with progress_container:
        st.subheader("âš™ï¸ æ™ºèƒ½åˆ†æè¿›åº¦")
        
        # åˆ›å»ºè¿›åº¦æ¡å’ŒçŠ¶æ€æ˜¾ç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()
        metrics_container = st.empty()
        
        try:
            # å‡†å¤‡ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            upload_dir = Path("data/uploads")
            upload_dir.mkdir(parents=True, exist_ok=True)
            
            # ä»ä¼šè¯çŠ¶æ€è·å–åŸå§‹æ•°æ®å¹¶ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            if st.session_state.raw_data is not None:
                temp_file_path = upload_dir / f"temp_analysis_{int(time.time())}.ndjson"
                
                # å°†DataFrameè½¬æ¢å›NDJSONæ ¼å¼
                with open(temp_file_path, 'w', encoding='utf-8') as f:
                    for _, row in st.session_state.raw_data.iterrows():
                        # é‡æ„åŸå§‹äº‹ä»¶æ ¼å¼
                        event_data = {
                            'event_date': row.get('event_date', ''),
                            'event_timestamp': row.get('event_timestamp', 0),
                            'event_name': row.get('event_name', ''),
                            'user_pseudo_id': row.get('user_pseudo_id', ''),
                            'user_id': row.get('user_id'),
                            'platform': row.get('platform', 'WEB'),
                            'device': row.get('device', {}),
                            'geo': row.get('geo', {}),
                            'traffic_source': row.get('traffic_source', {}),
                            'event_params': row.get('event_params', []),
                            'user_properties': row.get('user_properties', []),
                            'items': row.get('items', [])
                        }
                        f.write(json.dumps(event_data, ensure_ascii=False) + '\n')
                
                status_text.text("ğŸš€ å¯åŠ¨æ™ºèƒ½åˆ†æå·¥ä½œæµç¨‹...")
                progress_bar.progress(10)
                
                # æ‰§è¡Œå®Œæ•´å·¥ä½œæµç¨‹
                start_time = time.time()
                
                result = integration_manager.execute_complete_workflow(
                    file_path=str(temp_file_path),
                    analysis_types=analysis_types
                )
                
                end_time = time.time()
                total_time = end_time - start_time
                
                # æ›´æ–°è¿›åº¦
                progress_bar.progress(100)
                status_text.text("âœ… æ™ºèƒ½åˆ†æå®Œæˆ!")
                
                # æ˜¾ç¤ºæ‰§è¡ŒæŒ‡æ ‡
                with metrics_container.container():
                    col1, col2, col3, col4 = st.columns(4)
                    
                    execution_summary = result['execution_summary']
                    
                    with col1:
                        st.metric(
                            "æ€»æ‰§è¡Œæ—¶é—´",
                            f"{total_time:.2f}ç§’",
                            help="å®Œæ•´å·¥ä½œæµç¨‹çš„æ‰§è¡Œæ—¶é—´"
                        )
                    
                    with col2:
                        st.metric(
                            "æˆåŠŸåˆ†æ",
                            execution_summary['successful_analyses'],
                            help="æˆåŠŸå®Œæˆçš„åˆ†æä»»åŠ¡æ•°é‡"
                        )
                    
                    with col3:
                        st.metric(
                            "å¤„ç†æ•°æ®é‡",
                            f"{execution_summary['data_size']:,}",
                            help="å¤„ç†çš„äº‹ä»¶è®°å½•æ•°é‡"
                        )
                    
                    with col4:
                        processing_rate = execution_summary['data_size'] / total_time if total_time > 0 else 0
                        st.metric(
                            "å¤„ç†é€Ÿç‡",
                            f"{processing_rate:.0f}/ç§’",
                            help="æ¯ç§’å¤„ç†çš„è®°å½•æ•°é‡"
                        )
                
                # ä¿å­˜ç»“æœåˆ°ä¼šè¯çŠ¶æ€
                st.session_state.workflow_results = result
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_file_path.exists():
                    temp_file_path.unlink()
                
                st.success("ğŸ‰ æ™ºèƒ½åˆ†ææˆåŠŸå®Œæˆ!")
                st.rerun()
                
            else:
                st.error("âŒ æ— æ³•è·å–åŸå§‹æ•°æ®ï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡ä»¶")
                
        except Exception as e:
            st.error(f"âŒ æ™ºèƒ½åˆ†æå¤±è´¥: {str(e)}")
            progress_bar.progress(0)
            status_text.text("âŒ åˆ†æå¤±è´¥")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if 'temp_file_path' in locals() and temp_file_path.exists():
                temp_file_path.unlink()


def show_system_status():
    """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
    st.subheader("ğŸ“Š ç³»ç»ŸçŠ¶æ€ç›‘æ§")
    
    integration_manager = st.session_state.integration_manager
    
    # è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€
    health = integration_manager.get_system_health()
    
    # æ˜¾ç¤ºæ•´ä½“çŠ¶æ€
    col1, col2, col3 = st.columns(3)
    
    with col1:
        status_color = {
            'healthy': 'normal',
            'warning': 'inverse', 
            'critical': 'off'
        }.get(health['overall_status'], 'normal')
        
        st.metric(
            "ç³»ç»ŸçŠ¶æ€",
            health['overall_status'].upper(),
            delta_color=status_color
        )
    
    with col2:
        st.metric(
            "æ´»è·ƒå·¥ä½œæµç¨‹",
            health['active_workflows']
        )
    
    with col3:
        st.metric(
            "ç¼“å­˜é¡¹æ•°é‡",
            health['cache_size']
        )
    
    # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
    if health.get('current_metrics'):
        st.subheader("âš¡ æ€§èƒ½æŒ‡æ ‡")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            cpu_usage = health['current_metrics'].get('cpu_usage', 0)
            st.metric(
                "CPUä½¿ç”¨ç‡",
                f"{cpu_usage:.1f}%",
                delta=f"{cpu_usage - health.get('average_metrics', {}).get('cpu_usage', cpu_usage):.1f}%"
            )
        
        with col2:
            memory_usage = health['current_metrics'].get('memory_usage', 0)
            st.metric(
                "å†…å­˜ä½¿ç”¨ç‡", 
                f"{memory_usage:.1f}%",
                delta=f"{memory_usage - health.get('average_metrics', {}).get('memory_usage', memory_usage):.1f}%"
            )
        
        with col3:
            memory_available = health['current_metrics'].get('memory_available', 0)
            st.metric(
                "å¯ç”¨å†…å­˜",
                f"{memory_available:.1f}GB"
            )
    
    # æ˜¾ç¤ºå·¥ä½œæµç¨‹å†å²
    workflow_history = integration_manager.get_execution_history()
    if workflow_history:
        st.subheader("ğŸ“ˆ å·¥ä½œæµç¨‹å†å²")
        
        # åˆ›å»ºå†å²æ•°æ®è¡¨æ ¼
        history_data = []
        for record in workflow_history[-10:]:  # æ˜¾ç¤ºæœ€è¿‘10ä¸ª
            history_data.append({
                'å·¥ä½œæµç¨‹ID': record.get('workflow_id', 'N/A')[:20] + '...',
                'çŠ¶æ€': record.get('status', 'unknown'),
                'å¼€å§‹æ—¶é—´': record.get('start_time', 'N/A'),
                'åˆ†æç±»å‹': ', '.join(record.get('analysis_types', [])),
                'æ–‡ä»¶è·¯å¾„': Path(record.get('file_path', '')).name if record.get('file_path') else 'N/A'
            })
        
        if history_data:
            df = pd.DataFrame(history_data)
            st.dataframe(df, use_container_width=True)
    
    # ç³»ç»Ÿæ“ä½œ
    st.subheader("ğŸ”§ ç³»ç»Ÿæ“ä½œ")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ§¹ æ¸…ç†ç¼“å­˜"):
            integration_manager._cleanup_cache()
            st.success("âœ… ç¼“å­˜å·²æ¸…ç†")
            st.rerun()
    
    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…ç†å†…å­˜"):
            integration_manager._trigger_memory_cleanup()
            st.success("âœ… å†…å­˜å·²æ¸…ç†")
            st.rerun()
    
    with col3:
        if st.button("ğŸ”„ é‡ç½®çŠ¶æ€"):
            integration_manager.reset_execution_state()
            st.success("âœ… æ‰§è¡ŒçŠ¶æ€å·²é‡ç½®")
            st.rerun()


def show_workflow_results():
    """æ˜¾ç¤ºå·¥ä½œæµç¨‹ç»“æœ"""
    st.subheader("ğŸ“‹ åˆ†æç»“æœ")
    
    result = st.session_state.workflow_results
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š ç»“æœæ¦‚è§ˆ", "ğŸ” è¯¦ç»†åˆ†æ", "ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨", "ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š"])
    
    with tab1:
        show_results_overview(result)
    
    with tab2:
        show_detailed_analysis(result)
    
    with tab3:
        show_visualizations(result)
    
    with tab4:
        show_export_options(result)


def show_results_overview(result):
    """æ˜¾ç¤ºç»“æœæ¦‚è§ˆ"""
    st.write("### ğŸ“Š æ‰§è¡Œæ¦‚è§ˆ")
    
    execution_summary = result['execution_summary']
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "å·¥ä½œæµç¨‹ID",
            result['workflow_id'][:12] + "...",
            help=f"å®Œæ•´ID: {result['workflow_id']}"
        )
    
    with col2:
        st.metric(
            "æ€»æ‰§è¡Œæ—¶é—´",
            f"{execution_summary['total_execution_time']:.2f}ç§’"
        )
    
    with col3:
        st.metric(
            "æˆåŠŸåˆ†æ",
            f"{execution_summary['successful_analyses']}/{execution_summary['successful_analyses'] + execution_summary['failed_analyses']}"
        )
    
    with col4:
        st.metric(
            "æ•°æ®è§„æ¨¡",
            f"{execution_summary['data_size']:,} æ¡"
        )
    
    # åˆ†æç»“æœçŠ¶æ€
    st.write("### ğŸ“ˆ åˆ†ææ¨¡å—çŠ¶æ€")
    
    analysis_results = result['analysis_results']
    
    for analysis_type, analysis_result in analysis_results.items():
        with st.expander(f"{'âœ…' if analysis_result.status == 'completed' else 'âŒ'} {analysis_type.replace('_', ' ').title()}",
                        expanded=analysis_result.status == 'completed'):
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write(f"**çŠ¶æ€**: {analysis_result.status}")
                st.write(f"**æ‰§è¡Œæ—¶é—´**: {analysis_result.execution_time:.2f}ç§’")
                st.write(f"**æ´å¯Ÿæ•°é‡**: {len(analysis_result.insights)}")
                st.write(f"**å»ºè®®æ•°é‡**: {len(analysis_result.recommendations)}")
            
            with col2:
                if analysis_result.insights:
                    st.write("**ä¸»è¦æ´å¯Ÿ**:")
                    for insight in analysis_result.insights[:3]:
                        st.write(f"â€¢ {insight}")

                    if len(analysis_result.insights) > 3:
                        st.write(f"... è¿˜æœ‰ {len(analysis_result.insights) - 3} ä¸ªæ´å¯Ÿ")


def show_detailed_analysis(result):
    """æ˜¾ç¤ºè¯¦ç»†åˆ†æ"""
    st.write("### ğŸ” è¯¦ç»†åˆ†æç»“æœ")
    
    analysis_results = result['analysis_results']
    
    # é€‰æ‹©è¦æŸ¥çœ‹çš„åˆ†æ
    selected_analysis = st.selectbox(
        "é€‰æ‹©åˆ†ææ¨¡å—",
        options=list(analysis_results.keys()),
        format_func=lambda x: x.replace('_', ' ').title()
    )
    
    if selected_analysis:
        analysis_result = analysis_results[selected_analysis]
        
        if analysis_result.status == 'completed':
            # æ˜¾ç¤ºæ´å¯Ÿ
            if analysis_result.insights:
                st.write("#### ğŸ’¡ å…³é”®æ´å¯Ÿ")
                for i, insight in enumerate(analysis_result.insights, 1):
                    st.write(f"{i}. {insight}")

            # æ˜¾ç¤ºå»ºè®®
            if analysis_result.recommendations:
                st.write("#### ğŸ¯ è¡ŒåŠ¨å»ºè®®")
                for i, recommendation in enumerate(analysis_result.recommendations, 1):
                    st.write(f"{i}. {recommendation}")

            # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
            if hasattr(analysis_result, 'data') and analysis_result.data:
                st.write("#### ğŸ“Š æ•°æ®æ‘˜è¦")
                data_summary = analysis_result.data
                
                summary_data = []
                for key, value in data_summary.items():
                    if isinstance(value, dict):
                        summary_data.append({
                            'å­—æ®µ': key,
                            'ç±»å‹': value.get('type', 'unknown'),
                            'æè¿°': str(value)[:100] + '...' if len(str(value)) > 100 else str(value)
                        })
                
                if summary_data:
                    df = pd.DataFrame(summary_data)
                    st.dataframe(df, use_container_width=True)
        
        else:
            st.error(f"âŒ åˆ†æå¤±è´¥: {getattr(analysis_result, 'error_message', 'æœªçŸ¥é”™è¯¯')}")


def show_visualizations(result):
    """æ˜¾ç¤ºå¯è§†åŒ–å›¾è¡¨"""
    st.write("### ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨")
    
    visualizations = result.get('visualizations', {})
    
    if not visualizations:
        st.info("ğŸ“Š æš‚æ— å¯è§†åŒ–å›¾è¡¨æ•°æ®")
        return
    
    # é€‰æ‹©è¦æŸ¥çœ‹çš„å¯è§†åŒ–
    viz_options = []
    for analysis_type, viz_data in visualizations.items():
        if viz_data:
            for viz_name in viz_data.keys():
                viz_options.append(f"{analysis_type} - {viz_name}")
    
    if viz_options:
        selected_viz = st.selectbox("é€‰æ‹©å›¾è¡¨", viz_options)
        
        if selected_viz:
            analysis_type, viz_name = selected_viz.split(' - ', 1)
            viz_data = visualizations[analysis_type][viz_name]

            # æ˜¾ç¤ºå›¾è¡¨ä¿¡æ¯
            st.info(f"ğŸ“Š å›¾è¡¨: {viz_name}")

            # æ£€æŸ¥æ˜¯å¦ä¸ºPlotlyå›¾è¡¨å¯¹è±¡
            try:
                import plotly.graph_objects as go

                if isinstance(viz_data, go.Figure):
                    # å¦‚æœæ˜¯Plotlyå›¾è¡¨ï¼Œç›´æ¥æ˜¾ç¤º
                    st.plotly_chart(viz_data, use_container_width=True)
                elif hasattr(viz_data, 'to_json'):
                    # å¦‚æœæœ‰to_jsonæ–¹æ³•ï¼Œå°è¯•è½¬æ¢ä¸ºPlotlyå›¾è¡¨
                    st.plotly_chart(viz_data, use_container_width=True)
                else:
                    # å¦‚æœæ˜¯å…¶ä»–æ•°æ®ç±»å‹ï¼Œå°è¯•æ˜¾ç¤ºä¸ºJSON
                    try:
                        if isinstance(viz_data, dict):
                            st.json(viz_data)
                        else:
                            st.write("å›¾è¡¨æ•°æ®æ ¼å¼:", type(viz_data).__name__)
                            st.write(str(viz_data)[:1000] + "..." if len(str(viz_data)) > 1000 else str(viz_data))
                    except Exception as json_error:
                        st.error(f"æ— æ³•æ˜¾ç¤ºå›¾è¡¨æ•°æ®: {str(json_error)}")
                        st.write("æ•°æ®ç±»å‹:", type(viz_data).__name__)

            except Exception as e:
                st.error(f"å›¾è¡¨æ˜¾ç¤ºå¤±è´¥: {str(e)}")
                st.write("æ•°æ®ç±»å‹:", type(viz_data).__name__)
                # å°è¯•æ˜¾ç¤ºéƒ¨åˆ†æ•°æ®ç”¨äºè°ƒè¯•
                try:
                    st.write("æ•°æ®é¢„è§ˆ:", str(viz_data)[:500] + "..." if len(str(viz_data)) > 500 else str(viz_data))
                except:
                    st.write("æ— æ³•é¢„è§ˆæ•°æ®")
    else:
        st.info("ğŸ“Š æš‚æ— å¯ç”¨çš„å¯è§†åŒ–å›¾è¡¨")


def show_export_options(result):
    """æ˜¾ç¤ºå¯¼å‡ºé€‰é¡¹"""
    st.write("### ğŸ“¥ å¯¼å‡ºåˆ†ææŠ¥å‘Š")
    
    col1, col2 = st.columns(2)
    
    with col1:
        export_format = st.selectbox(
            "å¯¼å‡ºæ ¼å¼",
            options=['json', 'pdf', 'excel'],
            format_func=lambda x: {
                'json': 'JSON æ ¼å¼',
                'pdf': 'PDF æŠ¥å‘Š',
                'excel': 'Excel è¡¨æ ¼'
            }.get(x, x)
        )
        
        include_raw_data = st.checkbox(
            "åŒ…å«åŸå§‹æ•°æ®",
            value=False,
            help="åŒ…å«åŸå§‹æ•°æ®ä¼šå¢åŠ æ–‡ä»¶å¤§å°"
        )
    
    with col2:
        st.write("**å¯¼å‡ºå†…å®¹é¢„è§ˆ**:")
        st.write("â€¢ æ‰§è¡Œæ‘˜è¦")
        st.write("â€¢ åˆ†æç»“æœå’Œæ´å¯Ÿ")
        st.write("â€¢ è¡ŒåŠ¨å»ºè®®")
        st.write("â€¢ ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡")
        if include_raw_data:
            st.write("â€¢ åŸå§‹æ•°æ®")
    
    if st.button("ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š", type="primary"):
        try:
            integration_manager = st.session_state.integration_manager
            workflow_id = result['workflow_id']
            
            # å¯¼å‡ºæŠ¥å‘Š
            file_path = integration_manager.export_workflow_results(
                workflow_id=workflow_id,
                export_format=export_format,
                include_raw_data=include_raw_data
            )
            
            # è¯»å–æ–‡ä»¶å†…å®¹ç”¨äºä¸‹è½½
            with open(file_path, 'rb') as f:
                file_data = f.read()
            
            # æä¾›ä¸‹è½½é“¾æ¥
            st.download_button(
                label=f"â¬‡ï¸ ä¸‹è½½ {export_format.upper()} æŠ¥å‘Š",
                data=file_data,
                file_name=Path(file_path).name,
                mime=get_mime_type(export_format)
            )
            
            st.success(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {Path(file_path).name}")
            
        except Exception as e:
            st.error(f"âŒ å¯¼å‡ºå¤±è´¥: {str(e)}")

if __name__ == "__main__":
    main()