"""
ç³»ç»Ÿé›†æˆæ¼”ç¤º

è¯¥è„šæœ¬æ¼”ç¤ºå®Œæ•´çš„GA4æ•°æ®åˆ†æå·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®å¤„ç†ã€åˆ†æå¼•æ“ã€æ™ºèƒ½ä½“å’Œç•Œé¢ç»„ä»¶çš„é›†æˆ
- å®Œæ•´åˆ†ææµç¨‹çš„æ‰§è¡Œ
- æ€§èƒ½ç›‘æ§å’Œç»“æœå±•ç¤º
- æŠ¥å‘Šç”Ÿæˆå’Œå¯¼å‡º
"""

import json
import time
import tempfile
from pathlib import Path
from datetime import datetime
import pandas as pd
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from system.integration_manager import IntegrationManager, WorkflowConfig
from utils.logger import setup_logger

logger = setup_logger()


def create_sample_ga4_data(num_events: int = 5000) -> str:
    """
    åˆ›å»ºç¤ºä¾‹GA4æ•°æ®æ–‡ä»¶
    
    Args:
        num_events: äº‹ä»¶æ•°é‡
        
    Returns:
        ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    """
    logger.info(f"åˆ›å»ºåŒ…å« {num_events} ä¸ªäº‹ä»¶çš„ç¤ºä¾‹GA4æ•°æ®")
    
    sample_events = []
    
    # äº‹ä»¶ç±»å‹å’Œæƒé‡
    event_types = [
        ("page_view", 0.4),
        ("scroll", 0.2),
        ("click", 0.15),
        ("sign_up", 0.1),
        ("login", 0.08),
        ("purchase", 0.05),
        ("search", 0.02)
    ]
    
    # é¡µé¢åˆ—è¡¨
    pages = [
        "/", "/products", "/about", "/contact", "/blog", 
        "/login", "/signup", "/checkout", "/profile", "/help"
    ]
    
    # ç”¨æˆ·ç±»å‹
    user_types = ["free", "premium", "enterprise"]
    
    # è®¾å¤‡ç±»å‹
    devices = [
        {"category": "desktop", "os": "Windows", "browser": "Chrome"},
        {"category": "mobile", "os": "Android", "browser": "Chrome"},
        {"category": "mobile", "os": "iOS", "browser": "Safari"},
        {"category": "tablet", "os": "iPadOS", "browser": "Safari"}
    ]
    
    # åœ°ç†ä½ç½®
    locations = [
        {"country": "China", "region": "Beijing", "city": "Beijing"},
        {"country": "China", "region": "Shanghai", "city": "Shanghai"},
        {"country": "China", "region": "Guangdong", "city": "Shenzhen"},
        {"country": "United States", "region": "California", "city": "San Francisco"},
        {"country": "United Kingdom", "region": "England", "city": "London"}
    ]
    
    import random
    
    # ç”Ÿæˆäº‹ä»¶æ•°æ®
    for i in range(num_events):
        # é€‰æ‹©äº‹ä»¶ç±»å‹
        rand = random.random()
        cumulative = 0
        event_name = "page_view"
        for event_type, weight in event_types:
            cumulative += weight
            if rand <= cumulative:
                event_name = event_type
                break
        
        # é€‰æ‹©ç”¨æˆ·
        user_id = f"user_{i % (num_events // 10)}"  # 10%çš„ç”¨æˆ·æ•°é‡
        
        # é€‰æ‹©è®¾å¤‡å’Œä½ç½®
        device = random.choice(devices)
        location = random.choice(locations)
        
        # ç”Ÿæˆæ—¶é—´æˆ³ï¼ˆæœ€è¿‘30å¤©å†…ï¼‰
        base_timestamp = 1733097600000000  # 2024-12-01çš„å¾®ç§’æ—¶é—´æˆ³
        timestamp_offset = random.randint(0, 30 * 24 * 60 * 60 * 1000000)  # 30å¤©å†…éšæœº
        event_timestamp = base_timestamp + timestamp_offset
        
        # åˆ›å»ºäº‹ä»¶
        event = {
            "event_date": datetime.fromtimestamp(event_timestamp / 1000000).strftime("%Y%m%d"),
            "event_timestamp": event_timestamp,
            "event_name": event_name,
            "user_pseudo_id": user_id,
            "user_id": user_id if random.random() < 0.3 else None,  # 30%çš„ç”¨æˆ·æœ‰user_id
            "platform": "WEB",
            "device": {
                "category": device["category"],
                "operating_system": device["os"],
                "browser": device["browser"],
                "browser_version": "120.0.0.0",
                "language": "zh-cn" if location["country"] == "China" else "en-us",
                "time_zone_offset_seconds": 28800 if location["country"] == "China" else -28800
            },
            "geo": {
                "continent": "Asia" if location["country"] == "China" else "North America" if location["country"] == "United States" else "Europe",
                "country": location["country"],
                "region": location["region"],
                "city": location["city"]
            },
            "traffic_source": {
                "name": random.choice(["(direct)", "google", "facebook", "twitter", "email"]),
                "medium": random.choice(["(none)", "organic", "cpc", "social", "email"]),
                "source": random.choice(["(direct)", "google.com", "facebook.com", "twitter.com", "newsletter"])
            },
            "event_params": [],
            "user_properties": [
                {
                    "key": "user_type",
                    "value": {
                        "string_value": random.choice(user_types),
                        "set_timestamp_micros": event_timestamp
                    }
                }
            ],
            "items": []
        }
        
        # æ ¹æ®äº‹ä»¶ç±»å‹æ·»åŠ ç‰¹å®šå‚æ•°
        if event_name == "page_view":
            page = random.choice(pages)
            event["event_params"].extend([
                {
                    "key": "page_title",
                    "value": {"string_value": f"Page {page}"}
                },
                {
                    "key": "page_location",
                    "value": {"string_value": f"https://example.com{page}"}
                }
            ])
        elif event_name == "purchase":
            event["event_params"].extend([
                {
                    "key": "currency",
                    "value": {"string_value": "USD"}
                },
                {
                    "key": "value",
                    "value": {"double_value": random.uniform(10, 500)}
                }
            ])
            event["items"] = [
                {
                    "item_id": f"item_{random.randint(1, 100)}",
                    "item_name": f"Product {random.randint(1, 100)}",
                    "item_category": random.choice(["Electronics", "Clothing", "Books", "Home"]),
                    "price": random.uniform(10, 200),
                    "quantity": random.randint(1, 3)
                }
            ]
        elif event_name == "search":
            event["event_params"].append({
                "key": "search_term",
                "value": {"string_value": random.choice(["product", "help", "support", "pricing", "features"])}
            })
        
        sample_events.append(event)
    
    # å†™å…¥ä¸´æ—¶æ–‡ä»¶
    temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.ndjson', delete=False)
    for event in sample_events:
        temp_file.write(json.dumps(event, ensure_ascii=False) + '\n')
    temp_file.close()
    
    logger.info(f"ç¤ºä¾‹æ•°æ®å·²åˆ›å»º: {temp_file.name}")
    return temp_file.name


def demonstrate_complete_workflow():
    """æ¼”ç¤ºå®Œæ•´çš„å·¥ä½œæµç¨‹"""
    logger.info("=" * 80)
    logger.info("å¼€å§‹æ¼”ç¤ºå®Œæ•´çš„GA4æ•°æ®åˆ†æå·¥ä½œæµç¨‹")
    logger.info("=" * 80)
    
    # 1. åˆ›å»ºç¤ºä¾‹æ•°æ®
    logger.info("\n1. åˆ›å»ºç¤ºä¾‹GA4æ•°æ®")
    sample_file = create_sample_ga4_data(3000)
    
    try:
        # 2. åˆå§‹åŒ–é›†æˆç®¡ç†å™¨
        logger.info("\n2. åˆå§‹åŒ–ç³»ç»Ÿé›†æˆç®¡ç†å™¨")
        config = WorkflowConfig(
            enable_parallel_processing=True,
            max_workers=4,
            memory_limit_gb=8.0,
            timeout_minutes=30,
            enable_caching=True,
            cache_ttl_hours=24,
            enable_monitoring=True,
            auto_cleanup=True
        )
        
        integration_manager = IntegrationManager(config)
        logger.info("âœ… é›†æˆç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # 3. æ‰§è¡Œå®Œæ•´å·¥ä½œæµç¨‹
        logger.info("\n3. æ‰§è¡Œå®Œæ•´çš„åˆ†æå·¥ä½œæµç¨‹")
        start_time = time.time()
        
        result = integration_manager.execute_complete_workflow(
            file_path=sample_file,
            analysis_types=[
                'event_analysis',
                'retention_analysis', 
                'conversion_analysis',
                'user_segmentation',
                'path_analysis'
            ]
        )
        
        end_time = time.time()
        total_time = end_time - start_time
        
        logger.info(f"âœ… å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        # 4. å±•ç¤ºæ‰§è¡Œç»“æœ
        logger.info("\n4. åˆ†æç»“æœæ‘˜è¦")
        logger.info("-" * 50)
        
        execution_summary = result['execution_summary']
        logger.info(f"å·¥ä½œæµç¨‹ID: {result['workflow_id']}")
        logger.info(f"æ•°æ®å¤§å°: {execution_summary['data_size']:,} æ¡è®°å½•")
        logger.info(f"æˆåŠŸåˆ†æ: {execution_summary['successful_analyses']}")
        logger.info(f"å¤±è´¥åˆ†æ: {execution_summary['failed_analyses']}")
        logger.info(f"æ€»æ‰§è¡Œæ—¶é—´: {execution_summary['total_execution_time']:.2f}ç§’")
        
        # 5. å±•ç¤ºå„ä¸ªåˆ†æç»“æœ
        logger.info("\n5. å„åˆ†ææ¨¡å—ç»“æœ")
        logger.info("-" * 50)
        
        analysis_results = result['analysis_results']
        for analysis_type, analysis_result in analysis_results.items():
            status_icon = "âœ…" if analysis_result['status'] == 'completed' else "âŒ"
            logger.info(f"{status_icon} {analysis_type}:")
            logger.info(f"   çŠ¶æ€: {analysis_result['status']}")
            logger.info(f"   æ‰§è¡Œæ—¶é—´: {analysis_result['execution_time']:.2f}ç§’")
            logger.info(f"   æ´å¯Ÿæ•°é‡: {len(analysis_result['insights'])}")
            logger.info(f"   å»ºè®®æ•°é‡: {len(analysis_result['recommendations'])}")
            
            # å±•ç¤ºå‰å‡ ä¸ªæ´å¯Ÿ
            if analysis_result['insights']:
                logger.info("   ä¸»è¦æ´å¯Ÿ:")
                for i, insight in enumerate(analysis_result['insights'][:3]):
                    logger.info(f"     â€¢ {insight}")
                if len(analysis_result['insights']) > 3:
                    logger.info(f"     ... è¿˜æœ‰ {len(analysis_result['insights']) - 3} ä¸ªæ´å¯Ÿ")
            
            logger.info("")
        
        # 6. å±•ç¤ºæ•°æ®å¤„ç†ç»“æœ
        logger.info("\n6. æ•°æ®å¤„ç†ç»“æœ")
        logger.info("-" * 50)
        
        data_processing = result['data_processing']
        data_summary = data_processing['summary']
        
        logger.info(f"æ€»äº‹ä»¶æ•°: {data_summary.get('total_events', 0):,}")
        logger.info(f"ç‹¬ç«‹ç”¨æˆ·æ•°: {data_summary.get('unique_users', 0):,}")
        logger.info(f"äº‹ä»¶ç±»å‹æ•°: {len(data_summary.get('event_types', {}))}")
        logger.info(f"å¹³å°åˆ†å¸ƒ: {data_summary.get('platforms', {})}")
        
        date_range = data_summary.get('date_range', {})
        if date_range:
            logger.info(f"æ—¶é—´èŒƒå›´: {date_range.get('start')} - {date_range.get('end')}")
        
        # 7. å±•ç¤ºç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        logger.info("\n7. ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡")
        logger.info("-" * 50)
        
        system_metrics = result['system_metrics']
        if system_metrics:
            logger.info(f"CPUä½¿ç”¨ç‡: {system_metrics.get('cpu_usage', 0):.1f}%")
            logger.info(f"å†…å­˜ä½¿ç”¨ç‡: {system_metrics.get('memory_usage', 0):.1f}%")
            logger.info(f"å¯ç”¨å†…å­˜: {system_metrics.get('memory_available', 0):.1f}GB")
        
        # 8. æµ‹è¯•ç¼“å­˜æ€§èƒ½
        logger.info("\n8. æµ‹è¯•ç¼“å­˜æ€§èƒ½")
        logger.info("-" * 50)
        
        # é‡ç½®æ‰§è¡ŒçŠ¶æ€ä½†ä¿ç•™ç¼“å­˜
        integration_manager.analysis_results.clear()
        integration_manager.current_workflow = None
        
        # å†æ¬¡æ‰§è¡Œç›¸åŒçš„å·¥ä½œæµç¨‹
        cache_start_time = time.time()
        cached_result = integration_manager.execute_complete_workflow(
            file_path=sample_file,
            analysis_types=['event_analysis', 'retention_analysis']
        )
        cache_end_time = time.time()
        cache_time = cache_end_time - cache_start_time
        
        logger.info(f"ç¼“å­˜æ‰§è¡Œæ—¶é—´: {cache_time:.2f}ç§’")
        logger.info(f"æ€§èƒ½æå‡: {((total_time - cache_time) / total_time * 100):.1f}%")
        
        # 9. å¯¼å‡ºç»“æœ
        logger.info("\n9. å¯¼å‡ºåˆ†æç»“æœ")
        logger.info("-" * 50)
        
        workflow_id = result['workflow_id']
        
        try:
            # å¯¼å‡ºJSONæ ¼å¼
            json_file = integration_manager.export_workflow_results(
                workflow_id=workflow_id,
                export_format='json',
                include_raw_data=False
            )
            logger.info(f"âœ… JSONæŠ¥å‘Šå·²å¯¼å‡º: {json_file}")
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = Path(json_file).stat().st_size / 1024 / 1024  # MB
            logger.info(f"   æ–‡ä»¶å¤§å°: {file_size:.2f}MB")
            
        except Exception as e:
            logger.error(f"âŒ æŠ¥å‘Šå¯¼å‡ºå¤±è´¥: {e}")
        
        # 10. å±•ç¤ºç³»ç»Ÿå¥åº·çŠ¶æ€
        logger.info("\n10. ç³»ç»Ÿå¥åº·çŠ¶æ€")
        logger.info("-" * 50)
        
        health = integration_manager.get_system_health()
        status_icon = {"healthy": "âœ…", "warning": "âš ï¸", "critical": "âŒ"}.get(health['overall_status'], "â“")
        
        logger.info(f"æ•´ä½“çŠ¶æ€: {status_icon} {health['overall_status']}")
        logger.info(f"æ´»è·ƒå·¥ä½œæµç¨‹: {health['active_workflows']}")
        logger.info(f"æ€»å·¥ä½œæµç¨‹æ•°: {health['total_workflows']}")
        logger.info(f"ç¼“å­˜é¡¹æ•°é‡: {health['cache_size']}")
        logger.info(f"ç›‘æ§çŠ¶æ€: {'å¯ç”¨' if health['monitoring_enabled'] else 'ç¦ç”¨'}")
        
        if health['average_metrics']:
            logger.info(f"å¹³å‡CPUä½¿ç”¨ç‡: {health['average_metrics']['cpu_usage']:.1f}%")
            logger.info(f"å¹³å‡å†…å­˜ä½¿ç”¨ç‡: {health['average_metrics']['memory_usage']:.1f}%")
        
        # 11. æ¼”ç¤ºå¹¶å‘å¤„ç†èƒ½åŠ›
        logger.info("\n11. æ¼”ç¤ºå¹¶å‘å¤„ç†èƒ½åŠ›")
        logger.info("-" * 50)
        
        demonstrate_concurrent_processing(integration_manager, sample_file)
        
        # 12. æ¸…ç†å’Œå…³é—­
        logger.info("\n12. ç³»ç»Ÿæ¸…ç†å’Œå…³é—­")
        logger.info("-" * 50)
        
        integration_manager.shutdown()
        logger.info("âœ… ç³»ç»Ÿå·²æ­£å¸¸å…³é—­")
        
        logger.info("\n" + "=" * 80)
        logger.info("å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤ºå®Œæˆï¼")
        logger.info("=" * 80)
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import os
        if os.path.exists(sample_file):
            os.unlink(sample_file)
            logger.info(f"ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†: {sample_file}")


def demonstrate_concurrent_processing(integration_manager, sample_file):
    """æ¼”ç¤ºå¹¶å‘å¤„ç†èƒ½åŠ›"""
    import threading
    import queue
    
    logger.info("å¯åŠ¨3ä¸ªå¹¶å‘åˆ†æä»»åŠ¡...")
    
    results_queue = queue.Queue()
    start_time = time.time()
    
    def run_concurrent_analysis(thread_id):
        try:
            # åˆ›å»ºç‹¬ç«‹çš„é›†æˆç®¡ç†å™¨å®ä¾‹
            thread_config = WorkflowConfig(
                enable_parallel_processing=False,  # é¿å…åµŒå¥—å¹¶è¡Œ
                max_workers=1,
                timeout_minutes=5,
                enable_caching=True
            )
            thread_manager = IntegrationManager(thread_config)
            
            result = thread_manager.execute_complete_workflow(
                file_path=sample_file,
                analysis_types=['event_analysis']
            )
            
            results_queue.put((thread_id, 'success', result['execution_summary']['total_execution_time']))
            
        except Exception as e:
            results_queue.put((thread_id, 'error', str(e)))
    
    # å¯åŠ¨å¹¶å‘çº¿ç¨‹
    threads = []
    for i in range(3):
        thread = threading.Thread(target=run_concurrent_analysis, args=(i,))
        threads.append(thread)
        thread.start()
    
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in threads:
        thread.join(timeout=60)
    
    end_time = time.time()
    total_concurrent_time = end_time - start_time
    
    # æ”¶é›†ç»“æœ
    successful_tasks = 0
    failed_tasks = 0
    total_task_time = 0
    
    while not results_queue.empty():
        thread_id, status, result = results_queue.get()
        if status == 'success':
            successful_tasks += 1
            total_task_time += result
            logger.info(f"  çº¿ç¨‹ {thread_id}: âœ… æˆåŠŸ ({result:.2f}ç§’)")
        else:
            failed_tasks += 1
            logger.info(f"  çº¿ç¨‹ {thread_id}: âŒ å¤±è´¥ - {result}")
    
    logger.info(f"å¹¶å‘æ‰§è¡Œç»“æœ:")
    logger.info(f"  æ€»æ—¶é—´: {total_concurrent_time:.2f}ç§’")
    logger.info(f"  æˆåŠŸä»»åŠ¡: {successful_tasks}")
    logger.info(f"  å¤±è´¥ä»»åŠ¡: {failed_tasks}")
    if successful_tasks > 0:
        logger.info(f"  å¹³å‡ä»»åŠ¡æ—¶é—´: {total_task_time / successful_tasks:.2f}ç§’")
        logger.info(f"  å¹¶å‘æ•ˆç‡: {(total_task_time / total_concurrent_time):.1f}x")


def demonstrate_error_handling():
    """æ¼”ç¤ºé”™è¯¯å¤„ç†èƒ½åŠ›"""
    logger.info("\n" + "=" * 80)
    logger.info("æ¼”ç¤ºé”™è¯¯å¤„ç†å’Œæ¢å¤èƒ½åŠ›")
    logger.info("=" * 80)
    
    integration_manager = IntegrationManager()
    
    # 1. æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯
    logger.info("\n1. æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯å¤„ç†")
    try:
        result = integration_manager.execute_complete_workflow(
            file_path="nonexistent_file.ndjson",
            analysis_types=['event_analysis']
        )
        logger.error("âŒ åº”è¯¥æŠ›å‡ºå¼‚å¸¸ä½†æ²¡æœ‰")
    except Exception as e:
        logger.info(f"âœ… æ­£ç¡®æ•è·æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯: {type(e).__name__}")
    
    # 2. æµ‹è¯•æ— æ•ˆåˆ†æç±»å‹
    logger.info("\n2. æµ‹è¯•æ— æ•ˆåˆ†æç±»å‹å¤„ç†")
    
    # åˆ›å»ºæœ€å°æœ‰æ•ˆæ•°æ®æ–‡ä»¶
    minimal_data = [
        {
            "event_date": "20241201",
            "event_timestamp": 1733097600000000,
            "event_name": "page_view",
            "user_pseudo_id": "user_1"
        }
    ]
    
    temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.ndjson', delete=False)
    for event in minimal_data:
        temp_file.write(json.dumps(event) + '\n')
    temp_file.close()
    
    try:
        result = integration_manager.execute_complete_workflow(
            file_path=temp_file.name,
            analysis_types=['invalid_analysis_type']
        )
        
        # éªŒè¯é”™è¯¯è¢«æ­£ç¡®å¤„ç†
        analysis_results = result['analysis_results']
        if 'invalid_analysis_type' in analysis_results:
            status = analysis_results['invalid_analysis_type']['status']
            if status == 'failed':
                logger.info("âœ… æ­£ç¡®å¤„ç†æ— æ•ˆåˆ†æç±»å‹é”™è¯¯")
            else:
                logger.error(f"âŒ é”™è¯¯å¤„ç†ä¸æ­£ç¡®ï¼ŒçŠ¶æ€: {status}")
        
    except Exception as e:
        logger.error(f"âŒ æ„å¤–é”™è¯¯: {e}")
    
    finally:
        import os
        if os.path.exists(temp_file.name):
            os.unlink(temp_file.name)
    
    integration_manager.shutdown()
    logger.info("âœ… é”™è¯¯å¤„ç†æ¼”ç¤ºå®Œæˆ")


def demonstrate_performance_optimization():
    """æ¼”ç¤ºæ€§èƒ½ä¼˜åŒ–ç‰¹æ€§"""
    logger.info("\n" + "=" * 80)
    logger.info("æ¼”ç¤ºæ€§èƒ½ä¼˜åŒ–ç‰¹æ€§")
    logger.info("=" * 80)
    
    # åˆ›å»ºå¤§æ•°æ®é›†
    logger.info("\n1. åˆ›å»ºå¤§æ•°æ®é›†è¿›è¡Œæ€§èƒ½æµ‹è¯•")
    large_sample_file = create_sample_ga4_data(10000)
    
    try:
        # æµ‹è¯•ä¸²è¡Œvså¹¶è¡Œæ€§èƒ½
        logger.info("\n2. æ¯”è¾ƒä¸²è¡Œä¸å¹¶è¡Œæ‰§è¡Œæ€§èƒ½")
        
        # ä¸²è¡Œæ‰§è¡Œ
        logger.info("æ‰§è¡Œä¸²è¡Œåˆ†æ...")
        serial_config = WorkflowConfig(enable_parallel_processing=False, max_workers=1)
        serial_manager = IntegrationManager(serial_config)
        
        serial_start = time.time()
        serial_result = serial_manager.execute_complete_workflow(
            file_path=large_sample_file,
            analysis_types=['event_analysis', 'retention_analysis', 'conversion_analysis']
        )
        serial_time = time.time() - serial_start
        
        # å¹¶è¡Œæ‰§è¡Œ
        logger.info("æ‰§è¡Œå¹¶è¡Œåˆ†æ...")
        parallel_config = WorkflowConfig(enable_parallel_processing=True, max_workers=4)
        parallel_manager = IntegrationManager(parallel_config)
        
        parallel_start = time.time()
        parallel_result = parallel_manager.execute_complete_workflow(
            file_path=large_sample_file,
            analysis_types=['event_analysis', 'retention_analysis', 'conversion_analysis']
        )
        parallel_time = time.time() - parallel_start
        
        # æ€§èƒ½å¯¹æ¯”
        logger.info(f"\næ€§èƒ½å¯¹æ¯”ç»“æœ:")
        logger.info(f"ä¸²è¡Œæ‰§è¡Œæ—¶é—´: {serial_time:.2f}ç§’")
        logger.info(f"å¹¶è¡Œæ‰§è¡Œæ—¶é—´: {parallel_time:.2f}ç§’")
        
        if parallel_time < serial_time:
            speedup = serial_time / parallel_time
            improvement = ((serial_time - parallel_time) / serial_time) * 100
            logger.info(f"âœ… å¹¶è¡ŒåŠ é€Ÿæ¯”: {speedup:.2f}x")
            logger.info(f"âœ… æ€§èƒ½æå‡: {improvement:.1f}%")
        else:
            logger.info("âš ï¸ å¹¶è¡Œæ‰§è¡Œæœªæ˜¾ç¤ºæ€§èƒ½ä¼˜åŠ¿ï¼ˆå¯èƒ½ç”±äºæ•°æ®é‡è¾ƒå°ï¼‰")
        
        # æ¸…ç†
        serial_manager.shutdown()
        parallel_manager.shutdown()
        
    finally:
        import os
        if os.path.exists(large_sample_file):
            os.unlink(large_sample_file)
    
    logger.info("âœ… æ€§èƒ½ä¼˜åŒ–æ¼”ç¤ºå®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹GA4æ•°æ®åˆ†æå¹³å°ç³»ç»Ÿé›†æˆæ¼”ç¤º")
    
    try:
        # 1. å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤º
        demonstrate_complete_workflow()
        
        # 2. é”™è¯¯å¤„ç†æ¼”ç¤º
        demonstrate_error_handling()
        
        # 3. æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º
        demonstrate_performance_optimization()
        
        logger.info("\nğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        logger.info("\nä¸»è¦ç‰¹æ€§å±•ç¤º:")
        logger.info("âœ… å®Œæ•´çš„GA4æ•°æ®åˆ†æå·¥ä½œæµç¨‹")
        logger.info("âœ… å¤šæ™ºèƒ½ä½“åä½œåˆ†æ")
        logger.info("âœ… å¹¶è¡Œå¤„ç†å’Œæ€§èƒ½ä¼˜åŒ–")
        logger.info("âœ… æ™ºèƒ½ç¼“å­˜æœºåˆ¶")
        logger.info("âœ… å®æ—¶æ€§èƒ½ç›‘æ§")
        logger.info("âœ… é”™è¯¯å¤„ç†å’Œæ¢å¤")
        logger.info("âœ… ç»“æœå¯¼å‡ºå’ŒæŠ¥å‘Šç”Ÿæˆ")
        logger.info("âœ… ç³»ç»Ÿå¥åº·ç›‘æ§")
        logger.info("âœ… èµ„æºç®¡ç†å’Œæ¸…ç†")
        
    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    logger.info("\nğŸ‘‹ æ¼”ç¤ºç»“æŸ")


if __name__ == "__main__":
    main()