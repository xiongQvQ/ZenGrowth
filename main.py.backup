"""
ç”¨æˆ·è¡Œä¸ºåˆ†ææ™ºèƒ½ä½“å¹³å°ä¸»å…¥å£
åŸºäºCrewAIå’ŒStreamlitçš„å¤šæ™ºèƒ½ä½“åä½œåˆ†æç³»ç»Ÿ
"""

import streamlit as st
import sys
import os
import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import Optional, Dict, Any
import time
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# æ—¶é—´ç²’åº¦å’Œå‘¨æœŸçš„ä¸­è‹±æ–‡æ˜ å°„
TIME_GRANULARITY_MAPPING = {
    "æ—¥": "daily",
    "å‘¨": "weekly",
    "æœˆ": "monthly",
    "Daily": "daily",
    "Weekly": "weekly",
    "Monthly": "monthly",
    "day": "daily",
    "week": "weekly",
    "month": "monthly"
}

COHORT_PERIOD_MAPPING = {
    "æ—¥": "daily",
    "å‘¨": "weekly",
    "æœˆ": "monthly",
    "Daily": "daily",
    "Weekly": "weekly",
    "Monthly": "monthly",
    "æ—¥ç•™å­˜": "daily",
    "å‘¨ç•™å­˜": "weekly",
    "æœˆç•™å­˜": "monthly",
    "Daily Retention": "daily",
    "Weekly Retention": "weekly",
    "Monthly Retention": "monthly"
}

def translate_time_granularity(chinese_term: str) -> str:
    """å°†ä¸­æ–‡æ—¶é—´ç²’åº¦è½¬æ¢ä¸ºè‹±æ–‡"""
    return TIME_GRANULARITY_MAPPING.get(chinese_term, chinese_term)

def translate_cohort_period(chinese_term: str) -> str:
    """å°†ä¸­æ–‡é˜Ÿåˆ—å‘¨æœŸè½¬æ¢ä¸ºè‹±æ–‡"""
    return COHORT_PERIOD_MAPPING.get(chinese_term, chinese_term)

from config.settings import settings, validate_config
from utils.logger import setup_logger
from config.llm_provider_manager import get_provider_manager
from tools.ga4_data_parser import GA4DataParser
from tools.data_validator import DataValidator
from tools.data_storage_manager import DataStorageManager
from engines.event_analysis_engine import EventAnalysisEngine
from engines.retention_analysis_engine import RetentionAnalysisEngine
from engines.conversion_analysis_engine import ConversionAnalysisEngine
from engines.user_segmentation_engine import UserSegmentationEngine
from engines.path_analysis_engine import PathAnalysisEngine
from visualization.chart_generator import ChartGenerator
from visualization.advanced_visualizer import AdvancedVisualizer
from utils.report_exporter import ReportExporter
from utils.config_manager import config_manager
from utils.i18n import t, i18n
from system.integration_manager import IntegrationManager, WorkflowConfig

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title=settings.app_title,
    page_icon=settings.app_icon,
    layout="wide",
    initial_sidebar_state="expanded"
)


def main():
    """ä¸»åº”ç”¨å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger()
    
    # éªŒè¯é…ç½®
    if not validate_config():
        st.error(t("settings.system_config_incomplete"))
        st.stop()
    
    # æ£€æŸ¥æä¾›å•†å¥åº·çŠ¶æ€
    check_provider_health()
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    initialize_session_state()
    
    # åº”ç”¨æ ‡é¢˜
    st.title(f"{settings.app_icon} {t('app.title', settings.app_title)}")
    st.markdown("---")
    
    # ä¾§è¾¹æ å¯¼èˆª
    with st.sidebar:
        st.header(t("app.navigation"))
        
        # æ˜¾ç¤ºæ•°æ®çŠ¶æ€
        show_data_status()
        
        page = st.selectbox(
            t("app.select_module"),
            [
                t("navigation.data_upload"),
                t("navigation.intelligent_analysis"),
                t("navigation.event_analysis"), 
                t("navigation.retention_analysis"),
                t("navigation.conversion_analysis"),
                t("navigation.user_segmentation"),
                t("navigation.path_analysis"),
                t("navigation.comprehensive_report"),
                t("navigation.system_settings")
            ]
        )
    
    # ä¸»å†…å®¹åŒºåŸŸ  
    if page == t("navigation.data_upload"):
        show_data_upload_page()
        
    elif page == t("navigation.intelligent_analysis"):
        if not st.session_state.data_loaded:
            st.warning(f"âš ï¸ {t('common.warning')}: {t('data_upload.upload_ga4_file')}")
        else:
            show_intelligent_analysis_page()
        
    elif page == t("navigation.event_analysis"):
        if not st.session_state.data_loaded:
            st.warning(f"âš ï¸ {t('common.warning')}: {t('data_upload.upload_ga4_file')}")
        else:
            show_event_analysis_page()
        
    elif page == t("navigation.retention_analysis"):
        if not st.session_state.data_loaded:
            st.warning(f"âš ï¸ {t('common.warning')}: {t('data_upload.upload_ga4_file')}")
        else:
            show_retention_analysis_page()
        
    elif page == t("navigation.conversion_analysis"):
        if not st.session_state.data_loaded:
            st.warning(f"âš ï¸ {t('common.warning')}: {t('data_upload.upload_ga4_file')}")
        else:
            show_conversion_analysis_page()
        
    elif page == t("navigation.user_segmentation"):
        if not st.session_state.data_loaded:
            st.warning(f"âš ï¸ {t('common.warning')}: {t('data_upload.upload_ga4_file')}")
        else:
            show_user_segmentation_page()
        
    elif page == t("navigation.path_analysis"):
        if not st.session_state.data_loaded:
            st.warning(f"âš ï¸ {t('common.warning')}: {t('data_upload.upload_ga4_file')}")
        else:
            show_path_analysis_page()
        
    elif page == t("navigation.comprehensive_report"):
        if not st.session_state.data_loaded:
            st.warning(f"âš ï¸ {t('common.warning')}: {t('data_upload.upload_ga4_file')}")
        else:
            show_comprehensive_report_page()
        
    elif page == t("navigation.system_settings"):
        st.header(t("settings.system_config"))
        show_system_settings()
    
    # é¡µè„š
    st.markdown("---")
    st.markdown(
        f"ğŸ’¡ **{t('common.tip')}**: {t('app.usage_hint')}"
    )


def check_provider_health():
    """æ£€æŸ¥æä¾›å•†å¥åº·çŠ¶æ€"""
    if 'provider_health_checked' not in st.session_state:
        try:
            # å¿«é€Ÿæµ‹è¯• LLM æ˜¯å¦å¯ç”¨ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„å¥åº·æ£€æŸ¥
            with st.spinner(f"ğŸ”§ {t('settings.llm_provider_checking')}..."):
                from config.crew_config import get_llm
                llm = get_llm()
                # ç®€å•æµ‹è¯•è°ƒç”¨
                response = llm.invoke("Test")
                
                st.session_state.provider_health_checked = True
                st.session_state.healthy_providers = ["volcano"]  # å‡è®¾ volcano å¯ç”¨
                st.success(t("settings.llm_provider_check_complete"))
                
        except Exception as e:
            # ä¸é˜»æ­¢åº”ç”¨å¯åŠ¨ï¼Œåªæ˜¾ç¤ºè­¦å‘Š
            st.warning(f"{t('settings.llm_provider_check_failed')}: {e}")
            st.info(t("settings.app_still_usable"))
            st.info(t("settings.check_env_config"))
            
            st.session_state.provider_health_checked = True
            st.session_state.healthy_providers = []


def initialize_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    if 'data_loaded' not in st.session_state:
        st.session_state.data_loaded = False
    if 'raw_data' not in st.session_state:
        st.session_state.raw_data = None
    if 'processed_data' not in st.session_state:
        st.session_state.processed_data = None
    if 'data_summary' not in st.session_state:
        st.session_state.data_summary = None
    if 'validation_report' not in st.session_state:
        st.session_state.validation_report = None
    if 'storage_manager' not in st.session_state:
        st.session_state.storage_manager = DataStorageManager()
    if 'integration_manager' not in st.session_state:
        # åˆå§‹åŒ–ç³»ç»Ÿé›†æˆç®¡ç†å™¨
        config = WorkflowConfig(
            enable_parallel_processing=True,
            max_workers=4,
            memory_limit_gb=8.0,
            timeout_minutes=30,
            enable_caching=True,
            cache_ttl_hours=24,
            enable_monitoring=True,
            auto_cleanup=True
        )
        st.session_state.integration_manager = IntegrationManager(config)
        # ç¡®ä¿é›†æˆç®¡ç†å™¨ä½¿ç”¨ç›¸åŒçš„å­˜å‚¨ç®¡ç†å™¨å¹¶é‡æ–°åˆå§‹åŒ–å¼•æ“
        st.session_state.integration_manager.storage_manager = st.session_state.storage_manager
        
        # é‡æ–°åˆå§‹åŒ–åˆ†æå¼•æ“ä»¥ä½¿ç”¨æ­£ç¡®çš„å­˜å‚¨ç®¡ç†å™¨
        from engines.event_analysis_engine import EventAnalysisEngine
        from engines.retention_analysis_engine import RetentionAnalysisEngine
        from engines.conversion_analysis_engine import ConversionAnalysisEngine
        
        st.session_state.integration_manager.event_engine = EventAnalysisEngine(st.session_state.storage_manager)
        st.session_state.integration_manager.retention_engine = RetentionAnalysisEngine(st.session_state.storage_manager)
        st.session_state.integration_manager.conversion_engine = ConversionAnalysisEngine(st.session_state.storage_manager)
    if 'workflow_results' not in st.session_state:
        st.session_state.workflow_results = None


def show_data_status():
    """æ˜¾ç¤ºæ•°æ®çŠ¶æ€"""
    st.markdown(f"### {t('data_upload.data_status')}")
    
    if st.session_state.data_loaded:
        st.success(t("data_upload.data_loaded"))
        if st.session_state.data_summary:
            summary = st.session_state.data_summary
            st.write(f"ğŸ“… **{t('metrics.time_range')}**: {summary.get('date_range', {}).get('start', 'N/A')} - {summary.get('date_range', {}).get('end', 'N/A')}")
            st.write(f"ğŸ‘¥ **{t('metrics.unique_users')}**: {summary.get('unique_users', 0):,}")
            st.write(f"ğŸ“ **{t('metrics.total_events')}**: {summary.get('total_events', 0):,}")
            st.write(f"ğŸ¯ **{t('metrics.event_types_count')}**: {len(summary.get('event_types', {}))}")
        
        if st.button(t("data_upload.clear_data"), type="secondary"):
            clear_session_data()
            st.rerun()
    else:
        st.info(t("data_upload.no_data"))


def clear_session_data():
    """æ¸…é™¤ä¼šè¯æ•°æ®"""
    st.session_state.data_loaded = False
    st.session_state.raw_data = None
    st.session_state.processed_data = None
    st.session_state.data_summary = None
    st.session_state.validation_report = None
    st.session_state.storage_manager = DataStorageManager()


def get_mime_type(format_type: str) -> str:
    """è·å–MIMEç±»å‹"""
    mime_types = {
        'json': 'application/json',
        'pdf': 'application/pdf',
        'excel': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    }
    return mime_types.get(format_type, 'application/octet-stream')


def show_data_upload_page():
    """æ˜¾ç¤ºæ•°æ®ä¸Šä¼ é¡µé¢"""
    st.header(t("data_upload.title"))
    
    # æ–‡ä»¶ä¸Šä¼ è¯´æ˜
    with st.expander(t("data_upload.usage_instructions"), expanded=False):
        st.markdown("""
        ### æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        - **GA4 NDJSONæ ¼å¼**: ä»Google Analytics 4å¯¼å‡ºçš„NDJSONæ–‡ä»¶
        - **æ–‡ä»¶å¤§å°é™åˆ¶**: æœ€å¤§ {max_size}MB
        
        ### æ•°æ®è¦æ±‚
        - æ–‡ä»¶å¿…é¡»åŒ…å«å®Œæ•´çš„äº‹ä»¶æ•°æ®ç»“æ„
        - æ”¯æŒçš„äº‹ä»¶ç±»å‹: page_view, sign_up, login, purchase, searchç­‰
        - å¿…éœ€å­—æ®µ: event_date, event_timestamp, event_name, user_pseudo_idç­‰
        
        ### å¤„ç†æµç¨‹
        1. ä¸Šä¼ æ–‡ä»¶å¹¶éªŒè¯æ ¼å¼
        2. è§£æäº‹ä»¶æ•°æ®å’Œç”¨æˆ·ä¿¡æ¯
        3. æ•°æ®è´¨é‡æ£€æŸ¥å’Œæ¸…æ´—
        4. ç”Ÿæˆæ•°æ®æ‘˜è¦æŠ¥å‘Š
        """.format(max_size=settings.max_file_size_mb))
    
    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    st.subheader(t("data_upload.file_upload"))
    
    uploaded_file = st.file_uploader(
        t("data_upload.upload_ga4_file"),
        type=['ndjson', 'json', 'jsonl'],
        help=f"{t('data_upload.supported_formats')} (æœ€å¤§{settings.max_file_size_mb}MB)"
    )
    
    if uploaded_file is not None:
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size_mb = uploaded_file.size / (1024 * 1024)
        
        if file_size_mb > settings.max_file_size_mb:
            st.error(t("data_upload.file_size_exceeded").format(size=f"{file_size_mb:.1f}", limit=settings.max_file_size_mb))
            return
        
        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        st.info(f"{t('data_upload.file_name')}: {uploaded_file.name}")
        st.info(f"{t('data_upload.file_size')}: {file_size_mb:.2f}MB")
        
        # å¤„ç†æŒ‰é’®
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            if st.button(t("data_upload.start_processing"), type="primary"):
                process_uploaded_file(uploaded_file)
        
        with col2:
            if st.button(t("data_upload.preview_file")):
                preview_file(uploaded_file)
    
    # æ˜¾ç¤ºå¤„ç†ç»“æœ
    if st.session_state.data_loaded:
        show_processing_results()


def preview_file(uploaded_file):
    """é¢„è§ˆä¸Šä¼ çš„æ–‡ä»¶"""
    st.subheader(t("data_upload.file_preview"))
    
    try:
        # è¯»å–å‰å‡ è¡Œè¿›è¡Œé¢„è§ˆ
        content = uploaded_file.read().decode('utf-8')
        lines = content.split('\n')[:5]  # åªæ˜¾ç¤ºå‰5è¡Œ
        
        st.write(f"**{t('data_upload.file_lines')}**: ~{len(content.split())}")
        st.write(f"**{t('data_upload.first_lines')}**:")
        
        for i, line in enumerate(lines, 1):
            if line.strip():
                try:
                    # å°è¯•è§£æJSONå¹¶æ ¼å¼åŒ–æ˜¾ç¤º
                    json_data = json.loads(line)
                    st.code(f"{t('data_upload.line_number').format(number=i)}: {json.dumps(json_data, indent=2, ensure_ascii=False)[:500]}...")
                except json.JSONDecodeError:
                    st.code(f"{t('data_upload.line_number').format(number=i)}: {line[:500]}...")
        
        # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        uploaded_file.seek(0)
        
    except Exception as e:
        st.error(f"{t('data_upload.file_preview_failed')}: {str(e)}")


def process_uploaded_file(uploaded_file):
    """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶"""
    progress_container = st.container()
    
    with progress_container:
        st.subheader(t("data_upload.processing_progress"))
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # Step 1: Save file
            status_text.text(t("data_upload.step_saving_file"))
            progress_bar.progress(10)
            
            # ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
            upload_dir = Path("data/uploads")
            upload_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            file_path = upload_dir / uploaded_file.name
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # Step 2: Parse data
            status_text.text(t("data_upload.step_parsing_data"))
            progress_bar.progress(30)
            
            parser = GA4DataParser()
            raw_data = parser.parse_ndjson(str(file_path))
            
            # Step 3: Data validation
            status_text.text(t("data_upload.step_validating_data"))
            progress_bar.progress(50)
            
            validator = DataValidator()
            validation_report = validator.validate_dataframe(raw_data)
            
            # Step 4: Data processing
            status_text.text(t("data_upload.step_processing_data"))
            progress_bar.progress(70)
            
            # æå–äº‹ä»¶æ•°æ®
            events_data = parser.extract_events(raw_data)
            user_data = parser.extract_user_properties(raw_data)
            session_data = parser.extract_sessions(raw_data)
            
            # å¤„ç†äº‹ä»¶æ•°æ® - å¦‚æœæ˜¯å­—å…¸ï¼Œåˆå¹¶æ‰€æœ‰äº‹ä»¶ç±»å‹
            if isinstance(events_data, dict):
                # åˆå¹¶æ‰€æœ‰äº‹ä»¶ç±»å‹çš„æ•°æ®
                all_events_list = []
                for event_type, event_df in events_data.items():
                    if not event_df.empty:
                        all_events_list.append(event_df)
                
                if all_events_list:
                    combined_events = pd.concat(all_events_list, ignore_index=True)
                    st.success(t("data_upload.merged_events").format(types=len(events_data), events=len(combined_events)))
                else:
                    combined_events = pd.DataFrame()
                    st.warning(t("data_upload.no_valid_events"))
            else:
                combined_events = events_data
            
            # Step 5: Store data
            status_text.text(t("data_upload.step_storing_data"))
            progress_bar.progress(90)
            
            # å­˜å‚¨åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.raw_data = raw_data
            st.session_state.processed_data = {
                'events': combined_events,
                'users': user_data,
                'sessions': session_data
            }
            
            # å­˜å‚¨åˆ°æ•°æ®ç®¡ç†å™¨
            storage_manager = st.session_state.storage_manager
            storage_manager.store_events(combined_events)
            storage_manager.store_users(user_data)
            storage_manager.store_sessions(session_data)
            
            # åˆ·æ–°é›†æˆç®¡ç†å™¨çš„å­˜å‚¨ç®¡ç†å™¨
            if 'integration_manager' in st.session_state:
                st.session_state.integration_manager.refresh_storage_manager(storage_manager)
            
            # ç”Ÿæˆæ•°æ®æ‘˜è¦
            data_summary = parser.validate_data_quality(raw_data)
            st.session_state.data_summary = data_summary
            st.session_state.validation_report = validation_report
            
            # Complete
            status_text.text(t("data_upload.step_completed"))
            progress_bar.progress(100)
            
            st.session_state.data_loaded = True
            
            # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
            st.success(t("data_upload.processing_success"))
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if file_path.exists():
                file_path.unlink()
            
            time.sleep(1)
            st.rerun()
            
        except Exception as e:
            st.error(f"{t('data_upload.processing_failed')}: {str(e)}")
            progress_bar.progress(0)
            status_text.text(t("data_upload.step_failed"))
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if 'file_path' in locals() and file_path.exists():
                file_path.unlink()


def show_processing_results():
    """æ˜¾ç¤ºå¤„ç†ç»“æœ"""
    st.markdown("---")
    st.subheader(t("data_upload.processing_results"))
    
    if st.session_state.data_summary:
        summary = st.session_state.data_summary
        
        # åŸºç¡€ç»Ÿè®¡
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "æ€»äº‹ä»¶æ•°",
                f"{summary.get('total_events', 0):,}",
                help="æ•°æ®é›†ä¸­çš„æ€»äº‹ä»¶æ•°é‡"
            )
        
        with col2:
            st.metric(
                "ç‹¬ç«‹ç”¨æˆ·æ•°",
                f"{summary.get('unique_users', 0):,}",
                help="æ•°æ®é›†ä¸­çš„ç‹¬ç«‹ç”¨æˆ·æ•°é‡"
            )
        
        with col3:
            date_range = summary.get('date_range', {})
            days = (pd.to_datetime(date_range.get('end', '')) - pd.to_datetime(date_range.get('start', ''))).days
            st.metric(
                "æ•°æ®å¤©æ•°",
                f"{days}å¤©",
                help="æ•°æ®è¦†ç›–çš„æ—¶é—´èŒƒå›´"
            )
        
        with col4:
            st.metric(
                "äº‹ä»¶ç±»å‹æ•°",
                len(summary.get('event_types', {})),
                help="æ•°æ®ä¸­åŒ…å«çš„ä¸åŒäº‹ä»¶ç±»å‹æ•°é‡"
            )
        
        # è¯¦ç»†ä¿¡æ¯
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**ğŸ“… æ—¶é—´èŒƒå›´**")
            date_range = summary.get('date_range', {})
            st.write(f"- å¼€å§‹æ—¶é—´: {date_range.get('start', 'N/A')}")
            st.write(f"- ç»“æŸæ—¶é—´: {date_range.get('end', 'N/A')}")
            
            st.write("**ğŸ¯ äº‹ä»¶ç±»å‹åˆ†å¸ƒ**")
            event_types = summary.get('event_types', {})
            for event_type, count in list(event_types.items())[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                st.write(f"- {event_type}: {count:,}")
            if len(event_types) > 10:
                st.write(f"- ... è¿˜æœ‰ {len(event_types) - 10} ç§äº‹ä»¶ç±»å‹")
        
        with col2:
            st.write("**ğŸ“± å¹³å°åˆ†å¸ƒ**")
            platforms = summary.get('platforms', {})
            for platform, count in platforms.items():
                st.write(f"- {platform}: {count:,}")
            
            # æ•°æ®è´¨é‡é—®é¢˜
            if summary.get('data_issues'):
                st.write("**âš ï¸ æ•°æ®è´¨é‡é—®é¢˜**")
                for issue in summary.get('data_issues', []):
                    st.warning(f"- {issue}")
    
    # éªŒè¯æŠ¥å‘Š
    if st.session_state.validation_report:
        with st.expander("ğŸ” è¯¦ç»†éªŒè¯æŠ¥å‘Š", expanded=False):
            validation_report = st.session_state.validation_report
            
            if validation_report.get('validation_passed'):
                st.success("âœ… æ•°æ®éªŒè¯é€šè¿‡")
            else:
                st.error("âŒ æ•°æ®éªŒè¯å‘ç°é—®é¢˜")
            
            # é”™è¯¯ä¿¡æ¯
            if validation_report.get('errors'):
                st.write("**é”™è¯¯ä¿¡æ¯:**")
                for error in validation_report['errors']:
                    st.error(f"- {error}")
            
            # è­¦å‘Šä¿¡æ¯
            if validation_report.get('warnings'):
                st.write("**è­¦å‘Šä¿¡æ¯:**")
                for warning in validation_report['warnings']:
                    st.warning(f"- {warning}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = validation_report.get('statistics', {})
            if stats:
                st.write("**ç»Ÿè®¡ä¿¡æ¯:**")
                st.json(stats)


def show_system_settings():
    """æ˜¾ç¤ºç³»ç»Ÿè®¾ç½®é¡µé¢"""
    st.header(t("settings.title"))
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs([
        t("settings.system_config"), 
        t("settings.analysis_config"), 
        t("settings.export_config"), 
        t("settings.config_management")
    ])
    
    with tab1:
        show_system_config_tab()
    
    with tab2:
        show_analysis_config_tab()
    
    with tab3:
        show_export_config_tab()
    
    with tab4:
        show_config_management_tab()


def show_system_config_tab():
    """æ˜¾ç¤ºç³»ç»Ÿé…ç½®æ ‡ç­¾é¡µ"""
    st.subheader("ğŸ”§ ç³»ç»Ÿé…ç½®")
    
    # è·å–å½“å‰é…ç½®
    system_config = config_manager.get_system_config()
    
    # APIé…ç½®
    st.write(f"### {t('settings.api_config')}")
    
    # LLMæä¾›å•†é€‰æ‹©
    st.write(f"#### {t('settings.llm_provider_config')}")
    col1, col2 = st.columns(2)
    
    with col1:
        default_provider = st.selectbox(
            "é»˜è®¤LLMæä¾›å•†",
            options=["google", "volcano"],
            index=0 if system_config.get('api_settings', {}).get('default_llm_provider', 'google') == 'google' else 1,
            help=t("settings.default_llm_provider_help")
        )
        
        enabled_providers = st.multiselect(
            "å¯ç”¨çš„æä¾›å•†",
            options=["google", "volcano"],
            default=system_config.get('api_settings', {}).get('enabled_providers', ['google', 'volcano']),
            help=t("settings.enabled_providers_help")
        )
    
    with col2:
        enable_fallback = st.checkbox(
            "å¯ç”¨æä¾›å•†å›é€€",
            value=system_config.get('api_settings', {}).get('enable_fallback', True),
            help=t("settings.enable_fallback_help")
        )
        
        if enable_fallback:
            fallback_order = st.multiselect(
                "å›é€€é¡ºåº",
                options=["google", "volcano"],
                default=system_config.get('api_settings', {}).get('fallback_order', ['google', 'volcano']),
                help=t("settings.fallback_order_help")
            )
        else:
            fallback_order = []
    
    # Google APIé…ç½®
    st.write(f"#### {t('settings.google_gemini_config')}")
    col1, col2 = st.columns(2)
    
    with col1:
        current_google_key = system_config.get('api_settings', {}).get('google_api_key', '')
        new_google_key = st.text_input(
            "Google APIå¯†é’¥",
            value=current_google_key[:20] + "..." if len(current_google_key) > 20 else current_google_key,
            type="password",
            help=t("settings.google_api_key_help")
        )
        
        google_model = st.selectbox(
            "Googleæ¨¡å‹",
            options=["gemini-2.5-pro", "gemini-1.5-pro", "gemini-1.5-flash"],
            index=0 if system_config.get('api_settings', {}).get('llm_model') == "gemini-2.5-pro" else 0
        )
    
    with col2:
        google_temperature = st.slider(
            "Googleæ¨¡å‹æ¸©åº¦",
            min_value=0.0,
            max_value=2.0,
            value=system_config.get('api_settings', {}).get('llm_temperature', 0.1),
            step=0.1,
            help=t("settings.google_temperature_help")
        )
        
        google_max_tokens = st.number_input(
            "Googleæœ€å¤§Tokenæ•°",
            min_value=1000,
            max_value=8000,
            value=system_config.get('api_settings', {}).get('llm_max_tokens', 4000),
            step=500
        )
    
    # Volcano APIé…ç½®
    st.write(f"#### {t('settings.volcano_ark_config')}")
    col1, col2 = st.columns(2)
    
    with col1:
        current_ark_key = system_config.get('api_settings', {}).get('ark_api_key', '')
        new_ark_key = st.text_input(
            "Volcano ARK APIå¯†é’¥",
            value=current_ark_key[:20] + "..." if len(current_ark_key) > 20 else current_ark_key,
            type="password",
            help=t("settings.volcano_api_key_help")
        )
        
        ark_base_url = st.text_input(
            "ARK APIåŸºç¡€URL",
            value=system_config.get('api_settings', {}).get('ark_base_url', 'https://ark.cn-beijing.volces.com/api/v3'),
            help=t("settings.volcano_base_url_help")
        )
    
    with col2:
        ark_model = st.text_input(
            "Volcanoæ¨¡å‹åç§°",
            value=system_config.get('api_settings', {}).get('ark_model', 'doubao-seed-1-6-250615'),
            help=t("settings.volcano_model_help")
        )
        
        ark_temperature = st.slider(
            "Volcanoæ¨¡å‹æ¸©åº¦",
            min_value=0.0,
            max_value=2.0,
            value=system_config.get('api_settings', {}).get('ark_temperature', 0.1),
            step=0.1,
            help=t("settings.volcano_temperature_help")
        )
    
    # å¤šæ¨¡æ€é…ç½®
    st.write(f"#### {t('settings.multimodal_config')}")
    col1, col2 = st.columns(2)
    
    with col1:
        enable_multimodal = st.checkbox(
            "å¯ç”¨å¤šæ¨¡æ€æ”¯æŒ",
            value=system_config.get('api_settings', {}).get('enable_multimodal', True),
            help=t("settings.enable_multimodal_help")
        )
        
        max_image_size = st.number_input(
            "æœ€å¤§å›¾ç‰‡å¤§å° (MB)",
            min_value=1,
            max_value=50,
            value=system_config.get('api_settings', {}).get('max_image_size_mb', 10),
            step=1
        )
    
    with col2:
        if enable_multimodal:
            supported_formats = st.multiselect(
                "æ”¯æŒçš„å›¾ç‰‡æ ¼å¼",
                options=["jpg", "jpeg", "png", "gif", "webp", "bmp"],
                default=system_config.get('api_settings', {}).get('supported_image_formats', ['jpg', 'jpeg', 'png', 'gif', 'webp']),
                help=t("settings.supported_image_formats_help")
            )
            
            image_timeout = st.number_input(
                "å›¾ç‰‡åˆ†æè¶…æ—¶ (ç§’)",
                min_value=10,
                max_value=300,
                value=system_config.get('api_settings', {}).get('image_analysis_timeout', 60),
                step=10
            )
        else:
            supported_formats = []
            image_timeout = 60
    
    # æ•°æ®å¤„ç†é…ç½®
    st.write(f"### {t('settings.data_processing_config')}")
    col1, col2 = st.columns(2)
    
    with col1:
        max_file_size = st.number_input(
            "æœ€å¤§æ–‡ä»¶å¤§å° (MB)",
            min_value=10,
            max_value=500,
            value=system_config.get('data_processing', {}).get('max_file_size_mb', 100),
            step=10
        )
        
        chunk_size = st.number_input(
            "æ•°æ®å¤„ç†å—å¤§å°",
            min_value=1000,
            max_value=50000,
            value=system_config.get('data_processing', {}).get('chunk_size', 10000),
            step=1000
        )
    
    with col2:
        memory_limit = st.number_input(
            "å†…å­˜é™åˆ¶ (GB)",
            min_value=1,
            max_value=16,
            value=system_config.get('data_processing', {}).get('memory_limit_gb', 4),
            step=1
        )
        
        cleanup_temp = st.checkbox(
            "è‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶",
            value=system_config.get('data_processing', {}).get('cleanup_temp_files', True)
        )
    
    # ç•Œé¢é…ç½®
    st.write(f"### {t('settings.ui_config')}")
    col1, col2 = st.columns(2)
    
    with col1:
        ui_theme = st.selectbox(
            t("settings.interface_theme"),
            options=["light", "dark"],
            index=0 if system_config.get('ui_settings', {}).get('theme') == "light" else 1
        )
        
        language = st.selectbox(
            t("settings.interface_language"),
            options=["zh-CN", "en-US"],
            index=0 if system_config.get('ui_settings', {}).get('language') == "zh-CN" else 1
        )
    
    with col2:
        page_size = st.number_input(
            t("settings.page_size"),
            min_value=10,
            max_value=100,
            value=system_config.get('ui_settings', {}).get('page_size', 20),
            step=5
        )
        
        show_debug = st.checkbox(
            t("settings.show_debug"),
            value=system_config.get('ui_settings', {}).get('show_debug_info', False)
        )
    
    # ä¿å­˜é…ç½®æŒ‰é’®
    if st.button(t("settings.save_config"), type="primary"):
        try:
            # æ„å»ºAPIé…ç½®å­—å…¸
            api_config = {
                # æä¾›å•†é…ç½®
                'default_llm_provider': default_provider,
                'enabled_providers': enabled_providers,
                'enable_fallback': enable_fallback,
                'fallback_order': fallback_order,
                
                # Googleé…ç½®
                'llm_model': google_model,
                'llm_temperature': google_temperature,
                'llm_max_tokens': google_max_tokens,
                
                # Volcanoé…ç½®
                'ark_base_url': ark_base_url,
                'ark_model': ark_model,
                'ark_temperature': ark_temperature,
                
                # å¤šæ¨¡æ€é…ç½®
                'enable_multimodal': enable_multimodal,
                'max_image_size_mb': max_image_size,
                'supported_image_formats': supported_formats,
                'image_analysis_timeout': image_timeout
            }
            
            # æ›´æ–°APIå¯†é’¥ï¼ˆå¦‚æœæœ‰å˜åŒ–ï¼‰
            if new_google_key and new_google_key != current_google_key[:20] + "...":
                api_config['google_api_key'] = new_google_key
            
            if new_ark_key and new_ark_key != current_ark_key[:20] + "...":
                api_config['ark_api_key'] = new_ark_key
            
            # ä¿å­˜APIé…ç½®
            config_manager.update_system_config('api_settings', api_config)
            
            # æ›´æ–°æ•°æ®å¤„ç†é…ç½®
            config_manager.update_system_config('data_processing', {
                'max_file_size_mb': max_file_size,
                'chunk_size': chunk_size,
                'memory_limit_gb': memory_limit,
                'cleanup_temp_files': cleanup_temp
            })
            
            # æ›´æ–°ç•Œé¢é…ç½®
            config_manager.update_system_config('ui_settings', {
                'theme': ui_theme,
                'language': language,
                'page_size': page_size,
                'show_debug_info': show_debug
            })
            
            st.success(t("settings.config_saved"))
            st.rerun()
            
        except Exception as e:
            st.error(f"{t('settings.config_save_failed')}: {str(e)}")


def show_analysis_config_tab():
    """æ˜¾ç¤ºåˆ†æå‚æ•°é…ç½®æ ‡ç­¾é¡µ"""
    st.subheader(t("settings.analysis_config"))
    
    # è·å–å½“å‰åˆ†æé…ç½®
    analysis_config = config_manager.get_analysis_config()
    
    # äº‹ä»¶åˆ†æé…ç½®
    with st.expander("ğŸ“ˆ äº‹ä»¶åˆ†æé…ç½®", expanded=True):
        col1, col2 = st.columns(2)
        
        with col1:
            event_granularity = st.selectbox(
                "æ—¶é—´ç²’åº¦",
                options=["day", "week", "month"],
                index=["day", "week", "month"].index(analysis_config.get('event_analysis', {}).get('time_granularity', 'day'))
            )
            
            top_events_limit = st.number_input(
                "çƒ­é—¨äº‹ä»¶æ•°é‡é™åˆ¶",
                min_value=5,
                max_value=50,
                value=analysis_config.get('event_analysis', {}).get('top_events_limit', 10),
                step=5
            )
        
        with col2:
            trend_days = st.number_input(
                "è¶‹åŠ¿åˆ†æå¤©æ•°",
                min_value=7,
                max_value=90,
                value=analysis_config.get('event_analysis', {}).get('trend_analysis_days', 30),
                step=7
            )
            
            correlation_threshold = st.slider(
                "å…³è”æ€§é˜ˆå€¼",
                min_value=0.1,
                max_value=1.0,
                value=analysis_config.get('event_analysis', {}).get('correlation_threshold', 0.5),
                step=0.1
            )
    
    # ç•™å­˜åˆ†æé…ç½®
    with st.expander("ğŸ“ˆ ç•™å­˜åˆ†æé…ç½®", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            retention_periods_str = st.text_input(
                "ç•™å­˜åˆ†æå‘¨æœŸ (å¤©ï¼Œé€—å·åˆ†éš”)",
                value=",".join(map(str, analysis_config.get('retention_analysis', {}).get('retention_periods', [1, 7, 14, 30])))
            )
            
            cohort_type = st.selectbox(
                "é˜Ÿåˆ—ç±»å‹",
                options=["daily", "weekly", "monthly"],
                index=["daily", "weekly", "monthly"].index(analysis_config.get('retention_analysis', {}).get('cohort_type', 'weekly'))
            )
        
        with col2:
            min_cohort_size = st.number_input(
                "æœ€å°é˜Ÿåˆ—å¤§å°",
                min_value=10,
                max_value=1000,
                value=analysis_config.get('retention_analysis', {}).get('min_cohort_size', 100),
                step=10
            )
            
            analysis_window = st.number_input(
                "åˆ†æçª—å£ (å¤©)",
                min_value=30,
                max_value=365,
                value=analysis_config.get('retention_analysis', {}).get('analysis_window_days', 90),
                step=30
            )
    
    # è½¬åŒ–åˆ†æé…ç½®
    with st.expander("ğŸ”„ è½¬åŒ–åˆ†æé…ç½®", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            conversion_window = st.number_input(
                "è½¬åŒ–çª—å£ (å°æ—¶)",
                min_value=1,
                max_value=168,
                value=analysis_config.get('conversion_analysis', {}).get('conversion_window_hours', 24),
                step=1
            )
            
            min_funnel_users = st.number_input(
                "æœ€å°æ¼æ–—ç”¨æˆ·æ•°",
                min_value=10,
                max_value=500,
                value=analysis_config.get('conversion_analysis', {}).get('min_funnel_users', 50),
                step=10
            )
        
        with col2:
            attribution_model = st.selectbox(
                "å½’å› æ¨¡å‹",
                options=["first_touch", "last_touch", "linear"],
                index=["first_touch", "last_touch", "linear"].index(analysis_config.get('conversion_analysis', {}).get('attribution_model', 'first_touch'))
            )
    
    # ç”¨æˆ·åˆ†ç¾¤é…ç½®
    with st.expander("ğŸ‘¥ ç”¨æˆ·åˆ†ç¾¤é…ç½®", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            clustering_method = st.selectbox(
                "èšç±»ç®—æ³•",
                options=["kmeans", "dbscan"],
                index=["kmeans", "dbscan"].index(analysis_config.get('user_segmentation', {}).get('clustering_method', 'kmeans'))
            )
            
            n_clusters = st.slider(
                "èšç±»æ•°é‡",
                min_value=2,
                max_value=15,
                value=analysis_config.get('user_segmentation', {}).get('n_clusters', 5),
                step=1
            )
        
        with col2:
            min_cluster_size = st.number_input(
                "æœ€å°èšç±»å¤§å°",
                min_value=10,
                max_value=500,
                value=analysis_config.get('user_segmentation', {}).get('min_cluster_size', 100),
                step=10
            )
            
            feature_types = st.multiselect(
                "ç‰¹å¾ç±»å‹",
                options=["behavioral", "demographic", "temporal"],
                default=analysis_config.get('user_segmentation', {}).get('feature_types', ['behavioral', 'demographic'])
            )
    
    # è·¯å¾„åˆ†æé…ç½®
    with st.expander("ğŸ›¤ï¸ è·¯å¾„åˆ†æé…ç½®", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            min_path_support = st.slider(
                "æœ€å°è·¯å¾„æ”¯æŒåº¦",
                min_value=0.001,
                max_value=0.1,
                value=analysis_config.get('path_analysis', {}).get('min_path_support', 0.01),
                step=0.001,
                format="%.3f"
            )
            
            max_path_length = st.number_input(
                "æœ€å¤§è·¯å¾„é•¿åº¦",
                min_value=3,
                max_value=20,
                value=analysis_config.get('path_analysis', {}).get('max_path_length', 10),
                step=1
            )
        
        with col2:
            session_timeout = st.number_input(
                "ä¼šè¯è¶…æ—¶ (åˆ†é’Ÿ)",
                min_value=5,
                max_value=120,
                value=analysis_config.get('path_analysis', {}).get('session_timeout_minutes', 30),
                step=5
            )
            
            include_bounce = st.checkbox(
                "åŒ…å«è·³å‡ºä¼šè¯",
                value=analysis_config.get('path_analysis', {}).get('include_bounce_sessions', False)
            )
    
    # ä¿å­˜åˆ†æé…ç½®
    if st.button(f"ğŸ’¾ {t('common.save')} {t('analysis.analysis_config')}", type="primary"):
        try:
            # è§£æç•™å­˜å‘¨æœŸ
            retention_periods = [int(x.strip()) for x in retention_periods_str.split(',') if x.strip().isdigit()]
            
            # æ›´æ–°äº‹ä»¶åˆ†æé…ç½®
            config_manager.update_analysis_config('event_analysis', {
                'time_granularity': event_granularity,
                'top_events_limit': top_events_limit,
                'trend_analysis_days': trend_days,
                'correlation_threshold': correlation_threshold
            })
            
            # æ›´æ–°ç•™å­˜åˆ†æé…ç½®
            config_manager.update_analysis_config('retention_analysis', {
                'retention_periods': retention_periods,
                'cohort_type': cohort_type,
                'min_cohort_size': min_cohort_size,
                'analysis_window_days': analysis_window
            })
            
            # æ›´æ–°è½¬åŒ–åˆ†æé…ç½®
            config_manager.update_analysis_config('conversion_analysis', {
                'conversion_window_hours': conversion_window,
                'min_funnel_users': min_funnel_users,
                'attribution_model': attribution_model
            })
            
            # æ›´æ–°ç”¨æˆ·åˆ†ç¾¤é…ç½®
            config_manager.update_analysis_config('user_segmentation', {
                'clustering_method': clustering_method,
                'n_clusters': n_clusters,
                'min_cluster_size': min_cluster_size,
                'feature_types': feature_types
            })
            
            # æ›´æ–°è·¯å¾„åˆ†æé…ç½®
            config_manager.update_analysis_config('path_analysis', {
                'min_path_support': min_path_support,
                'max_path_length': max_path_length,
                'session_timeout_minutes': session_timeout,
                'include_bounce_sessions': include_bounce
            })
            
            st.success(t("messages.config_saved"))
            
        except Exception as e:
            st.error(f"{t('messages.config_save_failed')}: {str(e)}")


def show_export_config_tab():
    """æ˜¾ç¤ºå¯¼å‡ºè®¾ç½®æ ‡ç­¾é¡µ"""
    st.subheader("ğŸ“¥ å¯¼å‡ºè®¾ç½®")
    
    # è·å–å½“å‰å¯¼å‡ºé…ç½®
    export_config = config_manager.get_system_config('export_settings')
    
    col1, col2 = st.columns(2)
    
    with col1:
        default_format = st.selectbox(
            "é»˜è®¤å¯¼å‡ºæ ¼å¼",
            options=["json", "pdf", "excel"],
            index=["json", "pdf", "excel"].index(export_config.get('default_format', 'json'))
        )
        
        include_raw_data = st.checkbox(
            "é»˜è®¤åŒ…å«åŸå§‹æ•°æ®",
            value=export_config.get('include_raw_data', False),
            help="åœ¨å¯¼å‡ºæŠ¥å‘Šæ—¶é»˜è®¤åŒ…å«åŸå§‹æ•°æ®"
        )
        
        compress_exports = st.checkbox(
            "å‹ç¼©å¯¼å‡ºæ–‡ä»¶",
            value=export_config.get('compress_exports', True),
            help="å¯¹å¤§å‹å¯¼å‡ºæ–‡ä»¶è¿›è¡Œå‹ç¼©"
        )
    
    with col2:
        export_dir = st.text_input(
            "å¯¼å‡ºç›®å½•",
            value=export_config.get('export_dir', 'reports'),
            help="æŠ¥å‘Šæ–‡ä»¶çš„é»˜è®¤ä¿å­˜ç›®å½•"
        )
        
        filename_template = st.text_input(
            "æ–‡ä»¶åæ¨¡æ¿",
            value=export_config.get('filename_template', 'analysis_report_{timestamp}'),
            help="ä½¿ç”¨{timestamp}ä½œä¸ºæ—¶é—´æˆ³å ä½ç¬¦"
        )
    
    # å¯¼å‡ºæ ¼å¼æ”¯æŒçŠ¶æ€
    st.write("### ğŸ“‹ å¯¼å‡ºæ ¼å¼æ”¯æŒçŠ¶æ€")
    exporter = ReportExporter()
    supported_formats = exporter.get_supported_formats()
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.write("**JSON**")
        st.success("âœ… æ”¯æŒ" if 'json' in supported_formats else "âŒ ä¸æ”¯æŒ")
    
    with col2:
        st.write("**PDF**")
        st.success("âœ… æ”¯æŒ" if 'pdf' in supported_formats else "âŒ ä¸æ”¯æŒ")
        if 'pdf' not in supported_formats:
            st.info("éœ€è¦å®‰è£… reportlab åº“")
    
    with col3:
        st.write("**Excel**")
        st.success("âœ… æ”¯æŒ" if 'excel' in supported_formats else "âŒ ä¸æ”¯æŒ")
        if 'excel' not in supported_formats:
            st.info("éœ€è¦å®‰è£… openpyxl åº“")
    
    # ä¿å­˜å¯¼å‡ºé…ç½®
    if st.button("ğŸ’¾ ä¿å­˜å¯¼å‡ºé…ç½®", type="primary"):
        try:
            config_manager.update_system_config('export_settings', {
                'default_format': default_format,
                'include_raw_data': include_raw_data,
                'compress_exports': compress_exports,
                'export_dir': export_dir,
                'filename_template': filename_template
            })
            
            st.success("âœ… å¯¼å‡ºé…ç½®ä¿å­˜æˆåŠŸ!")
            
        except Exception as e:
            st.error(f"{t('messages.config_save_failed')}: {str(e)}")


def show_config_management_tab():
    """æ˜¾ç¤ºé…ç½®ç®¡ç†æ ‡ç­¾é¡µ"""
    st.subheader("ğŸ”„ é…ç½®ç®¡ç†")
    
    # é…ç½®çŠ¶æ€æ¦‚è§ˆ
    st.write("### ğŸ“Š é…ç½®çŠ¶æ€æ¦‚è§ˆ")
    config_summary = config_manager.get_config_summary()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**åˆ†æé…ç½®**")
        analysis_summary = config_summary.get('analysis_config', {})
        st.write(f"- äº‹ä»¶åˆ†æ: {'âœ…' if analysis_summary.get('event_analysis_enabled') else 'âŒ'}")
        st.write(f"- ç•™å­˜å‘¨æœŸæ•°: {analysis_summary.get('retention_periods', 0)}")
        st.write(f"- èšç±»æ–¹æ³•: {analysis_summary.get('clustering_method', 'N/A')}")
        st.write(f"- æ¼æ–—æ­¥éª¤: {analysis_summary.get('funnel_steps_configured', 0)}")
    
    with col2:
        st.write("**ç³»ç»Ÿé…ç½®**")
        system_summary = config_summary.get('system_config', {})
        st.write(f"- APIé…ç½®: {'âœ…' if system_summary.get('api_configured') else 'âŒ'}")
        st.write(f"- æ–‡ä»¶å¤§å°é™åˆ¶: {system_summary.get('max_file_size_mb', 0)}MB")
        st.write(f"- é»˜è®¤å¯¼å‡ºæ ¼å¼: {system_summary.get('default_export_format', 'N/A')}")
        st.write(f"- ç•Œé¢ä¸»é¢˜: {system_summary.get('ui_theme', 'N/A')}")
    
    # é…ç½®éªŒè¯
    st.write("### âœ… é…ç½®éªŒè¯")
    if st.button("ğŸ” éªŒè¯é…ç½®", type="secondary"):
        validation_result = config_manager.validate_config()
        
        if validation_result['valid']:
            st.success("âœ… é…ç½®éªŒè¯é€šè¿‡!")
        else:
            st.error("âŒ é…ç½®éªŒè¯å¤±è´¥!")
            
            if validation_result['errors']:
                st.write("**é”™è¯¯:**")
                for error in validation_result['errors']:
                    st.error(f"â€¢ {error}")
        
        if validation_result['warnings']:
            st.write("**è­¦å‘Š:**")
            for warning in validation_result['warnings']:
                st.warning(f"â€¢ {warning}")
    
    # é…ç½®å¯¼å…¥å¯¼å‡º
    st.write("### ğŸ“ é…ç½®å¯¼å…¥å¯¼å‡º")
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**å¯¼å‡ºé…ç½®**")
        if st.button("ğŸ“¤ å¯¼å‡ºé…ç½®æ–‡ä»¶"):
            try:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                export_path = f"config/config_backup_{timestamp}.json"
                
                if config_manager.export_config(export_path):
                    st.success(f"âœ… é…ç½®å¯¼å‡ºæˆåŠŸ: {export_path}")
                    
                    # æä¾›ä¸‹è½½
                    with open(export_path, 'r', encoding='utf-8') as f:
                        config_data = f.read()
                    
                    st.download_button(
                        label="â¬‡ï¸ ä¸‹è½½é…ç½®æ–‡ä»¶",
                        data=config_data,
                        file_name=f"config_backup_{timestamp}.json",
                        mime="application/json"
                    )
                else:
                    st.error("âŒ é…ç½®å¯¼å‡ºå¤±è´¥")
                    
            except Exception as e:
                st.error(f"âŒ å¯¼å‡ºè¿‡ç¨‹å‡ºé”™: {str(e)}")
    
    with col2:
        st.write("**å¯¼å…¥é…ç½®**")
        uploaded_config = st.file_uploader(
            "é€‰æ‹©é…ç½®æ–‡ä»¶",
            type=['json'],
            help="ä¸Šä¼ ä¹‹å‰å¯¼å‡ºçš„é…ç½®æ–‡ä»¶"
        )
        
        if uploaded_config is not None:
            if st.button("ğŸ“¥ å¯¼å…¥é…ç½®", type="primary"):
                try:
                    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                    import tempfile
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as tmp_file:
                        tmp_file.write(uploaded_config.read().decode('utf-8'))
                        tmp_path = tmp_file.name
                    
                    if config_manager.import_config(tmp_path):
                        st.success("âœ… é…ç½®å¯¼å…¥æˆåŠŸ!")
                        st.info("è¯·åˆ·æ–°é¡µé¢ä»¥åº”ç”¨æ–°é…ç½®")
                    else:
                        st.error("âŒ é…ç½®å¯¼å…¥å¤±è´¥")
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    os.unlink(tmp_path)
                    
                except Exception as e:
                    st.error(f"âŒ å¯¼å…¥è¿‡ç¨‹å‡ºé”™: {str(e)}")
    
    # é‡ç½®é…ç½®
    st.write("### ğŸ”„ é‡ç½®é…ç½®")
    st.warning("âš ï¸ é‡ç½®æ“ä½œå°†æ¢å¤é»˜è®¤é…ç½®ï¼Œæ— æ³•æ’¤é”€!")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ”„ é‡ç½®åˆ†æé…ç½®", type="secondary"):
            if config_manager.reset_analysis_config():
                st.success("âœ… åˆ†æé…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼")
                st.rerun()
            else:
                st.error("âŒ é‡ç½®åˆ†æé…ç½®å¤±è´¥")
    
    with col2:
        if st.button("ğŸ”„ é‡ç½®ç³»ç»Ÿé…ç½®", type="secondary"):
            if config_manager.reset_system_config():
                st.success("âœ… ç³»ç»Ÿé…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼")
                st.rerun()
            else:
                st.error("âŒ é‡ç½®ç³»ç»Ÿé…ç½®å¤±è´¥")


def show_event_analysis_page():
    """æ˜¾ç¤ºäº‹ä»¶åˆ†æç»“æœé¡µé¢"""
    st.header(t('analysis.event_title'))
    
    # åˆå§‹åŒ–åˆ†æå¼•æ“å’Œå¯è§†åŒ–ç»„ä»¶
    if 'event_engine' not in st.session_state:
        st.session_state.event_engine = EventAnalysisEngine()
    if 'chart_generator' not in st.session_state:
        st.session_state.chart_generator = ChartGenerator()
    
    # åˆ†ææ§åˆ¶é¢æ¿
    with st.expander(t('analysis.analysis_config_expander'), expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            date_range = st.date_input(
                t('analysis.time_range_label'),
                value=(datetime.now() - timedelta(days=30), datetime.now()),
                help=t('analysis.time_range_help')
            )
        
        with col2:
            event_filter = st.multiselect(
                t('analysis.event_filter_label'),
                options=list(st.session_state.data_summary.get('event_types', {}).keys()),
                default=list(st.session_state.data_summary.get('event_types', {}).keys())[:5],
                help=t('analysis.event_filter_help')
            )
        
        with col3:
            analysis_granularity = st.selectbox(
                t('analysis.analysis_granularity_label'),
                options=[t('analysis.daily'), t('analysis.weekly'), t('analysis.monthly')],
                index=0,
                help=t('analysis.analysis_granularity_help')
            )
    
    # æ‰§è¡Œåˆ†ææŒ‰é’®
    if st.button(t('analysis.start_event_analysis'), type="primary"):
        with st.spinner(t('analysis.event_analysis_processing')):
            try:
                # è·å–æ•°æ®
                raw_data = st.session_state.raw_data
                
                # åº”ç”¨ç­›é€‰æ¡ä»¶
                if event_filter:
                    filtered_data = raw_data[raw_data['event_name'].isin(event_filter)]
                else:
                    filtered_data = raw_data
                
                # æ‰§è¡Œåˆ†æ
                engine = st.session_state.event_engine
                
                # äº‹ä»¶é¢‘æ¬¡åˆ†æ
                frequency_results = engine.analyze_event_frequency(filtered_data)
                
                # äº‹ä»¶è¶‹åŠ¿åˆ†æ - è½¬æ¢ä¸­æ–‡ç²’åº¦ä¸ºè‹±æ–‡
                english_granularity = translate_time_granularity(analysis_granularity)
                trend_results = engine.analyze_event_trends(filtered_data, time_granularity=english_granularity)
                
                # å­˜å‚¨åˆ†æç»“æœ
                st.session_state.event_analysis_results = {
                    'frequency': frequency_results,
                    'trends': trend_results,
                    'filtered_data': filtered_data
                }
                
                st.success(t('analysis.event_analysis_complete'))
                
            except Exception as e:
                st.error(f"{t('analysis.analysis_failed')}: {str(e)}")
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    if 'event_analysis_results' in st.session_state:
        results = st.session_state.event_analysis_results
        chart_gen = st.session_state.chart_generator
        
        st.markdown("---")
        st.subheader(t('analysis.analysis_results_header'))
        
        # å…³é”®æŒ‡æ ‡æ¦‚è§ˆ
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            total_events = len(results['filtered_data'])
            st.metric(t('analysis.total_events'), f"{total_events:,}")
        
        with col2:
            unique_users = results['filtered_data']['user_pseudo_id'].nunique()
            st.metric(t('analysis.active_users'), f"{unique_users:,}")
        
        with col3:
            avg_events_per_user = total_events / unique_users if unique_users > 0 else 0
            st.metric(t('analysis.avg_events_per_user'), f"{avg_events_per_user:.1f}")
        
        with col4:
            event_types = results['filtered_data']['event_name'].nunique()
            st.metric(t('analysis.event_types_count'), f"{event_types}")
        
        # äº‹ä»¶æ—¶é—´çº¿å›¾è¡¨
        st.subheader(t('analysis.event_timeline'))
        try:
            timeline_chart = chart_gen.create_event_timeline(results['filtered_data'])
            st.plotly_chart(timeline_chart, use_container_width=True)
        except Exception as e:
            st.error(f"{t('analysis.timeline_chart_failed')}: {str(e)}")
        
        # äº‹ä»¶é¢‘æ¬¡åˆ†å¸ƒ
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader(t('analysis.event_frequency_distribution'))
            event_counts = results['filtered_data']['event_name'].value_counts()
            st.bar_chart(event_counts)
        
        with col2:
            st.subheader(t('analysis.user_activity_distribution'))
            user_activity = results['filtered_data'].groupby('user_pseudo_id').size()

            # åˆ›å»ºç”¨æˆ·æ´»è·ƒåº¦åˆ†å¸ƒç›´æ–¹å›¾
            activity_df = pd.DataFrame({
                'user_id': user_activity.index,
                'event_count': user_activity.values
            })

            fig = px.histogram(
                activity_df,
                x='event_count',
                nbins=20,
                title=t('analysis.user_event_count_distribution'),
                labels={'event_count': t('analysis.event_count'), 'count': t('analysis.user_count')}
            )
            fig.update_layout(
                xaxis_title=t('analysis.event_count'),
                yaxis_title=t('analysis.user_count'),
                showlegend=False
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # è¯¦ç»†æ•°æ®è¡¨
        with st.expander(t('analysis.detailed_data'), expanded=False):
            st.dataframe(
                results['filtered_data'][['event_date', 'event_name', 'user_pseudo_id', 'platform']].head(1000),
                use_container_width=True
            )


def show_retention_analysis_page():
    """æ˜¾ç¤ºç•™å­˜åˆ†æç»“æœé¡µé¢"""
    st.header(t('analysis.retention_title'))
    
    # åˆå§‹åŒ–åˆ†æå¼•æ“
    if 'retention_engine' not in st.session_state:
        st.session_state.retention_engine = RetentionAnalysisEngine()
    if 'chart_generator' not in st.session_state:
        st.session_state.chart_generator = ChartGenerator()
    
    # åˆ†æé…ç½®
    with st.expander(t('analysis.retention_analysis_config'), expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            retention_type = st.selectbox(
                t('analysis.retention_type'),
                options=[t('analysis.daily_retention'), t('analysis.weekly_retention'), t('analysis.monthly_retention')],
                index=0
            )
        
        with col2:
            cohort_period = st.selectbox(
                t('analysis.cohort_period'),
                options=[t('analysis.daily'), t('analysis.weekly'), t('analysis.monthly')],
                index=1
            )
        
        with col3:
            analysis_periods = st.slider(
                t('analysis.analysis_periods'),
                min_value=7,
                max_value=30,
                value=14,
                help=t('analysis.analysis_periods_help')
            )
    
    # æ‰§è¡Œç•™å­˜åˆ†æ
    if st.button(t('analysis.start_retention_analysis'), type="primary"):
        with st.spinner(t('analysis.retention_analysis_processing')):
            try:
                raw_data = st.session_state.raw_data
                engine = st.session_state.retention_engine
                
                # æ‰§è¡Œç•™å­˜åˆ†æ - è½¬æ¢ä¸­æ–‡ç±»å‹ä¸ºè‹±æ–‡
                english_retention_type = translate_cohort_period(retention_type)
                english_cohort_period = translate_cohort_period(cohort_period)

                # æ‰§è¡Œå®Œæ•´çš„ç•™å­˜åˆ†æï¼Œè·å–åŒ…å«é˜Ÿåˆ—æ•°æ®çš„ç»“æœ
                if english_retention_type == "daily":
                    retention_results = engine.calculate_retention_rates(
                        events=raw_data,
                        analysis_type='daily',
                        max_periods=analysis_periods
                    )
                elif english_retention_type == "weekly":
                    retention_results = engine.calculate_retention_rates(
                        events=raw_data,
                        analysis_type='weekly',
                        max_periods=analysis_periods
                    )
                elif english_retention_type == "monthly":
                    retention_results = engine.calculate_retention_rates(
                        events=raw_data,
                        analysis_type='monthly',
                        max_periods=analysis_periods
                    )
                else:
                    # é»˜è®¤ä½¿ç”¨æœˆåº¦åˆ†æ
                    retention_results = engine.calculate_retention_rates(
                        events=raw_data,
                        analysis_type='monthly',
                        max_periods=analysis_periods
                    )

                # ä»ç•™å­˜åˆ†æç»“æœä¸­æå–é˜Ÿåˆ—æ•°æ®å¹¶è½¬æ¢ä¸ºçƒ­åŠ›å›¾æ‰€éœ€æ ¼å¼
                cohort_viz_data = []
                if retention_results and hasattr(retention_results, 'cohorts'):
                    for cohort in retention_results.cohorts:
                        cohort_period = cohort.cohort_period
                        retention_rates = cohort.retention_rates

                        for period_num, rate in enumerate(retention_rates):
                            cohort_viz_data.append({
                                'cohort_group': cohort_period,
                                'period_number': period_num,
                                'retention_rate': rate
                            })

                # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®
                if not cohort_viz_data:
                    cohort_viz_data = [
                        {'cohort_group': '2024-01', 'period_number': 0, 'retention_rate': 1.0},
                        {'cohort_group': '2024-01', 'period_number': 1, 'retention_rate': 0.7},
                        {'cohort_group': '2024-01', 'period_number': 2, 'retention_rate': 0.5},
                        {'cohort_group': '2024-02', 'period_number': 0, 'retention_rate': 1.0},
                        {'cohort_group': '2024-02', 'period_number': 1, 'retention_rate': 0.6},
                        {'cohort_group': '2024-02', 'period_number': 2, 'retention_rate': 0.4}
                    ]

                # è½¬æ¢ä¸ºDataFrame
                cohort_data = pd.DataFrame(cohort_viz_data)

                st.session_state.retention_results = {
                    'retention_data': retention_results,
                    'cohort_data': cohort_data,
                    'cohorts': retention_results.cohorts if retention_results and hasattr(retention_results, 'cohorts') else [],
                    'overall_retention_rates': retention_results.overall_retention_rates if retention_results and hasattr(retention_results, 'overall_retention_rates') else {}
                }
                
                st.success("âœ… ç•™å­˜åˆ†æå®Œæˆ!")
                
            except Exception as e:
                st.error(f"âŒ ç•™å­˜åˆ†æå¤±è´¥: {str(e)}")
    
    # æ˜¾ç¤ºç•™å­˜åˆ†æç»“æœ
    if 'retention_results' in st.session_state:
        results = st.session_state.retention_results
        chart_gen = st.session_state.chart_generator
        
        st.markdown("---")
        st.subheader("ğŸ“Š ç•™å­˜åˆ†æç»“æœ")
        
        # ç•™å­˜çƒ­åŠ›å›¾
        if 'cohort_data' in results and results['cohort_data'] is not None:
            # æ£€æŸ¥æ˜¯å¦ä¸ºDataFrameä¸”ä¸ä¸ºç©ºï¼Œæˆ–è€…æ˜¯å¦ä¸ºéç©ºå­—å…¸/åˆ—è¡¨
            cohort_data = results['cohort_data']
            is_valid_data = False

            if isinstance(cohort_data, pd.DataFrame) and not cohort_data.empty:
                is_valid_data = True
            elif isinstance(cohort_data, (dict, list)) and len(cohort_data) > 0:
                is_valid_data = True

            if is_valid_data:
                st.subheader("ğŸ”¥ ç•™å­˜çƒ­åŠ›å›¾")
                try:
                    # ä½¿ç”¨å¤„ç†è¿‡çš„cohort_dataè€Œä¸æ˜¯åŸå§‹çš„results['cohort_data']
                    heatmap_chart = chart_gen.create_retention_heatmap(cohort_data)
                    st.plotly_chart(heatmap_chart, use_container_width=True)
                except Exception as e:
                    st.error(f"çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")
                    # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                    st.info(f"æ•°æ®ç±»å‹: {type(cohort_data)}")
                    if isinstance(cohort_data, pd.DataFrame):
                        st.info(f"DataFrameå½¢çŠ¶: {cohort_data.shape}")
                        st.info(f"DataFrameåˆ—: {list(cohort_data.columns)}")
                    elif isinstance(cohort_data, (dict, list)):
                        st.info(f"æ•°æ®é•¿åº¦: {len(cohort_data)}")
                        if len(cohort_data) > 0:
                            st.info(f"æ•°æ®ç¤ºä¾‹: {str(cohort_data)[:200]}...")
        
        # ç•™å­˜æ›²çº¿
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“ˆ æ•´ä½“ç•™å­˜æ›²çº¿")

            # å°è¯•ä»å¤šä¸ªå¯èƒ½çš„æ•°æ®æºè·å–ç•™å­˜æ•°æ®
            retention_curve_data = None

            # æ£€æŸ¥ä¸åŒçš„æ•°æ®ç»“æ„
            if 'overall_retention_rates' in results and results['overall_retention_rates']:
                # ä»æ•´ä½“ç•™å­˜ç‡åˆ›å»ºæ›²çº¿æ•°æ®
                overall_rates = results['overall_retention_rates']
                if isinstance(overall_rates, dict):
                    curve_data = []
                    for k, v in overall_rates.items():
                        # å¤„ç†ä¸åŒç±»å‹çš„é”®
                        if isinstance(k, str) and k.startswith('period_'):
                            # å­—ç¬¦ä¸²é”®ï¼Œå¦‚ 'period_0', 'period_1'
                            period_num = int(k.replace('period_', ''))
                            curve_data.append({'period': period_num, 'retention_rate': v})
                        elif isinstance(k, int):
                            # æ•´æ•°é”®ï¼Œç›´æ¥ä½¿ç”¨
                            curve_data.append({'period': k, 'retention_rate': v})
                        elif isinstance(k, str) and k.isdigit():
                            # æ•°å­—å­—ç¬¦ä¸²é”®ï¼Œå¦‚ '0', '1'
                            curve_data.append({'period': int(k), 'retention_rate': v})

                    if curve_data:
                        retention_curve_data = pd.DataFrame(curve_data)
            elif 'cohorts' in results and results['cohorts']:
                # ä»é˜Ÿåˆ—æ•°æ®è®¡ç®—å¹³å‡ç•™å­˜ç‡
                cohorts = results['cohorts']
                if isinstance(cohorts, list) and len(cohorts) > 0:
                    # è®¡ç®—æ‰€æœ‰é˜Ÿåˆ—çš„å¹³å‡ç•™å­˜ç‡
                    # å®‰å…¨åœ°è·å–æ‰€æœ‰é˜Ÿåˆ—çš„ç•™å­˜ç‡æ•°æ®
                    cohort_retention_data = []
                    for cohort in cohorts:
                        if hasattr(cohort, 'retention_rates') and cohort.retention_rates:
                            cohort_retention_data.append(cohort.retention_rates)
                        else:
                            cohort_retention_data.append([])

                    if cohort_retention_data:
                        max_periods = max(len(rates) for rates in cohort_retention_data) if cohort_retention_data else 0
                        avg_rates = []
                        for period in range(max_periods):
                            period_rates = [
                                rates[period]
                                for rates in cohort_retention_data
                                if len(rates) > period
                            ]
                            if period_rates:
                                avg_rates.append({
                                    'period': period,
                                    'retention_rate': sum(period_rates) / len(period_rates)
                                })
                        retention_curve_data = pd.DataFrame(avg_rates)
            elif 'retention_data' in results and results['retention_data'] is not None:
                # åŸæœ‰çš„æ•°æ®ç»“æ„
                retention_data = results['retention_data']
                try:
                    if isinstance(retention_data, pd.DataFrame):
                        retention_curve_data = retention_data
                    elif isinstance(retention_data, (list, dict)):
                        retention_curve_data = pd.DataFrame(retention_data)
                except Exception:
                    pass

            # æ˜¾ç¤ºç•™å­˜æ›²çº¿
            if retention_curve_data is not None and not retention_curve_data.empty:
                if 'period' in retention_curve_data.columns and 'retention_rate' in retention_curve_data.columns:
                    st.line_chart(retention_curve_data.set_index('period')['retention_rate'])
                else:
                    st.info("ç•™å­˜æ•°æ®æ ¼å¼ä¸å®Œæ•´ï¼Œæ— æ³•æ˜¾ç¤ºæ›²çº¿å›¾")
            else:
                st.info("æš‚æ— ç•™å­˜æ›²çº¿æ•°æ®")
        
        with col2:
            st.subheader("ğŸ“Š ç•™å­˜ç‡åˆ†å¸ƒ")

            # å°è¯•ä»å¤šä¸ªå¯èƒ½çš„æ•°æ®æºè·å–åˆ†å¸ƒæ•°æ®
            distribution_data = None

            # æ£€æŸ¥ä¸åŒçš„æ•°æ®ç»“æ„
            if 'cohorts' in results and results['cohorts']:
                # ä»é˜Ÿåˆ—æ•°æ®åˆ›å»ºåˆ†å¸ƒæ•°æ®
                cohorts = results['cohorts']
                if isinstance(cohorts, list) and len(cohorts) > 0:
                    # åˆ›å»ºåŒ…å«æ‰€æœ‰é˜Ÿåˆ—å’Œæ—¶æœŸçš„æ•°æ®
                    dist_data = []
                    for cohort in cohorts:
                        # å®‰å…¨åœ°è®¿é—®CohortDataå¯¹è±¡çš„å±æ€§
                        if hasattr(cohort, 'cohort_period'):
                            cohort_period = cohort.cohort_period
                        else:
                            cohort_period = 'Unknown'

                        if hasattr(cohort, 'retention_rates'):
                            retention_rates = cohort.retention_rates
                        else:
                            retention_rates = []

                        for period_num, rate in enumerate(retention_rates):
                            dist_data.append({
                                'cohort_group': cohort_period,
                                'period_number': period_num,
                                'retention_rate': rate
                            })

                    if dist_data:
                        distribution_data = pd.DataFrame(dist_data)
            elif 'cohort_data' in results and results['cohort_data'] is not None:
                # åŸæœ‰çš„æ•°æ®ç»“æ„
                cohort_data = results['cohort_data']
                try:
                    if isinstance(cohort_data, pd.DataFrame) and not cohort_data.empty:
                        distribution_data = cohort_data
                    elif isinstance(cohort_data, (dict, list)) and len(cohort_data) > 0:
                        distribution_data = pd.DataFrame(cohort_data)
                except Exception:
                    pass

            # æ˜¾ç¤ºç•™å­˜ç‡åˆ†å¸ƒ
            if distribution_data is not None and not distribution_data.empty:
                if 'period_number' in distribution_data.columns and 'retention_rate' in distribution_data.columns:
                    # è®¡ç®—æ¯ä¸ªæ—¶æœŸçš„å¹³å‡ç•™å­˜ç‡
                    avg_retention = distribution_data.groupby('period_number')['retention_rate'].mean()
                    st.bar_chart(avg_retention)
                elif 'period' in distribution_data.columns and 'retention_rate' in distribution_data.columns:
                    # å¤‡ç”¨åˆ—å
                    avg_retention = distribution_data.groupby('period')['retention_rate'].mean()
                    st.bar_chart(avg_retention)
                else:
                    st.info("ç•™å­˜æ•°æ®æ ¼å¼ä¸å®Œæ•´ï¼Œæ— æ³•æ˜¾ç¤ºåˆ†å¸ƒå›¾")
            else:
                st.info("æš‚æ— ç•™å­˜ç‡åˆ†å¸ƒæ•°æ®")


def show_conversion_analysis_page():
    """æ˜¾ç¤ºè½¬åŒ–åˆ†æç»“æœé¡µé¢"""
    st.header(t('analysis.conversion_title'))
    
    # åˆå§‹åŒ–åˆ†æå¼•æ“
    if 'conversion_engine' not in st.session_state:
        st.session_state.conversion_engine = ConversionAnalysisEngine()
    if 'chart_generator' not in st.session_state:
        st.session_state.chart_generator = ChartGenerator()
    
    # æ¼æ–—é…ç½®
    with st.expander(t('analysis.conversion_funnel_config'), expanded=True):
        st.write(t('analysis.define_funnel_steps'))
        
        available_events = list(st.session_state.data_summary.get('event_types', {}).keys())
        
        funnel_steps = []
        for i in range(5):  # æœ€å¤š5ä¸ªæ­¥éª¤
            col1, col2 = st.columns([3, 1])
            with col1:
                step_event = st.selectbox(
                    f"{t('analysis.funnel_step')} {i+1}",
                    options=[""] + available_events,
                    key=f"funnel_step_{i}",
                    help=t('analysis.select_funnel_step_help').format(step=i+1)
                )
            with col2:
                if step_event:
                    funnel_steps.append(step_event)
                    st.success("âœ“")
                else:
                    break
        
        if len(funnel_steps) < 2:
            st.warning(t('analysis.funnel_steps_warning'))
    
    # æ‰§è¡Œè½¬åŒ–åˆ†æ
    if st.button(t('analysis.start_conversion_analysis'), type="primary") and len(funnel_steps) >= 2:
        with st.spinner(t('analysis.conversion_analysis_processing')):
            try:
                raw_data = st.session_state.raw_data
                engine = st.session_state.conversion_engine
                
                # æ„å»ºè½¬åŒ–æ¼æ–—
                funnel_result = engine.build_conversion_funnel(raw_data, funnel_steps)
                
                # è¯†åˆ«æµå¤±ç‚¹
                bottlenecks = engine.identify_drop_off_points(raw_data, funnel_steps)
                
                st.session_state.conversion_results = {
                    'funnel_data': funnel_result,
                    'bottlenecks': bottlenecks,
                    'funnel_steps': funnel_steps
                }
                
                st.success(t('analysis.conversion_analysis_complete'))
                
            except Exception as e:
                st.error(f"{t('analysis.conversion_analysis_failed')}: {str(e)}")
    
    # æ˜¾ç¤ºè½¬åŒ–åˆ†æç»“æœ
    if 'conversion_results' in st.session_state:
        results = st.session_state.conversion_results
        chart_gen = st.session_state.chart_generator
        
        st.markdown("---")
        st.subheader(t('analysis.conversion_analysis_results'))
        
        # è½¬åŒ–æ¼æ–—å›¾
        if 'funnel_data' in results:
            st.subheader(t('analysis.conversion_funnel'))
            try:
                funnel_chart = chart_gen.create_funnel_chart(results['funnel_data'])
                st.plotly_chart(funnel_chart, use_container_width=True)
            except Exception as e:
                st.error(f"{t('analysis.funnel_chart_failed')}: {str(e)}")
        
        # è½¬åŒ–æŒ‡æ ‡
        col1, col2, col3 = st.columns(3)
        
        if 'funnel_data' in results and results['funnel_data'] is not None:
            funnel_data = results['funnel_data']

            # æ£€æŸ¥æ•°æ®ç±»å‹å¹¶å¤„ç†
            if isinstance(funnel_data, pd.DataFrame) and not funnel_data.empty:
                is_valid_funnel = True
            elif isinstance(funnel_data, (dict, list)) and len(funnel_data) > 0:
                # å°è¯•è½¬æ¢ä¸ºDataFrame
                try:
                    funnel_data = pd.DataFrame(funnel_data)
                    is_valid_funnel = not funnel_data.empty
                except:
                    is_valid_funnel = False
            else:
                is_valid_funnel = False

            if is_valid_funnel:
                funnel_df = funnel_data  # Use the processed data

                with col1:
                    overall_conversion = funnel_df.iloc[-1]['user_count'] / funnel_df.iloc[0]['user_count'] * 100
                    st.metric("æ•´ä½“è½¬åŒ–ç‡", f"{overall_conversion:.1f}%")

                with col2:
                    total_users = funnel_df.iloc[0]['user_count']
                    st.metric("æ¼æ–—å…¥å£ç”¨æˆ·", f"{total_users:,}")

                with col3:
                    converted_users = funnel_df.iloc[-1]['user_count']
                    st.metric("æœ€ç»ˆè½¬åŒ–ç”¨æˆ·", f"{converted_users:,}")
        
        # ç“¶é¢ˆåˆ†æ
        if 'bottlenecks' in results and results['bottlenecks'] is not None:
            st.subheader("ğŸš¨ è½¬åŒ–ç“¶é¢ˆåˆ†æ")
            bottlenecks_data = results['bottlenecks']

            # å®‰å…¨åœ°å¤„ç†ç“¶é¢ˆæ•°æ®
            try:
                if isinstance(bottlenecks_data, list):
                    for bottleneck in bottlenecks_data:
                        if isinstance(bottleneck, dict):
                            step = bottleneck.get('step', 'æœªçŸ¥æ­¥éª¤')
                            drop_rate = bottleneck.get('drop_rate', 0)
                            st.warning(f"**{step}**: æµå¤±ç‡ {drop_rate:.1f}%")
                        else:
                            st.warning(f"ç“¶é¢ˆä¿¡æ¯: {str(bottleneck)}")
                elif isinstance(bottlenecks_data, dict):
                    # å¦‚æœæ˜¯å•ä¸ªç“¶é¢ˆå­—å…¸
                    step = bottlenecks_data.get('step', 'æœªçŸ¥æ­¥éª¤')
                    drop_rate = bottlenecks_data.get('drop_rate', 0)
                    st.warning(f"**{step}**: æµå¤±ç‡ {drop_rate:.1f}%")
                else:
                    st.info(f"ç“¶é¢ˆæ•°æ®æ ¼å¼: {type(bottlenecks_data).__name__}")
                    st.write(str(bottlenecks_data))
            except Exception as e:
                st.error(f"ç“¶é¢ˆåˆ†ææ˜¾ç¤ºå¤±è´¥: {str(e)}")
                st.info("è¯·æ£€æŸ¥ç“¶é¢ˆæ•°æ®æ ¼å¼")


def show_user_segmentation_page():
    """æ˜¾ç¤ºç”¨æˆ·åˆ†ç¾¤ç»“æœé¡µé¢"""
    st.header(t("analysis.segmentation_title"))
    
    # åˆå§‹åŒ–åˆ†æå¼•æ“
    if 'segmentation_engine' not in st.session_state:
        st.session_state.segmentation_engine = UserSegmentationEngine()
    if 'advanced_visualizer' not in st.session_state:
        st.session_state.advanced_visualizer = AdvancedVisualizer()
    
    # åˆ†ç¾¤é…ç½®
    with st.expander(t("analysis.segmentation_config"), expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            clustering_method = st.selectbox(
                t("analysis.clustering_algorithm"),
                options=["K-Means", "DBSCAN"],
                index=0
            )
        
        with col2:
            n_clusters = st.slider(
                t("analysis.cluster_count_label"),
                min_value=2,
                max_value=10,
                value=4,
                help=t("analysis.cluster_count_help")
            )
        
        with col3:
            feature_types = st.multiselect(
                t("analysis.feature_types"),
                options=[t("analysis.behavioral_features"), t("analysis.device_features"), t("analysis.geographic_features"), t("analysis.temporal_features")],
                default=[t("analysis.behavioral_features"), t("analysis.device_features")]
            )
    
    # æ‰§è¡Œç”¨æˆ·åˆ†ç¾¤
    if st.button(t("analysis.start_user_segmentation"), type="primary"):
        with st.spinner(t("analysis.user_segmentation_processing")):
            try:
                raw_data = st.session_state.raw_data
                engine = st.session_state.segmentation_engine

                # ç¡®ä¿raw_dataæ˜¯DataFrameæ ¼å¼
                if isinstance(raw_data, list):
                    events_df = pd.DataFrame(raw_data)
                elif isinstance(raw_data, pd.DataFrame):
                    events_df = raw_data
                else:
                    st.error(t("messages.data_format_not_supported"))
                    st.stop()

                # æå–ç”¨æˆ·ç‰¹å¾ - ä½¿ç”¨æ­£ç¡®çš„å‚æ•°
                user_features = engine.extract_user_features(events=events_df)

                if not user_features:
                    st.warning(t("analysis.cannot_extract_features"))
                    st.info(t("analysis.check_required_fields_segmentation"))
                    st.stop()

                # æ‰§è¡Œèšç±» - ä¼ å…¥ç”¨æˆ·ç‰¹å¾
                segmentation_result = engine.create_user_segments(
                    user_features=user_features,
                    method=clustering_method.lower().replace('-', ''),
                    n_clusters=n_clusters
                )

                st.session_state.segmentation_results = {
                    'segments': segmentation_result,
                    'user_features': user_features
                }

                st.success(t("analysis.user_segmentation_complete"))

            except Exception as e:
                st.error(f"{t('analysis.user_segmentation_failed')}: {str(e)}")
                st.error(f"{t('common.error')}: {type(e).__name__}")
                import traceback
                st.text(traceback.format_exc())
    
    # æ˜¾ç¤ºåˆ†ç¾¤ç»“æœ
    if 'segmentation_results' in st.session_state:
        results = st.session_state.segmentation_results
        visualizer = st.session_state.advanced_visualizer
        
        st.markdown("---")
        st.subheader(t("analysis.user_segmentation_results"))
        
        # åˆ†ç¾¤æ¦‚è§ˆ
        if 'segments' in results:
            segmentation_result = results['segments']

            # å®‰å…¨åœ°å¤„ç†SegmentationResultå¯¹è±¡
            if hasattr(segmentation_result, 'segments'):
                # å¦‚æœæ˜¯SegmentationResultå¯¹è±¡ï¼Œè·å–segmentsåˆ—è¡¨
                segments = segmentation_result.segments
            elif isinstance(segmentation_result, list):
                # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
                segments = segmentation_result
            else:
                # å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢
                segments = []
                st.warning(f"{t('analysis.unknown_segmentation_type')}: {type(segmentation_result)}")

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric(t("analysis.segment_count"), len(segments))
            with col2:
                # å®‰å…¨åœ°è®¡ç®—æ€»ç”¨æˆ·æ•°
                total_users = 0
                for seg in segments:
                    if hasattr(seg, 'user_count'):
                        # å¦‚æœæ˜¯UserSegmentå¯¹è±¡
                        total_users += seg.user_count
                    elif isinstance(seg, dict) and 'user_ids' in seg:
                        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
                        total_users += len(seg.get('user_ids', []))
                    elif isinstance(seg, dict) and 'user_count' in seg:
                        # å¦‚æœå­—å…¸ä¸­æœ‰user_count
                        total_users += seg.get('user_count', 0)

                st.metric(t("analysis.total_users"), f"{total_users:,}")
            with col3:
                avg_size = total_users / len(segments) if segments else 0
                st.metric(t("analysis.avg_segment_size"), f"{avg_size:.0f}")
        
        # åˆ†ç¾¤æ•£ç‚¹å›¾
        if 'user_features' in results and results['user_features'] is not None:
            st.subheader(t("analysis.user_segmentation_visualization"))
            try:
                # åˆ›å»ºç¤ºä¾‹æ•£ç‚¹å›¾æ•°æ®
                scatter_data = pd.DataFrame({
                    'user_id': [f'user_{i}' for i in range(100)],
                    'segment': [f'{t("analysis.segment_1") if i%4==0 else t("analysis.segment_2") if i%4==1 else t("analysis.segment_3") if i%4==2 else t("analysis.segment_4")}' for i in range(100)],
                    'x_feature': np.random.normal(0, 1, 100),
                    'y_feature': np.random.normal(0, 1, 100)
                })
                
                scatter_chart = visualizer.create_user_segmentation_scatter(scatter_data)
                st.plotly_chart(scatter_chart, use_container_width=True)
            except Exception as e:
                st.error(f"{t('analysis.scatter_plot_failed')}: {str(e)}")
        
        # åˆ†ç¾¤ç‰¹å¾é›·è¾¾å›¾
        st.subheader(t("analysis.segment_feature_comparison"))
        try:
            # åˆ›å»ºç¤ºä¾‹é›·è¾¾å›¾æ•°æ®
            radar_data = pd.DataFrame({
                'segment': [t('analysis.segment_1'), t('analysis.segment_1'), t('analysis.segment_1'), t('analysis.segment_2'), t('analysis.segment_2'), t('analysis.segment_2')],
                'feature_name': [t('analysis.activity_level'), t('analysis.conversion_rate'), t('analysis.retention_rate'), t('analysis.activity_level'), t('analysis.conversion_rate'), t('analysis.retention_rate')],
                'feature_value': [0.8, 0.3, 0.6, 0.5, 0.7, 0.4]
            })
            
            radar_chart = visualizer.create_feature_radar_chart(radar_data)
            st.plotly_chart(radar_chart, use_container_width=True)
        except Exception as e:
            st.error(f"{t('analysis.radar_chart_failed')}: {str(e)}")


def show_path_analysis_page():
    """æ˜¾ç¤ºè·¯å¾„åˆ†æç»“æœé¡µé¢"""
    st.header(t("analysis.path_title"))
    
    # åˆå§‹åŒ–åˆ†æå¼•æ“
    if 'path_engine' not in st.session_state:
        st.session_state.path_engine = PathAnalysisEngine()
    if 'advanced_visualizer' not in st.session_state:
        st.session_state.advanced_visualizer = AdvancedVisualizer()
    
    # è·¯å¾„åˆ†æé…ç½®
    with st.expander(t("analysis.path_analysis_config"), expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            session_timeout = st.slider(
                t("analysis.session_timeout_minutes"),
                min_value=10,
                max_value=120,
                value=30,
                help=t("analysis.session_timeout_help")
            )
        
        with col2:
            min_path_length = st.slider(
                t("analysis.min_path_length_label"),
                min_value=2,
                max_value=10,
                value=3,
                help=t("analysis.min_path_length_help")
            )
        
        with col3:
            top_paths = st.slider(
                t("analysis.path_count_display"),
                min_value=5,
                max_value=20,
                value=10,
                help=t("analysis.path_count_display_help")
            )
    
    # æ‰§è¡Œè·¯å¾„åˆ†æ
    if st.button(t("analysis.start_path_analysis"), type="primary"):
        with st.spinner(t("analysis.path_analysis_processing")):
            try:
                raw_data = st.session_state.raw_data
                engine = st.session_state.path_engine
                
                # è®¾ç½®ä¼šè¯è¶…æ—¶æ—¶é—´
                engine.session_timeout_minutes = session_timeout

                # ç¡®ä¿raw_dataæ˜¯DataFrameæ ¼å¼
                if isinstance(raw_data, list):
                    events_df = pd.DataFrame(raw_data)
                elif isinstance(raw_data, pd.DataFrame):
                    events_df = raw_data
                else:
                    st.error(t("messages.data_format_not_supported"))
                    st.stop()

                # é‡æ„ç”¨æˆ·ä¼šè¯
                sessions = engine.reconstruct_user_sessions(events=events_df)
                
                # è¯†åˆ«è·¯å¾„æ¨¡å¼
                path_analysis_result = engine.identify_path_patterns(
                    sessions=sessions,
                    min_length=min_path_length,
                    max_length=10  # è®¾ç½®æœ€å¤§è·¯å¾„é•¿åº¦
                )

                # æå–å¸¸è§è·¯å¾„ï¼ˆé™åˆ¶æ•°é‡ï¼‰
                common_paths = path_analysis_result.common_patterns[:top_paths] if path_analysis_result.common_patterns else []
                
                st.session_state.path_results = {
                    'sessions': sessions,
                    'common_paths': common_paths
                }
                
                st.success(t("analysis.path_analysis_complete"))
                
            except Exception as e:
                st.error(f"{t('analysis.path_analysis_failed')}: {str(e)}")
    
    # æ˜¾ç¤ºè·¯å¾„åˆ†æç»“æœ
    if 'path_results' in st.session_state:
        results = st.session_state.path_results
        visualizer = st.session_state.advanced_visualizer
        
        st.markdown("---")
        st.subheader(t("analysis.path_analysis_results"))
        
        # è·¯å¾„ç»Ÿè®¡
        if 'sessions' in results:
            sessions = results['sessions']
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric(t("analysis.total_sessions"), f"{len(sessions):,}")
            with col2:
                # å®‰å…¨åœ°å¤„ç†UserSessionå¯¹è±¡çš„path_sequenceå±æ€§
                path_lengths = []
                for s in sessions:
                    if hasattr(s, 'path_sequence') and s.path_sequence:
                        path_lengths.append(len(s.path_sequence))
                    else:
                        path_lengths.append(0)
                avg_length = np.mean(path_lengths) if path_lengths else 0
                st.metric(t("analysis.avg_path_length"), f"{avg_length:.1f}")
            with col3:
                # å®‰å…¨åœ°å¤„ç†UserSessionå¯¹è±¡çš„path_sequenceå±æ€§
                unique_paths_set = set()
                for s in sessions:
                    if hasattr(s, 'path_sequence') and s.path_sequence:
                        unique_paths_set.add(tuple(s.path_sequence))
                unique_paths = len(unique_paths_set)
                st.metric(t("analysis.unique_paths_count"), f"{unique_paths:,}")
        
        # ç”¨æˆ·è¡Œä¸ºæµç¨‹å›¾
        st.subheader(t("analysis.user_behavior_flow"))
        try:
            # åˆ›å»ºç¤ºä¾‹æµç¨‹æ•°æ®
            flow_data = pd.DataFrame({
                'source': [t('analysis.home_page'), t('analysis.home_page'), t('analysis.product_page'), t('analysis.product_page'), t('analysis.cart_page')],
                'target': [t('analysis.product_page'), t('analysis.search_page'), t('analysis.cart_page'), t('analysis.detail_page'), t('analysis.checkout_page')],
                'value': [100, 50, 80, 30, 60]
            })
            
            flow_chart = visualizer.create_user_behavior_flow(flow_data)
            st.plotly_chart(flow_chart, use_container_width=True)
        except Exception as e:
            st.error(f"{t('analysis.flow_chart_failed')}: {str(e)}")
        
        # å¸¸è§è·¯å¾„åˆ—è¡¨
        if 'common_paths' in results and results['common_paths'] is not None:
            st.subheader(t("analysis.most_common_paths"))
            common_paths_data = results['common_paths']

            # å®‰å…¨åœ°åˆ›å»ºDataFrame
            try:
                if isinstance(common_paths_data, pd.DataFrame):
                    paths_df = common_paths_data
                elif isinstance(common_paths_data, (list, dict)):
                    paths_df = pd.DataFrame(common_paths_data)
                else:
                    paths_df = pd.DataFrame()

                if not paths_df.empty:
                    st.dataframe(paths_df, use_container_width=True)
                else:
                    st.info(t("analysis.no_common_paths_data"))

            except Exception as e:
                st.error(f"{t('analysis.path_data_display_failed')}: {str(e)}")
                st.info(t("analysis.check_path_data_format"))


def show_comprehensive_report_page():
    """æ˜¾ç¤ºç»¼åˆåˆ†ææŠ¥å‘Šé¡µé¢"""
    st.header(t("analysis.report_title"))
    
    # æŠ¥å‘Šé…ç½®
    with st.expander("ğŸ“Š æŠ¥å‘Šé…ç½®", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            report_sections = st.multiselect(
                "é€‰æ‹©æŠ¥å‘Šç« èŠ‚",
                options=["æ•°æ®æ¦‚è§ˆ", "äº‹ä»¶åˆ†æ", "ç•™å­˜åˆ†æ", "è½¬åŒ–åˆ†æ", "ç”¨æˆ·åˆ†ç¾¤", "è·¯å¾„åˆ†æ"],
                default=["æ•°æ®æ¦‚è§ˆ", "äº‹ä»¶åˆ†æ", "ç•™å­˜åˆ†æ"]
            )
        
        with col2:
            export_format = st.selectbox(
                "å¯¼å‡ºæ ¼å¼",
                options=["HTML", "PDF", "JSON"],
                index=0
            )
    
    # ç”ŸæˆæŠ¥å‘Š
    if st.button("ğŸ“ ç”Ÿæˆç»¼åˆæŠ¥å‘Š", type="primary"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆç»¼åˆæŠ¥å‘Š..."):
            try:
                # æ±‡æ€»æ‰€æœ‰åˆ†æç»“æœ
                report_data = {
                    'data_summary': st.session_state.get('data_summary', {}),
                    'event_analysis': st.session_state.get('event_analysis_results', {}),
                    'retention_analysis': st.session_state.get('retention_results', {}),
                    'conversion_analysis': st.session_state.get('conversion_results', {}),
                    'segmentation_analysis': st.session_state.get('segmentation_results', {}),
                    'path_analysis': st.session_state.get('path_results', {})
                }
                
                st.session_state.comprehensive_report = report_data
                st.success("âœ… ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
                
            except Exception as e:
                st.error(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
    
    # æ˜¾ç¤ºç»¼åˆæŠ¥å‘Š
    if 'comprehensive_report' in st.session_state:
        report = st.session_state.comprehensive_report
        
        st.markdown("---")
        
        # æ•°æ®æ¦‚è§ˆ
        if "æ•°æ®æ¦‚è§ˆ" in report_sections and 'data_summary' in report:
            st.subheader("ğŸ“Š æ•°æ®æ¦‚è§ˆ")
            summary = report['data_summary']
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æ€»äº‹ä»¶æ•°", f"{summary.get('total_events', 0):,}")
            with col2:
                st.metric("ç‹¬ç«‹ç”¨æˆ·æ•°", f"{summary.get('unique_users', 0):,}")
            with col3:
                st.metric("äº‹ä»¶ç±»å‹æ•°", len(summary.get('event_types', {})))
            with col4:
                date_range = summary.get('date_range', {})
                days = (pd.to_datetime(date_range.get('end', '')) - pd.to_datetime(date_range.get('start', ''))).days
                st.metric("æ•°æ®å¤©æ•°", f"{days}å¤©")
        
        # å…³é”®æ´å¯Ÿ
        st.subheader("ğŸ’¡ å…³é”®æ´å¯Ÿ")
        insights = [
            "ç”¨æˆ·æ´»è·ƒåº¦åœ¨å·¥ä½œæ—¥è¾ƒé«˜ï¼Œå‘¨æœ«æœ‰æ‰€ä¸‹é™",
            "ç§»åŠ¨ç«¯ç”¨æˆ·å æ¯”é€æ¸å¢åŠ ï¼Œéœ€è¦ä¼˜åŒ–ç§»åŠ¨ä½“éªŒ",
            "æ–°ç”¨æˆ·ç•™å­˜ç‡åœ¨ç¬¬7å¤©å‡ºç°æ˜æ˜¾ä¸‹é™",
            "è´­ä¹°è½¬åŒ–æ¼æ–—åœ¨è´­ç‰©è½¦ç¯èŠ‚æµå¤±ç‡æœ€é«˜",
            "é«˜ä»·å€¼ç”¨æˆ·ç¾¤ä½“ä¸»è¦é›†ä¸­åœ¨25-35å²å¹´é¾„æ®µ"
        ]
        
        for insight in insights:
            st.info(f"â€¢ {insight}")
        
        # è¡ŒåŠ¨å»ºè®®
        st.subheader("ğŸ¯ è¡ŒåŠ¨å»ºè®®")
        recommendations = [
            "**ä¼˜åŒ–ç§»åŠ¨ç«¯ä½“éªŒ**: é’ˆå¯¹ç§»åŠ¨ç«¯ç”¨æˆ·å¢é•¿è¶‹åŠ¿ï¼Œä¼˜åŒ–ç§»åŠ¨ç«¯ç•Œé¢å’Œäº¤äº’",
            "**æ”¹å–„æ–°ç”¨æˆ·å¼•å¯¼**: åŠ å¼ºæ–°ç”¨æˆ·ç¬¬ä¸€å‘¨çš„å¼•å¯¼å’Œæ¿€åŠ±æœºåˆ¶",
            "**ä¼˜åŒ–è´­ç‰©è½¦æµç¨‹**: ç®€åŒ–è´­ç‰©è½¦åˆ°ç»“ç®—çš„æµç¨‹ï¼Œå‡å°‘ç”¨æˆ·æµå¤±",
            "**ç²¾å‡†è¥é”€**: é’ˆå¯¹é«˜ä»·å€¼ç”¨æˆ·ç¾¤ä½“åˆ¶å®šä¸ªæ€§åŒ–è¥é”€ç­–ç•¥",
            "**æå‡å‘¨æœ«æ´»è·ƒåº¦**: æ¨å‡ºå‘¨æœ«ä¸“å±æ´»åŠ¨å’Œä¼˜æƒ æ¥æå‡ç”¨æˆ·æ´»è·ƒåº¦"
        ]
        
        for rec in recommendations:
            st.success(rec)
        
        # å¯¼å‡ºæŠ¥å‘Š
        st.markdown("---")
        st.subheader(t('analysis.report_export_header'))
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            export_format_final = st.selectbox(
                t('analysis.select_export_format_label'),
                options=["JSON", "PDF", "Excel"],
                index=0,
                key="final_export_format"
            )
        
        with col2:
            include_raw_data = st.checkbox(
                t('analysis.include_raw_data_checkbox'),
                value=False,
                help=t('analysis.include_raw_data_help')
            )
        
        with col3:
            custom_filename = st.text_input(
                t('analysis.custom_filename_label'),
                value="",
                placeholder=t('analysis.custom_filename_placeholder')
            )
        
        if st.button(t('analysis.export_report_button_format').format(format=export_format_final), type="primary"):
            with st.spinner(t('analysis.exporting_report_status').format(format=export_format_final)):
                try:
                    # åˆå§‹åŒ–æŠ¥å‘Šå¯¼å‡ºå™¨
                    exporter = ReportExporter()
                    
                    # å‡†å¤‡æŠ¥å‘Šæ•°æ®
                    export_data = {
                        'metadata': {
                            'generated_at': datetime.now().isoformat(),
                            'report_sections': report_sections,
                            'data_summary': report.get('data_summary', {})
                        },
                        'executive_summary': {
                            'key_metrics': {
                                'total_events': report.get('data_summary', {}).get('total_events', 0),
                                'unique_users': report.get('data_summary', {}).get('unique_users', 0),
                                'event_types': len(report.get('data_summary', {}).get('event_types', {}))
                            },
                            'key_insights': insights,
                            'recommendations': recommendations
                        },
                        'detailed_analysis': {}
                    }
                    
                    # æ·»åŠ é€‰ä¸­çš„åˆ†æç»“æœ
                    if t('analysis.event_analysis_option_report') in report_sections and 'event_analysis' in report:
                        export_data['detailed_analysis']['event_analysis'] = report['event_analysis']
                    
                    if t('analysis.retention_analysis_option_report') in report_sections and 'retention_analysis' in report:
                        export_data['detailed_analysis']['retention_analysis'] = report['retention_analysis']
                    
                    if t('analysis.conversion_analysis_option_report') in report_sections and 'conversion_analysis' in report:
                        export_data['detailed_analysis']['conversion_analysis'] = report['conversion_analysis']
                    
                    if t('analysis.user_segmentation_option_report') in report_sections and 'segmentation_analysis' in report:
                        export_data['detailed_analysis']['user_segmentation'] = report['segmentation_analysis']
                    
                    if t('analysis.path_analysis_option_report') in report_sections and 'path_analysis' in report:
                        export_data['detailed_analysis']['path_analysis'] = report['path_analysis']
                    
                    # å¦‚æœé€‰æ‹©åŒ…å«åŸå§‹æ•°æ®
                    if include_raw_data and st.session_state.get('raw_data') is not None:
                        # åªåŒ…å«æ•°æ®æ‘˜è¦ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§
                        raw_data = st.session_state.raw_data
                        export_data['raw_data_summary'] = {
                            'total_records': len(raw_data),
                            'columns': list(raw_data.columns),
                            'sample_records': raw_data.head(5).to_dict('records')
                        }
                    
                    # ç”Ÿæˆæ–‡ä»¶å
                    if custom_filename:
                        filename = f"{custom_filename}.{export_format_final.lower()}"
                    else:
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"comprehensive_report_{timestamp}.{export_format_final.lower()}"
                    
                    output_path = f"reports/{filename}"
                    
                    # æ‰§è¡Œå¯¼å‡º
                    result = exporter.export_report(export_data, export_format_final.lower(), output_path)
                    
                    if result['status'] == 'success':
                        st.success(t('analysis.report_export_success_msg'))
                        st.info(f"{t('analysis.file_path_label')}: {result['file_path']}")
                        st.info(t('analysis.file_size_label').format(size=f"{result['file_size'] / 1024:.1f}"))
                        
                        # æä¾›ä¸‹è½½é“¾æ¥
                        try:
                            with open(result['file_path'], 'rb') as f:
                                file_data = f.read()
                            
                            st.download_button(
                                label=t('analysis.download_report_button_format').format(format=export_format_final),
                                data=file_data,
                                file_name=filename,
                                mime=get_mime_type(export_format_final.lower())
                            )
                        except Exception as e:
                            st.warning(f"{t('analysis.download_button_creation_failed')}: {str(e)}")
                    else:
                        st.error(f"{t('analysis.export_report_failed')}: {result['message']}")
                        
                except Exception as e:
                    st.error(f"{t('analysis.export_process_error_msg')}: {str(e)}")
    



def show_intelligent_analysis_page():
    """æ˜¾ç¤ºæ™ºèƒ½åˆ†æé¡µé¢"""
    st.header(t('analysis.intelligent_title'))
    
    st.markdown(t('analysis.intelligent_description'))
    
    # åˆ†æé…ç½®
    st.subheader(t('analysis.analysis_config_header'))
    
    col1, col2 = st.columns(2)
    
    with col1:
        analysis_types = st.multiselect(
            t('analysis.select_analysis_types'),
            options=[
                'event_analysis',
                'retention_analysis', 
                'conversion_analysis',
                'user_segmentation',
                'path_analysis'
            ],
            default=[
                'event_analysis',
                'retention_analysis', 
                'conversion_analysis'
            ],
            format_func=lambda x: {
                'event_analysis': t('analysis.event_analysis_option'),
                'retention_analysis': t('analysis.retention_analysis_option'),
                'conversion_analysis': t('analysis.conversion_analysis_option'),
                'user_segmentation': t('analysis.user_segmentation_option'),
                'path_analysis': t('analysis.path_analysis_option')
            }.get(x, x)
        )
        
        enable_parallel = st.checkbox(
            t('analysis.enable_parallel_processing'),
            value=True,
            help=t('analysis.parallel_processing_help')
        )
    
    with col2:
        max_workers = st.slider(
            t('analysis.max_parallel_tasks'),
            min_value=1,
            max_value=8,
            value=4,
            disabled=not enable_parallel
        )
        
        enable_caching = st.checkbox(
            t('analysis.enable_intelligent_cache'),
            value=True,
            help=t('analysis.intelligent_cache_help')
        )
    
    # æ‰§è¡Œåˆ†æ
    st.subheader(t('analysis.execute_analysis_header'))
    
    if not analysis_types:
        st.warning(t('analysis.select_analysis_type_warning'))
        return
    
    col1, col2, col3 = st.columns([1, 1, 2])
    
    with col1:
        if st.button(t('analysis.start_intelligent_analysis'), type="primary"):
            execute_intelligent_analysis(analysis_types, enable_parallel, max_workers, enable_caching)
    
    with col2:
        if st.button(t('analysis.view_system_status')):
            show_system_status()
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    if st.session_state.workflow_results:
        show_workflow_results()


def execute_intelligent_analysis(analysis_types, enable_parallel, max_workers, enable_caching):
    """æ‰§è¡Œæ™ºèƒ½åˆ†æ"""
    
    # æ›´æ–°é›†æˆç®¡ç†å™¨é…ç½®
    integration_manager = st.session_state.integration_manager
    integration_manager.config.enable_parallel_processing = enable_parallel
    integration_manager.config.max_workers = max_workers
    integration_manager.config.enable_caching = enable_caching
    
    # åˆ›å»ºè¿›åº¦å®¹å™¨
    progress_container = st.container()
    
    with progress_container:
        st.subheader("âš™ï¸ æ™ºèƒ½åˆ†æè¿›åº¦")
        
        # åˆ›å»ºè¿›åº¦æ¡å’ŒçŠ¶æ€æ˜¾ç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()
        metrics_container = st.empty()
        
        try:
            # å‡†å¤‡ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            upload_dir = Path("data/uploads")
            upload_dir.mkdir(parents=True, exist_ok=True)
            
            # ä»ä¼šè¯çŠ¶æ€è·å–åŸå§‹æ•°æ®å¹¶ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            if st.session_state.raw_data is not None:
                temp_file_path = upload_dir / f"temp_analysis_{int(time.time())}.ndjson"
                
                # å°†DataFrameè½¬æ¢å›NDJSONæ ¼å¼
                with open(temp_file_path, 'w', encoding='utf-8') as f:
                    for _, row in st.session_state.raw_data.iterrows():
                        # é‡æ„åŸå§‹äº‹ä»¶æ ¼å¼
                        event_data = {
                            'event_date': row.get('event_date', ''),
                            'event_timestamp': row.get('event_timestamp', 0),
                            'event_name': row.get('event_name', ''),
                            'user_pseudo_id': row.get('user_pseudo_id', ''),
                            'user_id': row.get('user_id'),
                            'platform': row.get('platform', 'WEB'),
                            'device': row.get('device', {}),
                            'geo': row.get('geo', {}),
                            'traffic_source': row.get('traffic_source', {}),
                            'event_params': row.get('event_params', []),
                            'user_properties': row.get('user_properties', []),
                            'items': row.get('items', [])
                        }
                        f.write(json.dumps(event_data, ensure_ascii=False) + '\n')
                
                status_text.text("ğŸš€ å¯åŠ¨æ™ºèƒ½åˆ†æå·¥ä½œæµç¨‹...")
                progress_bar.progress(10)
                
                # æ‰§è¡Œå®Œæ•´å·¥ä½œæµç¨‹
                start_time = time.time()
                
                result = integration_manager.execute_complete_workflow(
                    file_path=str(temp_file_path),
                    analysis_types=analysis_types
                )
                
                end_time = time.time()
                total_time = end_time - start_time
                
                # æ›´æ–°è¿›åº¦
                progress_bar.progress(100)
                status_text.text("âœ… æ™ºèƒ½åˆ†æå®Œæˆ!")
                
                # æ˜¾ç¤ºæ‰§è¡ŒæŒ‡æ ‡
                with metrics_container.container():
                    col1, col2, col3, col4 = st.columns(4)
                    
                    execution_summary = result['execution_summary']
                    
                    with col1:
                        st.metric(
                            "æ€»æ‰§è¡Œæ—¶é—´",
                            f"{total_time:.2f}ç§’",
                            help="å®Œæ•´å·¥ä½œæµç¨‹çš„æ‰§è¡Œæ—¶é—´"
                        )
                    
                    with col2:
                        st.metric(
                            "æˆåŠŸåˆ†æ",
                            execution_summary['successful_analyses'],
                            help="æˆåŠŸå®Œæˆçš„åˆ†æä»»åŠ¡æ•°é‡"
                        )
                    
                    with col3:
                        st.metric(
                            "å¤„ç†æ•°æ®é‡",
                            f"{execution_summary['data_size']:,}",
                            help="å¤„ç†çš„äº‹ä»¶è®°å½•æ•°é‡"
                        )
                    
                    with col4:
                        processing_rate = execution_summary['data_size'] / total_time if total_time > 0 else 0
                        st.metric(
                            "å¤„ç†é€Ÿç‡",
                            f"{processing_rate:.0f}/ç§’",
                            help="æ¯ç§’å¤„ç†çš„è®°å½•æ•°é‡"
                        )
                
                # ä¿å­˜ç»“æœåˆ°ä¼šè¯çŠ¶æ€
                st.session_state.workflow_results = result
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_file_path.exists():
                    temp_file_path.unlink()
                
                st.success("ğŸ‰ æ™ºèƒ½åˆ†ææˆåŠŸå®Œæˆ!")
                st.rerun()
                
            else:
                st.error("âŒ æ— æ³•è·å–åŸå§‹æ•°æ®ï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡ä»¶")
                
        except Exception as e:
            st.error(f"âŒ æ™ºèƒ½åˆ†æå¤±è´¥: {str(e)}")
            progress_bar.progress(0)
            status_text.text("âŒ åˆ†æå¤±è´¥")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if 'temp_file_path' in locals() and temp_file_path.exists():
                temp_file_path.unlink()


def show_system_status():
    """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
    st.subheader("ğŸ“Š ç³»ç»ŸçŠ¶æ€ç›‘æ§")
    
    integration_manager = st.session_state.integration_manager
    
    # è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€
    health = integration_manager.get_system_health()
    
    # æ˜¾ç¤ºæ•´ä½“çŠ¶æ€
    col1, col2, col3 = st.columns(3)
    
    with col1:
        status_color = {
            'healthy': 'normal',
            'warning': 'inverse', 
            'critical': 'off'
        }.get(health['overall_status'], 'normal')
        
        st.metric(
            "ç³»ç»ŸçŠ¶æ€",
            health['overall_status'].upper(),
            delta_color=status_color
        )
    
    with col2:
        st.metric(
            "æ´»è·ƒå·¥ä½œæµç¨‹",
            health['active_workflows']
        )
    
    with col3:
        st.metric(
            "ç¼“å­˜é¡¹æ•°é‡",
            health['cache_size']
        )
    
    # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
    if health.get('current_metrics'):
        st.subheader("âš¡ æ€§èƒ½æŒ‡æ ‡")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            cpu_usage = health['current_metrics'].get('cpu_usage', 0)
            st.metric(
                "CPUä½¿ç”¨ç‡",
                f"{cpu_usage:.1f}%",
                delta=f"{cpu_usage - health.get('average_metrics', {}).get('cpu_usage', cpu_usage):.1f}%"
            )
        
        with col2:
            memory_usage = health['current_metrics'].get('memory_usage', 0)
            st.metric(
                "å†…å­˜ä½¿ç”¨ç‡", 
                f"{memory_usage:.1f}%",
                delta=f"{memory_usage - health.get('average_metrics', {}).get('memory_usage', memory_usage):.1f}%"
            )
        
        with col3:
            memory_available = health['current_metrics'].get('memory_available', 0)
            st.metric(
                "å¯ç”¨å†…å­˜",
                f"{memory_available:.1f}GB"
            )
    
    # æ˜¾ç¤ºå·¥ä½œæµç¨‹å†å²
    workflow_history = integration_manager.get_execution_history()
    if workflow_history:
        st.subheader("ğŸ“ˆ å·¥ä½œæµç¨‹å†å²")
        
        # åˆ›å»ºå†å²æ•°æ®è¡¨æ ¼
        history_data = []
        for record in workflow_history[-10:]:  # æ˜¾ç¤ºæœ€è¿‘10ä¸ª
            history_data.append({
                'å·¥ä½œæµç¨‹ID': record.get('workflow_id', 'N/A')[:20] + '...',
                'çŠ¶æ€': record.get('status', 'unknown'),
                'å¼€å§‹æ—¶é—´': record.get('start_time', 'N/A'),
                'åˆ†æç±»å‹': ', '.join(record.get('analysis_types', [])),
                'æ–‡ä»¶è·¯å¾„': Path(record.get('file_path', '')).name if record.get('file_path') else 'N/A'
            })
        
        if history_data:
            df = pd.DataFrame(history_data)
            st.dataframe(df, use_container_width=True)
    
    # ç³»ç»Ÿæ“ä½œ
    st.subheader("ğŸ”§ ç³»ç»Ÿæ“ä½œ")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ§¹ æ¸…ç†ç¼“å­˜"):
            integration_manager._cleanup_cache()
            st.success("âœ… ç¼“å­˜å·²æ¸…ç†")
            st.rerun()
    
    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…ç†å†…å­˜"):
            integration_manager._trigger_memory_cleanup()
            st.success("âœ… å†…å­˜å·²æ¸…ç†")
            st.rerun()
    
    with col3:
        if st.button("ğŸ”„ é‡ç½®çŠ¶æ€"):
            integration_manager.reset_execution_state()
            st.success("âœ… æ‰§è¡ŒçŠ¶æ€å·²é‡ç½®")
            st.rerun()


def show_workflow_results():
    """æ˜¾ç¤ºå·¥ä½œæµç¨‹ç»“æœ"""
    st.subheader("ğŸ“‹ åˆ†æç»“æœ")
    
    result = st.session_state.workflow_results
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š ç»“æœæ¦‚è§ˆ", "ğŸ” è¯¦ç»†åˆ†æ", "ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨", "ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š"])
    
    with tab1:
        show_results_overview(result)
    
    with tab2:
        show_detailed_analysis(result)
    
    with tab3:
        show_visualizations(result)
    
    with tab4:
        show_export_options(result)


def show_results_overview(result):
    """æ˜¾ç¤ºç»“æœæ¦‚è§ˆ"""
    st.write("### ğŸ“Š æ‰§è¡Œæ¦‚è§ˆ")
    
    execution_summary = result['execution_summary']
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "å·¥ä½œæµç¨‹ID",
            result['workflow_id'][:12] + "...",
            help=f"å®Œæ•´ID: {result['workflow_id']}"
        )
    
    with col2:
        st.metric(
            "æ€»æ‰§è¡Œæ—¶é—´",
            f"{execution_summary['total_execution_time']:.2f}ç§’"
        )
    
    with col3:
        st.metric(
            "æˆåŠŸåˆ†æ",
            f"{execution_summary['successful_analyses']}/{execution_summary['successful_analyses'] + execution_summary['failed_analyses']}"
        )
    
    with col4:
        st.metric(
            "æ•°æ®è§„æ¨¡",
            f"{execution_summary['data_size']:,} æ¡"
        )
    
    # åˆ†æç»“æœçŠ¶æ€
    st.write("### ğŸ“ˆ åˆ†ææ¨¡å—çŠ¶æ€")
    
    analysis_results = result['analysis_results']
    
    for analysis_type, analysis_result in analysis_results.items():
        with st.expander(f"{'âœ…' if analysis_result.status == 'completed' else 'âŒ'} {analysis_type.replace('_', ' ').title()}",
                        expanded=analysis_result.status == 'completed'):
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write(f"**çŠ¶æ€**: {analysis_result.status}")
                st.write(f"**æ‰§è¡Œæ—¶é—´**: {analysis_result.execution_time:.2f}ç§’")
                st.write(f"**æ´å¯Ÿæ•°é‡**: {len(analysis_result.insights)}")
                st.write(f"**å»ºè®®æ•°é‡**: {len(analysis_result.recommendations)}")
            
            with col2:
                if analysis_result.insights:
                    st.write("**ä¸»è¦æ´å¯Ÿ**:")
                    for insight in analysis_result.insights[:3]:
                        st.write(f"â€¢ {insight}")

                    if len(analysis_result.insights) > 3:
                        st.write(f"... è¿˜æœ‰ {len(analysis_result.insights) - 3} ä¸ªæ´å¯Ÿ")


def show_detailed_analysis(result):
    """æ˜¾ç¤ºè¯¦ç»†åˆ†æ"""
    st.write("### ğŸ” è¯¦ç»†åˆ†æç»“æœ")
    
    analysis_results = result['analysis_results']
    
    # é€‰æ‹©è¦æŸ¥çœ‹çš„åˆ†æ
    selected_analysis = st.selectbox(
        "é€‰æ‹©åˆ†ææ¨¡å—",
        options=list(analysis_results.keys()),
        format_func=lambda x: x.replace('_', ' ').title()
    )
    
    if selected_analysis:
        analysis_result = analysis_results[selected_analysis]
        
        if analysis_result.status == 'completed':
            # æ˜¾ç¤ºæ´å¯Ÿ
            if analysis_result.insights:
                st.write("#### ğŸ’¡ å…³é”®æ´å¯Ÿ")
                for i, insight in enumerate(analysis_result.insights, 1):
                    st.write(f"{i}. {insight}")

            # æ˜¾ç¤ºå»ºè®®
            if analysis_result.recommendations:
                st.write("#### ğŸ¯ è¡ŒåŠ¨å»ºè®®")
                for i, recommendation in enumerate(analysis_result.recommendations, 1):
                    st.write(f"{i}. {recommendation}")

            # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
            if hasattr(analysis_result, 'data') and analysis_result.data:
                st.write("#### ğŸ“Š æ•°æ®æ‘˜è¦")
                data_summary = analysis_result.data
                
                summary_data = []
                for key, value in data_summary.items():
                    if isinstance(value, dict):
                        summary_data.append({
                            'å­—æ®µ': key,
                            'ç±»å‹': value.get('type', 'unknown'),
                            'æè¿°': str(value)[:100] + '...' if len(str(value)) > 100 else str(value)
                        })
                
                if summary_data:
                    df = pd.DataFrame(summary_data)
                    st.dataframe(df, use_container_width=True)
        
        else:
            st.error(f"âŒ åˆ†æå¤±è´¥: {getattr(analysis_result, 'error_message', 'æœªçŸ¥é”™è¯¯')}")


def show_visualizations(result):
    """æ˜¾ç¤ºå¯è§†åŒ–å›¾è¡¨"""
    st.write("### ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨")
    
    visualizations = result.get('visualizations', {})
    
    if not visualizations:
        st.info("ğŸ“Š æš‚æ— å¯è§†åŒ–å›¾è¡¨æ•°æ®")
        return
    
    # é€‰æ‹©è¦æŸ¥çœ‹çš„å¯è§†åŒ–
    viz_options = []
    for analysis_type, viz_data in visualizations.items():
        if viz_data:
            for viz_name in viz_data.keys():
                viz_options.append(f"{analysis_type} - {viz_name}")
    
    if viz_options:
        selected_viz = st.selectbox("é€‰æ‹©å›¾è¡¨", viz_options)
        
        if selected_viz:
            analysis_type, viz_name = selected_viz.split(' - ', 1)
            viz_data = visualizations[analysis_type][viz_name]

            # æ˜¾ç¤ºå›¾è¡¨ä¿¡æ¯
            st.info(f"ğŸ“Š å›¾è¡¨: {viz_name}")

            # æ£€æŸ¥æ˜¯å¦ä¸ºPlotlyå›¾è¡¨å¯¹è±¡
            try:
                import plotly.graph_objects as go

                if isinstance(viz_data, go.Figure):
                    # å¦‚æœæ˜¯Plotlyå›¾è¡¨ï¼Œç›´æ¥æ˜¾ç¤º
                    st.plotly_chart(viz_data, use_container_width=True)
                elif hasattr(viz_data, 'to_json'):
                    # å¦‚æœæœ‰to_jsonæ–¹æ³•ï¼Œå°è¯•è½¬æ¢ä¸ºPlotlyå›¾è¡¨
                    st.plotly_chart(viz_data, use_container_width=True)
                else:
                    # å¦‚æœæ˜¯å…¶ä»–æ•°æ®ç±»å‹ï¼Œå°è¯•æ˜¾ç¤ºä¸ºJSON
                    try:
                        if isinstance(viz_data, dict):
                            st.json(viz_data)
                        else:
                            st.write("å›¾è¡¨æ•°æ®æ ¼å¼:", type(viz_data).__name__)
                            st.write(str(viz_data)[:1000] + "..." if len(str(viz_data)) > 1000 else str(viz_data))
                    except Exception as json_error:
                        st.error(f"æ— æ³•æ˜¾ç¤ºå›¾è¡¨æ•°æ®: {str(json_error)}")
                        st.write("æ•°æ®ç±»å‹:", type(viz_data).__name__)

            except Exception as e:
                st.error(f"å›¾è¡¨æ˜¾ç¤ºå¤±è´¥: {str(e)}")
                st.write("æ•°æ®ç±»å‹:", type(viz_data).__name__)
                # å°è¯•æ˜¾ç¤ºéƒ¨åˆ†æ•°æ®ç”¨äºè°ƒè¯•
                try:
                    st.write("æ•°æ®é¢„è§ˆ:", str(viz_data)[:500] + "..." if len(str(viz_data)) > 500 else str(viz_data))
                except:
                    st.write("æ— æ³•é¢„è§ˆæ•°æ®")
    else:
        st.info("ğŸ“Š æš‚æ— å¯ç”¨çš„å¯è§†åŒ–å›¾è¡¨")


def show_export_options(result):
    """æ˜¾ç¤ºå¯¼å‡ºé€‰é¡¹"""
    st.write("### ğŸ“¥ å¯¼å‡ºåˆ†ææŠ¥å‘Š")
    
    col1, col2 = st.columns(2)
    
    with col1:
        export_format = st.selectbox(
            "å¯¼å‡ºæ ¼å¼",
            options=['json', 'pdf', 'excel'],
            format_func=lambda x: {
                'json': 'JSON æ ¼å¼',
                'pdf': 'PDF æŠ¥å‘Š',
                'excel': 'Excel è¡¨æ ¼'
            }.get(x, x)
        )
        
        include_raw_data = st.checkbox(
            "åŒ…å«åŸå§‹æ•°æ®",
            value=False,
            help="åŒ…å«åŸå§‹æ•°æ®ä¼šå¢åŠ æ–‡ä»¶å¤§å°"
        )
    
    with col2:
        st.write("**å¯¼å‡ºå†…å®¹é¢„è§ˆ**:")
        st.write("â€¢ æ‰§è¡Œæ‘˜è¦")
        st.write("â€¢ åˆ†æç»“æœå’Œæ´å¯Ÿ")
        st.write("â€¢ è¡ŒåŠ¨å»ºè®®")
        st.write("â€¢ ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡")
        if include_raw_data:
            st.write("â€¢ åŸå§‹æ•°æ®")
    
    if st.button("ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š", type="primary"):
        try:
            integration_manager = st.session_state.integration_manager
            workflow_id = result['workflow_id']
            
            # å¯¼å‡ºæŠ¥å‘Š
            file_path = integration_manager.export_workflow_results(
                workflow_id=workflow_id,
                export_format=export_format,
                include_raw_data=include_raw_data
            )
            
            # è¯»å–æ–‡ä»¶å†…å®¹ç”¨äºä¸‹è½½
            with open(file_path, 'rb') as f:
                file_data = f.read()
            
            # æä¾›ä¸‹è½½é“¾æ¥
            st.download_button(
                label=f"â¬‡ï¸ ä¸‹è½½ {export_format.upper()} æŠ¥å‘Š",
                data=file_data,
                file_name=Path(file_path).name,
                mime=get_mime_type(export_format)
            )
            
            st.success(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {Path(file_path).name}")
            
        except Exception as e:
            st.error(f"âŒ å¯¼å‡ºå¤±è´¥: {str(e)}")

if __name__ == "__main__":
    main()