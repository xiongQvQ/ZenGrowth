# APIæ–‡æ¡£å’Œå¼€å‘è€…æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›ç”¨æˆ·è¡Œä¸ºåˆ†ææ™ºèƒ½ä½“å¹³å°çš„å®Œæ•´APIæ–‡æ¡£å’Œå¼€å‘è€…æŒ‡å—ã€‚å¹³å°é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œä¸»è¦åŒ…å«æ•°æ®å¤„ç†ã€åˆ†æå¼•æ“ã€æ™ºèƒ½ä½“ç³»ç»Ÿå’Œå¯è§†åŒ–ç»„ä»¶ç­‰æ ¸å¿ƒæ¨¡å—ã€‚

## ğŸ—ï¸ æ¶æ„æ¦‚è§ˆ

### æ ¸å¿ƒæ¨¡å—ç»“æ„

```
user-behavior-analytics-platform/
â”œâ”€â”€ agents/          # CrewAIæ™ºèƒ½ä½“æ¨¡å—
â”œâ”€â”€ engines/         # åˆ†æå¼•æ“æ¨¡å—  
â”œâ”€â”€ tools/           # æ•°æ®å¤„ç†å·¥å…·
â”œâ”€â”€ config/          # é…ç½®ç®¡ç†
â”œâ”€â”€ utils/           # å·¥å…·å‡½æ•°
â”œâ”€â”€ visualization/   # å¯è§†åŒ–ç»„ä»¶
â””â”€â”€ system/          # ç³»ç»Ÿé›†æˆ
```

### æ•°æ®æµæ¶æ„

```mermaid
graph TB
    A[GA4 NDJSONæ•°æ®] --> B[GA4DataParser]
    B --> C[DataStorageManager]
    C --> D[åˆ†æå¼•æ“å±‚]
    D --> E[CrewAIæ™ºèƒ½ä½“]
    E --> F[å¯è§†åŒ–ç»„ä»¶]
    F --> G[Streamlitç•Œé¢]
```

## ğŸ”§ æ ¸å¿ƒAPI

### 1. æ•°æ®å¤„ç†API

#### GA4DataParser

GA4æ•°æ®è§£æå™¨ï¼Œè´Ÿè´£NDJSONæ ¼å¼æ•°æ®çš„è§£æå’Œé¢„å¤„ç†ã€‚

```python
from tools.ga4_data_parser import GA4DataParser

# åˆå§‹åŒ–è§£æå™¨
parser = GA4DataParser()

# è§£æNDJSONæ–‡ä»¶
data = parser.parse_ndjson(file_path: str) -> pd.DataFrame

# æå–äº‹ä»¶æ•°æ®
events = parser.extract_events(data: pd.DataFrame) -> pd.DataFrame

# æå–ç”¨æˆ·å±æ€§
users = parser.extract_user_properties(data: pd.DataFrame) -> pd.DataFrame

# æå–ä¼šè¯æ•°æ®
sessions = parser.extract_sessions(data: pd.DataFrame) -> pd.DataFrame

# æ•°æ®è´¨é‡éªŒè¯
quality_report = parser.validate_data_quality(data: pd.DataFrame) -> Dict[str, Any]
```

**æ–¹æ³•è¯¦è§£**:

- `parse_ndjson(file_path: str) -> pd.DataFrame`
  - **å‚æ•°**: `file_path` - NDJSONæ–‡ä»¶è·¯å¾„
  - **è¿”å›**: è§£æåçš„DataFrame
  - **å¼‚å¸¸**: `FileNotFoundError`, `JSONDecodeError`

- `extract_events(data: pd.DataFrame) -> pd.DataFrame`
  - **å‚æ•°**: `data` - åŸå§‹GA4æ•°æ®
  - **è¿”å›**: äº‹ä»¶æ•°æ®DataFrameï¼ŒåŒ…å«å­—æ®µï¼š
    - `event_date`: äº‹ä»¶æ—¥æœŸ
    - `event_timestamp`: äº‹ä»¶æ—¶é—´æˆ³
    - `event_name`: äº‹ä»¶åç§°
    - `user_pseudo_id`: ç”¨æˆ·ä¼ªID
    - `user_id`: ç”¨æˆ·çœŸå®ID (å¯é€‰)
    - `platform`: å¹³å°ä¿¡æ¯
    - `device`: è®¾å¤‡ä¿¡æ¯
    - `geo`: åœ°ç†ä½ç½®ä¿¡æ¯

#### DataStorageManager

æ•°æ®å­˜å‚¨ç®¡ç†å™¨ï¼Œæä¾›å†…å­˜æ•°æ®å­˜å‚¨å’Œæ£€ç´¢åŠŸèƒ½ã€‚

```python
from tools.data_storage_manager import DataStorageManager

# åˆå§‹åŒ–å­˜å‚¨ç®¡ç†å™¨
storage = DataStorageManager()

# å­˜å‚¨æ•°æ®
storage.store_events(events: pd.DataFrame) -> None
storage.store_users(users: pd.DataFrame) -> None
storage.store_sessions(sessions: pd.DataFrame) -> None

# æ£€ç´¢æ•°æ®
events = storage.get_events(filters: Dict[str, Any] = None) -> pd.DataFrame
users = storage.get_users(filters: Dict[str, Any] = None) -> pd.DataFrame
sessions = storage.get_sessions(filters: Dict[str, Any] = None) -> pd.DataFrame

# æ•°æ®ç»Ÿè®¡
stats = storage.get_data_summary() -> Dict[str, Any]
```

**è¿‡æ»¤å™¨å‚æ•°**:
```python
filters = {
    'date_range': ('2024-01-01', '2024-01-31'),
    'event_names': ['page_view', 'purchase'],
    'user_ids': ['user123', 'user456'],
    'platforms': ['web', 'mobile']
}
```

### 2. åˆ†æå¼•æ“API

#### EventAnalysisEngine

äº‹ä»¶åˆ†æå¼•æ“ï¼Œæä¾›äº‹ä»¶é¢‘æ¬¡ã€è¶‹åŠ¿å’Œå…³è”æ€§åˆ†æã€‚

```python
from engines.event_analysis_engine import EventAnalysisEngine

# åˆå§‹åŒ–å¼•æ“
engine = EventAnalysisEngine(storage_manager)

# äº‹ä»¶é¢‘æ¬¡åˆ†æ
frequency_results = engine.analyze_event_frequency(
    event_types: Optional[List[str]] = None,
    date_range: Optional[Tuple[str, str]] = None,
    granularity: str = 'day'
) -> Dict[str, EventFrequencyResult]

# äº‹ä»¶è¶‹åŠ¿åˆ†æ
trend_results = engine.analyze_event_trends(
    event_types: Optional[List[str]] = None,
    date_range: Optional[Tuple[str, str]] = None,
    window_size: int = 7
) -> Dict[str, EventTrendResult]

# äº‹ä»¶å…³è”æ€§åˆ†æ
correlation_results = engine.analyze_event_correlations(
    event_pairs: Optional[List[Tuple[str, str]]] = None,
    method: str = 'chi2'
) -> Dict[str, EventCorrelationResult]
```

**æ•°æ®æ¨¡å‹**:

```python
@dataclass
class EventFrequencyResult:
    event_name: str
    total_count: int
    unique_users: int
    avg_per_user: float
    frequency_distribution: Dict[str, int]
    percentiles: Dict[str, float]

@dataclass
class EventTrendResult:
    event_name: str
    trend_data: pd.DataFrame
    trend_direction: str  # 'increasing', 'decreasing', 'stable'
    growth_rate: float
    seasonal_pattern: Optional[Dict[str, float]]
    anomalies: List[Dict[str, Any]]
```

#### RetentionAnalysisEngine

ç•™å­˜åˆ†æå¼•æ“ï¼Œæä¾›ç”¨æˆ·ç•™å­˜ç‡è®¡ç®—å’Œé˜Ÿåˆ—åˆ†æã€‚

```python
from engines.retention_analysis_engine import RetentionAnalysisEngine

# åˆå§‹åŒ–å¼•æ“
engine = RetentionAnalysisEngine(storage_manager)

# æ„å»ºç”¨æˆ·é˜Ÿåˆ—
cohorts = engine.build_user_cohorts(
    cohort_type: str = 'weekly',  # 'daily', 'weekly', 'monthly'
    date_range: Optional[Tuple[str, str]] = None
) -> pd.DataFrame

# è®¡ç®—ç•™å­˜ç‡
retention_rates = engine.calculate_retention_rates(
    cohorts: pd.DataFrame,
    periods: List[int] = [1, 7, 14, 30]
) -> pd.DataFrame

# ç”Ÿæˆç•™å­˜çƒ­åŠ›å›¾æ•°æ®
heatmap_data = engine.generate_retention_heatmap(
    retention_data: pd.DataFrame
) -> Dict[str, Any]
```

#### ConversionAnalysisEngine

è½¬åŒ–åˆ†æå¼•æ“ï¼Œæä¾›è½¬åŒ–æ¼æ–—æ„å»ºå’Œè½¬åŒ–ç‡è®¡ç®—ã€‚

```python
from engines.conversion_analysis_engine import ConversionAnalysisEngine

# åˆå§‹åŒ–å¼•æ“
engine = ConversionAnalysisEngine(storage_manager)

# æ„å»ºè½¬åŒ–æ¼æ–—
funnel = engine.build_conversion_funnel(
    funnel_steps: List[str],
    conversion_window_hours: int = 24,
    date_range: Optional[Tuple[str, str]] = None
) -> pd.DataFrame

# è®¡ç®—è½¬åŒ–ç‡
conversion_rates = engine.calculate_conversion_rates(
    funnel_data: pd.DataFrame
) -> Dict[str, float]

# è¯†åˆ«è½¬åŒ–ç“¶é¢ˆ
bottlenecks = engine.identify_conversion_bottlenecks(
    funnel_data: pd.DataFrame,
    threshold: float = 0.5
) -> List[Dict[str, Any]]
```

### 3. CrewAIæ™ºèƒ½ä½“API

#### æ™ºèƒ½ä½“åˆ›å»ºå’Œé…ç½®

```python
from config.crew_config import create_agent, create_task, CrewManager

# åˆ›å»ºå•ä¸ªæ™ºèƒ½ä½“
agent = create_agent(
    agent_type: str,  # 'data_processor', 'event_analyst', etc.
    tools: List[BaseTool] = None
)

# åˆ›å»ºä»»åŠ¡
task = create_task(
    description: str,
    agent: Agent,
    expected_output: str = None
)

# ä½¿ç”¨å›¢é˜Ÿç®¡ç†å™¨
crew_manager = CrewManager()
crew_manager.add_agent('event_analyst', tools=[event_analysis_tool])
crew_manager.add_task('åˆ†æç”¨æˆ·äº‹ä»¶æ¨¡å¼', 'event_analyst')
results = crew_manager.execute()
```

#### æ™ºèƒ½ä½“å·¥å…·å¼€å‘

åˆ›å»ºè‡ªå®šä¹‰æ™ºèƒ½ä½“å·¥å…·ï¼š

```python
from crewai.tools import BaseTool
from pydantic import BaseModel, Field

class CustomAnalysisTool(BaseTool):
    name: str = "custom_analysis"
    description: str = "è‡ªå®šä¹‰åˆ†æå·¥å…·æè¿°"
    
    def __init__(self, **kwargs):
        super().__init__()
        # åˆå§‹åŒ–å·¥å…·å‚æ•°
    
    def _run(self, **kwargs) -> Dict[str, Any]:
        """å·¥å…·æ‰§è¡Œé€»è¾‘"""
        # å®ç°åˆ†æé€»è¾‘
        return analysis_results
```

### 4. å¯è§†åŒ–API

#### ChartGenerator

åŸºç¡€å›¾è¡¨ç”Ÿæˆå™¨ï¼ŒåŸºäºPlotlyåˆ›å»ºäº¤äº’å¼å›¾è¡¨ã€‚

```python
from visualization.chart_generator import ChartGenerator

# åˆå§‹åŒ–å›¾è¡¨ç”Ÿæˆå™¨
chart_gen = ChartGenerator()

# åˆ›å»ºäº‹ä»¶æ—¶é—´çº¿å›¾
timeline_fig = chart_gen.create_event_timeline(
    data: pd.DataFrame,
    event_column: str = 'event_name',
    time_column: str = 'event_timestamp',
    title: str = 'äº‹ä»¶æ—¶é—´çº¿'
) -> plotly.graph_objects.Figure

# åˆ›å»ºç•™å­˜çƒ­åŠ›å›¾
heatmap_fig = chart_gen.create_retention_heatmap(
    data: pd.DataFrame,
    title: str = 'ç”¨æˆ·ç•™å­˜çƒ­åŠ›å›¾'
) -> plotly.graph_objects.Figure

# åˆ›å»ºè½¬åŒ–æ¼æ–—å›¾
funnel_fig = chart_gen.create_conversion_funnel(
    data: pd.DataFrame,
    steps_column: str = 'step',
    values_column: str = 'users',
    title: str = 'è½¬åŒ–æ¼æ–—'
) -> plotly.graph_objects.Figure
```

#### AdvancedVisualizer

é«˜çº§å¯è§†åŒ–ç»„ä»¶ï¼Œæä¾›å¤æ‚çš„äº¤äº’å¼å›¾è¡¨ã€‚

```python
from visualization.advanced_visualizer import AdvancedVisualizer

# åˆå§‹åŒ–é«˜çº§å¯è§†åŒ–å™¨
viz = AdvancedVisualizer()

# åˆ›å»ºç”¨æˆ·è¡Œä¸ºæµç¨‹å›¾
flow_fig = viz.create_user_flow_diagram(
    path_data: pd.DataFrame,
    min_flow_count: int = 10
) -> plotly.graph_objects.Figure

# åˆ›å»ºç”¨æˆ·åˆ†ç¾¤æ•£ç‚¹å›¾
scatter_fig = viz.create_user_segmentation_plot(
    user_data: pd.DataFrame,
    features: List[str],
    cluster_column: str = 'cluster'
) -> plotly.graph_objects.Figure
```

### 5. é…ç½®ç®¡ç†API

#### Settings

ç³»ç»Ÿé…ç½®ç®¡ç†ï¼ŒåŸºäºPydanticçš„é…ç½®ç±»ã€‚

```python
from config.settings import settings, get_google_api_key, validate_config

# è·å–é…ç½®å€¼
api_key = settings.google_api_key
model_name = settings.llm_model
temperature = settings.llm_temperature

# éªŒè¯é…ç½®
is_valid = validate_config()

# è·å–APIå¯†é’¥ï¼ˆå¸¦éªŒè¯ï¼‰
api_key = get_google_api_key()
```

#### ConfigManager

åŠ¨æ€é…ç½®ç®¡ç†å™¨ï¼Œæ”¯æŒè¿è¡Œæ—¶é…ç½®æ›´æ–°ã€‚

```python
from utils.config_manager import config_manager

# è·å–ç³»ç»Ÿé…ç½®
system_config = config_manager.get_system_config()
analysis_config = config_manager.get_analysis_config()

# æ›´æ–°é…ç½®
config_manager.update_system_config('api_settings', {
    'llm_temperature': 0.2,
    'llm_max_tokens': 5000
})

# ä¿å­˜é…ç½®
config_manager.save_config()
```

## ğŸ”Œ æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„åˆ†æå¼•æ“

1. **åˆ›å»ºå¼•æ“ç±»**

```python
from typing import Dict, Any
import pandas as pd

class CustomAnalysisEngine:
    """è‡ªå®šä¹‰åˆ†æå¼•æ“"""
    
    def __init__(self, storage_manager):
        self.storage_manager = storage_manager
    
    def analyze(self, **kwargs) -> Dict[str, Any]:
        """æ‰§è¡Œè‡ªå®šä¹‰åˆ†æ"""
        # è·å–æ•°æ®
        data = self.storage_manager.get_events()
        
        # æ‰§è¡Œåˆ†æé€»è¾‘
        results = self._perform_analysis(data)
        
        return results
    
    def _perform_analysis(self, data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå®ç°"""
        # å®ç°å…·ä½“åˆ†æé€»è¾‘
        pass
```

2. **åˆ›å»ºå¯¹åº”çš„æ™ºèƒ½ä½“å·¥å…·**

```python
from crewai.tools import BaseTool

class CustomAnalysisTool(BaseTool):
    name: str = "custom_analysis"
    description: str = "æ‰§è¡Œè‡ªå®šä¹‰åˆ†æ"
    
    def __init__(self, storage_manager):
        super().__init__()
        self.engine = CustomAnalysisEngine(storage_manager)
    
    def _run(self, **kwargs) -> Dict[str, Any]:
        return self.engine.analyze(**kwargs)
```

3. **é›†æˆåˆ°æ™ºèƒ½ä½“ç³»ç»Ÿ**

```python
# åœ¨crew_config.pyä¸­æ·»åŠ æ–°çš„æ™ºèƒ½ä½“è§’è‰²
AGENT_ROLES["custom_analyst"] = {
    "role": "è‡ªå®šä¹‰åˆ†æä¸“å®¶",
    "goal": "æ‰§è¡Œç‰¹å®šçš„è‡ªå®šä¹‰åˆ†æä»»åŠ¡",
    "backstory": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è‡ªå®šä¹‰åˆ†æå¸ˆ..."
}
```

### æ·»åŠ æ–°çš„å¯è§†åŒ–ç»„ä»¶

```python
import plotly.graph_objects as go
from typing import Dict, Any

class CustomVisualization:
    """è‡ªå®šä¹‰å¯è§†åŒ–ç»„ä»¶"""
    
    def create_custom_chart(self, data: pd.DataFrame, **kwargs) -> go.Figure:
        """åˆ›å»ºè‡ªå®šä¹‰å›¾è¡¨"""
        fig = go.Figure()
        
        # æ·»åŠ å›¾è¡¨å…ƒç´ 
        fig.add_trace(go.Scatter(
            x=data['x_column'],
            y=data['y_column'],
            mode='markers+lines'
        ))
        
        # è®¾ç½®å¸ƒå±€
        fig.update_layout(
            title=kwargs.get('title', 'è‡ªå®šä¹‰å›¾è¡¨'),
            xaxis_title=kwargs.get('x_title', 'Xè½´'),
            yaxis_title=kwargs.get('y_title', 'Yè½´')
        )
        
        return fig
```

## ğŸ§ª æµ‹è¯•æ¡†æ¶

### å•å…ƒæµ‹è¯•

```python
import unittest
from unittest.mock import Mock, patch
from engines.event_analysis_engine import EventAnalysisEngine

class TestEventAnalysisEngine(unittest.TestCase):
    
    def setUp(self):
        self.mock_storage = Mock()
        self.engine = EventAnalysisEngine(self.mock_storage)
    
    def test_analyze_event_frequency(self):
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = pd.DataFrame({
            'event_name': ['page_view', 'click', 'page_view'],
            'user_pseudo_id': ['user1', 'user1', 'user2'],
            'event_timestamp': [1640995200, 1640995260, 1640995320]
        })
        
        self.mock_storage.get_events.return_value = test_data
        
        # æ‰§è¡Œæµ‹è¯•
        results = self.engine.analyze_event_frequency()
        
        # éªŒè¯ç»“æœ
        self.assertIn('page_view', results)
        self.assertEqual(results['page_view'].total_count, 2)
```

### é›†æˆæµ‹è¯•

```python
import pytest
from system.integration_manager import IntegrationManager

@pytest.fixture
def integration_manager():
    return IntegrationManager()

def test_full_analysis_workflow(integration_manager):
    """æµ‹è¯•å®Œæ•´åˆ†æå·¥ä½œæµ"""
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_file = 'test_data/sample_ga4.ndjson'
    
    # æ‰§è¡Œå®Œæ•´å·¥ä½œæµ
    results = integration_manager.run_full_analysis(test_file)
    
    # éªŒè¯ç»“æœ
    assert 'event_analysis' in results
    assert 'retention_analysis' in results
    assert 'conversion_analysis' in results
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ç®¡ç†

```python
# ä½¿ç”¨æ•°æ®åˆ†å—å¤„ç†å¤§æ–‡ä»¶
def process_large_file(file_path: str, chunk_size: int = 10000):
    for chunk in pd.read_json(file_path, lines=True, chunksize=chunk_size):
        # å¤„ç†æ•°æ®å—
        processed_chunk = process_data_chunk(chunk)
        yield processed_chunk

# åŠæ—¶é‡Šæ”¾å†…å­˜
def cleanup_memory():
    import gc
    gc.collect()
```

### ç¼“å­˜æœºåˆ¶

```python
from functools import lru_cache
import pickle
from pathlib import Path

class AnalysisCache:
    """åˆ†æç»“æœç¼“å­˜"""
    
    def __init__(self, cache_dir: str = 'cache'):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    def get(self, key: str):
        cache_file = self.cache_dir / f"{key}.pkl"
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def set(self, key: str, value):
        cache_file = self.cache_dir / f"{key}.pkl"
        with open(cache_file, 'wb') as f:
            pickle.dump(value, f)
```

## ğŸ”’ å®‰å…¨è€ƒè™‘

### APIå¯†é’¥ç®¡ç†

```python
import os
from cryptography.fernet import Fernet

class SecureConfig:
    """å®‰å…¨é…ç½®ç®¡ç†"""
    
    def __init__(self):
        self.cipher_suite = Fernet(self._get_or_create_key())
    
    def _get_or_create_key(self):
        key_file = '.secret_key'
        if os.path.exists(key_file):
            with open(key_file, 'rb') as f:
                return f.read()
        else:
            key = Fernet.generate_key()
            with open(key_file, 'wb') as f:
                f.write(key)
            return key
    
    def encrypt_api_key(self, api_key: str) -> str:
        return self.cipher_suite.encrypt(api_key.encode()).decode()
    
    def decrypt_api_key(self, encrypted_key: str) -> str:
        return self.cipher_suite.decrypt(encrypted_key.encode()).decode()
```

### æ•°æ®éªŒè¯

```python
from pydantic import BaseModel, validator
from typing import List, Optional

class EventDataModel(BaseModel):
    """äº‹ä»¶æ•°æ®éªŒè¯æ¨¡å‹"""
    event_name: str
    user_pseudo_id: str
    event_timestamp: int
    event_date: str
    
    @validator('event_timestamp')
    def validate_timestamp(cls, v):
        if v <= 0:
            raise ValueError('æ—¶é—´æˆ³å¿…é¡»ä¸ºæ­£æ•°')
        return v
    
    @validator('event_name')
    def validate_event_name(cls, v):
        if not v or len(v.strip()) == 0:
            raise ValueError('äº‹ä»¶åç§°ä¸èƒ½ä¸ºç©º')
        return v.strip()
```

## ğŸ“š æœ€ä½³å®è·µ

### ä»£ç è§„èŒƒ

1. **éµå¾ªPEP 8ä»£ç é£æ ¼**
2. **ä½¿ç”¨ç±»å‹æ³¨è§£**
3. **ç¼–å†™è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²**
4. **å®ç°é€‚å½“çš„é”™è¯¯å¤„ç†**
5. **ä½¿ç”¨æ—¥å¿—è®°å½•å…³é”®æ“ä½œ**

### æ€§èƒ½ä¼˜åŒ–

1. **ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯**
2. **åˆç†ä½¿ç”¨ç¼“å­˜æœºåˆ¶**
3. **åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å†…å­˜**
4. **ä½¿ç”¨å¼‚æ­¥å¤„ç†æé«˜å¹¶å‘æ€§èƒ½**

### é”™è¯¯å¤„ç†

```python
import logging
from typing import Optional

logger = logging.getLogger(__name__)

def safe_analysis_execution(func):
    """å®‰å…¨åˆ†ææ‰§è¡Œè£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"åˆ†ææ‰§è¡Œå¤±è´¥: {func.__name__}, é”™è¯¯: {str(e)}")
            return None
    return wrapper

@safe_analysis_execution
def perform_analysis():
    # åˆ†æé€»è¾‘
    pass
```

---

*æœ¬APIæ–‡æ¡£æŒç»­æ›´æ–°ä¸­ï¼Œå¦‚æœ‰ç–‘é—®è¯·å‚è€ƒæºä»£ç æˆ–è”ç³»å¼€å‘å›¢é˜Ÿã€‚*