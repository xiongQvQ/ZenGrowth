# ç¤ºä¾‹å’Œæœ€ä½³å®è·µæ–‡æ¡£

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›ç”¨æˆ·è¡Œä¸ºåˆ†ææ™ºèƒ½ä½“å¹³å°çš„å®é™…ä½¿ç”¨ç¤ºä¾‹ã€æœ€ä½³å®è·µå’Œé«˜çº§ç”¨æ³•æŒ‡å—ã€‚é€šè¿‡å…·ä½“çš„ä»£ç ç¤ºä¾‹å’Œä¸šåŠ¡åœºæ™¯ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£å’Œä½¿ç”¨å¹³å°åŠŸèƒ½ã€‚

## ğŸ¯ ä½¿ç”¨åœºæ™¯ç¤ºä¾‹

### 1. å¤šæ¨¡æ€ç”¨æˆ·è¡Œä¸ºåˆ†æ

#### ä¸šåŠ¡èƒŒæ™¯
æŸç”µå•†å¹³å°å¸Œæœ›ç»“åˆç”¨æˆ·ä¸Šä¼ çš„å•†å“å›¾ç‰‡å’Œè¡Œä¸ºæ•°æ®ï¼Œåˆ†æç”¨æˆ·å¯¹ä¸åŒå•†å“ç±»å‹çš„åå¥½ï¼Œä¼˜åŒ–å•†å“æ¨èç®—æ³•ã€‚

#### å¤šæ¨¡æ€æ•°æ®å‡†å¤‡
```python
# å¤šæ¨¡æ€åˆ†ææ•°æ®ç»“æ„
multimodal_data = {
    "text_content": "ç”¨æˆ·æµè§ˆäº†è¿åŠ¨é‹å•†å“é¡µé¢ï¼Œåœç•™æ—¶é—´3åˆ†é’Ÿï¼ŒæŸ¥çœ‹äº†5å¼ å•†å“å›¾ç‰‡",
    "image_content": [
        {
            "type": "image_url",
            "image_url": {
                "url": "https://example.com/product_images/sneaker_1.jpg",
                "detail": "high"
            }
        },
        {
            "type": "image_url", 
            "image_url": {
                "url": "https://example.com/user_uploads/style_preference.jpg",
                "detail": "auto"
            }
        }
    ],
    "user_behavior": {
        "page_views": 15,
        "time_on_page": 180,
        "scroll_depth": 0.85,
        "click_events": ["zoom_image", "color_selection", "size_guide"]
    }
}
```

#### å¤šæ¨¡æ€åˆ†æå®ç°
```python
from config.multimodal_content_handler import MultiModalContentHandler
from config.llm_provider_manager import LLMProviderManager

class MultiModalAnalyzer:
    """å¤šæ¨¡æ€åˆ†æå™¨"""
    
    def __init__(self):
        self.content_handler = MultiModalContentHandler()
        self.provider_manager = LLMProviderManager()
    
    def analyze_user_preferences(self, multimodal_data, provider="volcano"):
        """åˆ†æç”¨æˆ·åå¥½ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰"""
        
        # 1. å‡†å¤‡å¤šæ¨¡æ€å†…å®¹
        content = self._prepare_multimodal_content(multimodal_data)
        
        # 2. é€‰æ‹©æ”¯æŒå¤šæ¨¡æ€çš„æä¾›å•†
        llm = self.provider_manager.get_llm(provider=provider)
        
        if not llm.supports_multimodal():
            # é™çº§åˆ°æ–‡æœ¬åˆ†æ
            return self._fallback_to_text_analysis(multimodal_data)
        
        # 3. æ„å»ºåˆ†ææç¤º
        analysis_prompt = self._build_multimodal_prompt(content)
        
        # 4. æ‰§è¡Œå¤šæ¨¡æ€åˆ†æ
        try:
            result = llm.invoke(analysis_prompt)
            return self._parse_analysis_result(result)
        except Exception as e:
            print(f"å¤šæ¨¡æ€åˆ†æå¤±è´¥ï¼Œé™çº§åˆ°æ–‡æœ¬åˆ†æ: {e}")
            return self._fallback_to_text_analysis(multimodal_data)
    
    def _prepare_multimodal_content(self, data):
        """å‡†å¤‡å¤šæ¨¡æ€å†…å®¹"""
        content = []
        
        # æ·»åŠ æ–‡æœ¬å†…å®¹
        if "text_content" in data:
            content.append({
                "type": "text",
                "text": data["text_content"]
            })
        
        # æ·»åŠ å›¾ç‰‡å†…å®¹
        if "image_content" in data:
            for image in data["image_content"]:
                # éªŒè¯å›¾ç‰‡URL
                if self.content_handler.validate_image_url(image["image_url"]["url"]):
                    content.append(image)
                else:
                    print(f"è·³è¿‡æ— æ•ˆå›¾ç‰‡: {image['image_url']['url']}")
        
        # æ·»åŠ è¡Œä¸ºæ•°æ®ä½œä¸ºæ–‡æœ¬
        if "user_behavior" in data:
            behavior_text = self._format_behavior_data(data["user_behavior"])
            content.append({
                "type": "text", 
                "text": f"ç”¨æˆ·è¡Œä¸ºæ•°æ®: {behavior_text}"
            })
        
        return content
    
    def _build_multimodal_prompt(self, content):
        """æ„å»ºå¤šæ¨¡æ€åˆ†ææç¤º"""
        return [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”¨æˆ·è¡Œä¸ºåˆ†æå¸ˆã€‚è¯·åˆ†ææä¾›çš„å¤šæ¨¡æ€æ•°æ®ï¼ˆåŒ…æ‹¬æ–‡æœ¬æè¿°ã€å›¾ç‰‡å’Œè¡Œä¸ºæ•°æ®ï¼‰ï¼Œ
                è¯†åˆ«ç”¨æˆ·åå¥½æ¨¡å¼ï¼Œå¹¶æä¾›ä»¥ä¸‹åˆ†æï¼š
                1. ç”¨æˆ·å…´è¶£ç±»åˆ«è¯†åˆ«
                2. è§†è§‰åå¥½åˆ†æï¼ˆåŸºäºå›¾ç‰‡å†…å®¹ï¼‰
                3. è¡Œä¸ºæ¨¡å¼æ€»ç»“
                4. ä¸ªæ€§åŒ–æ¨èå»ºè®®
                
                è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æ„åŒ–çš„åˆ†æç»“æœã€‚"""
            },
            {
                "role": "user", 
                "content": content
            }
        ]
    
    def _parse_analysis_result(self, result):
        """è§£æåˆ†æç»“æœ"""
        try:
            import json
            # å°è¯•è§£æJSONç»“æœ
            if isinstance(result, str):
                # æå–JSONéƒ¨åˆ†
                json_start = result.find('{')
                json_end = result.rfind('}') + 1
                if json_start != -1 and json_end != -1:
                    json_str = result[json_start:json_end]
                    return json.loads(json_str)
            
            return {"raw_result": result}
        except Exception as e:
            return {
                "error": f"ç»“æœè§£æå¤±è´¥: {e}",
                "raw_result": result
            }

# ä½¿ç”¨ç¤ºä¾‹
analyzer = MultiModalAnalyzer()

# æ‰§è¡Œå¤šæ¨¡æ€åˆ†æ
preferences = analyzer.analyze_user_preferences(
    multimodal_data, 
    provider="volcano"  # ä½¿ç”¨æ”¯æŒå¤šæ¨¡æ€çš„Volcanoæä¾›å•†
)

print("ç”¨æˆ·åå¥½åˆ†æç»“æœ:")
print(json.dumps(preferences, indent=2, ensure_ascii=False))
```

#### å¤šæ¨¡æ€å¯è§†åŒ–
```python
import plotly.graph_objects as go
from plotly.subplots import make_subplots

def create_multimodal_dashboard(analysis_results, images):
    """åˆ›å»ºå¤šæ¨¡æ€åˆ†æä»ªè¡¨æ¿"""
    
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('ç”¨æˆ·å…´è¶£åˆ†å¸ƒ', 'è§†è§‰åå¥½çƒ­åŠ›å›¾', 'è¡Œä¸ºæ—¶é—´çº¿', 'æ¨èåŒ¹é…åº¦'),
        specs=[[{"type": "pie"}, {"type": "heatmap"}],
               [{"type": "scatter"}, {"type": "bar"}]]
    )
    
    # 1. å…´è¶£åˆ†å¸ƒé¥¼å›¾
    if "interest_categories" in analysis_results:
        categories = analysis_results["interest_categories"]
        fig.add_trace(
            go.Pie(
                labels=list(categories.keys()),
                values=list(categories.values()),
                name="å…´è¶£åˆ†å¸ƒ"
            ),
            row=1, col=1
        )
    
    # 2. è§†è§‰åå¥½çƒ­åŠ›å›¾
    if "visual_preferences" in analysis_results:
        visual_data = analysis_results["visual_preferences"]
        fig.add_trace(
            go.Heatmap(
                z=visual_data.get("preference_matrix", []),
                x=visual_data.get("attributes", []),
                y=visual_data.get("categories", []),
                colorscale="Viridis"
            ),
            row=1, col=2
        )
    
    # 3. è¡Œä¸ºæ—¶é—´çº¿
    if "behavior_timeline" in analysis_results:
        timeline = analysis_results["behavior_timeline"]
        fig.add_trace(
            go.Scatter(
                x=timeline.get("timestamps", []),
                y=timeline.get("engagement_scores", []),
                mode='lines+markers',
                name="å‚ä¸åº¦å˜åŒ–"
            ),
            row=2, col=1
        )
    
    # 4. æ¨èåŒ¹é…åº¦
    if "recommendations" in analysis_results:
        recommendations = analysis_results["recommendations"]
        fig.add_trace(
            go.Bar(
                x=[rec["name"] for rec in recommendations],
                y=[rec["match_score"] for rec in recommendations],
                name="æ¨èåŒ¹é…åº¦"
            ),
            row=2, col=2
        )
    
    # æ›´æ–°å¸ƒå±€
    fig.update_layout(
        title="å¤šæ¨¡æ€ç”¨æˆ·è¡Œä¸ºåˆ†æä»ªè¡¨æ¿",
        height=800,
        showlegend=True
    )
    
    return fig

# åˆ›å»ºä»ªè¡¨æ¿
dashboard = create_multimodal_dashboard(preferences, multimodal_data["image_content"])
dashboard.show()
```

### 2. ç”µå•†ç½‘ç«™ç”¨æˆ·è¡Œä¸ºåˆ†æ

#### ä¸šåŠ¡èƒŒæ™¯
æŸç”µå•†ç½‘ç«™å¸Œæœ›åˆ†æç”¨æˆ·ä»æµè§ˆå•†å“åˆ°å®Œæˆè´­ä¹°çš„å®Œæ•´è¡Œä¸ºè·¯å¾„ï¼Œè¯†åˆ«è½¬åŒ–ç“¶é¢ˆå¹¶ä¼˜åŒ–ç”¨æˆ·ä½“éªŒã€‚

#### æ•°æ®å‡†å¤‡
```python
# ç¤ºä¾‹GA4äº‹ä»¶æ•°æ®ç»“æ„
sample_events = [
    {
        "event_date": "20240115",
        "event_timestamp": 1705123200000000,
        "event_name": "page_view",
        "user_pseudo_id": "user_12345",
        "event_params": [
            {"key": "page_title", "value": {"string_value": "å•†å“è¯¦æƒ…é¡µ"}},
            {"key": "page_location", "value": {"string_value": "/product/123"}}
        ]
    },
    {
        "event_date": "20240115", 
        "event_timestamp": 1705123260000000,
        "event_name": "add_to_cart",
        "user_pseudo_id": "user_12345",
        "event_params": [
            {"key": "item_id", "value": {"string_value": "product_123"}},
            {"key": "value", "value": {"double_value": 99.99}}
        ]
    },
    {
        "event_date": "20240115",
        "event_timestamp": 1705123320000000, 
        "event_name": "purchase",
        "user_pseudo_id": "user_12345",
        "event_params": [
            {"key": "transaction_id", "value": {"string_value": "txn_789"}},
            {"key": "value", "value": {"double_value": 99.99}}
        ]
    }
]
```

#### åˆ†ææµç¨‹
```python
from tools.ga4_data_parser import GA4DataParser
from engines.conversion_analysis_engine import ConversionAnalysisEngine
from engines.path_analysis_engine import PathAnalysisEngine
from visualization.chart_generator import ChartGenerator

# 1. æ•°æ®è§£æå’Œé¢„å¤„ç†
parser = GA4DataParser()
data = parser.parse_ndjson('ecommerce_data.ndjson')
events = parser.extract_events(data)

# 2. å®šä¹‰è½¬åŒ–æ¼æ–—
funnel_steps = [
    'page_view',      # æµè§ˆå•†å“
    'add_to_cart',    # åŠ å…¥è´­ç‰©è½¦
    'begin_checkout', # å¼€å§‹ç»“è´¦
    'purchase'        # å®Œæˆè´­ä¹°
]

# 3. è½¬åŒ–åˆ†æ
conversion_engine = ConversionAnalysisEngine(storage_manager)
funnel_data = conversion_engine.build_conversion_funnel(
    funnel_steps=funnel_steps,
    conversion_window_hours=24
)

conversion_rates = conversion_engine.calculate_conversion_rates(funnel_data)
bottlenecks = conversion_engine.identify_conversion_bottlenecks(funnel_data)

# 4. ç”¨æˆ·è·¯å¾„åˆ†æ
path_engine = PathAnalysisEngine(storage_manager)
user_paths = path_engine.extract_user_paths(
    min_path_length=2,
    max_path_length=10
)

common_paths = path_engine.find_common_paths(
    user_paths,
    min_support=0.05
)

# 5. å¯è§†åŒ–ç»“æœ
chart_gen = ChartGenerator()
funnel_chart = chart_gen.create_conversion_funnel(
    funnel_data,
    title="ç”µå•†è½¬åŒ–æ¼æ–—åˆ†æ"
)

path_chart = chart_gen.create_user_flow_diagram(
    common_paths,
    title="ç”¨æˆ·è¡Œä¸ºè·¯å¾„å›¾"
)
```

#### ä¸šåŠ¡æ´å¯Ÿ
```python
def generate_ecommerce_insights(conversion_rates, bottlenecks, common_paths):
    """ç”Ÿæˆç”µå•†ä¸šåŠ¡æ´å¯Ÿ"""
    insights = []
    
    # è½¬åŒ–ç‡åˆ†æ
    overall_conversion = conversion_rates.get('overall_conversion', 0)
    if overall_conversion < 0.02:  # ä½äº2%
        insights.append({
            'type': 'conversion_optimization',
            'priority': 'high',
            'message': f'æ•´ä½“è½¬åŒ–ç‡ä»…ä¸º{overall_conversion:.2%}ï¼Œå»ºè®®ä¼˜åŒ–è´­ä¹°æµç¨‹',
            'recommendations': [
                'ç®€åŒ–ç»“è´¦æµç¨‹',
                'ä¼˜åŒ–æ”¯ä»˜é¡µé¢è®¾è®¡',
                'æä¾›å¤šç§æ”¯ä»˜æ–¹å¼',
                'å¢åŠ ä¿¡ä»»æ ‡è¯†'
            ]
        })
    
    # ç“¶é¢ˆè¯†åˆ«
    for bottleneck in bottlenecks:
        if bottleneck['drop_rate'] > 0.5:  # æµå¤±ç‡è¶…è¿‡50%
            insights.append({
                'type': 'bottleneck_alert',
                'priority': 'high',
                'message': f'{bottleneck["step"]}æ­¥éª¤æµå¤±ç‡é«˜è¾¾{bottleneck["drop_rate"]:.1%}',
                'recommendations': [
                    f'é‡ç‚¹ä¼˜åŒ–{bottleneck["step"]}é¡µé¢ä½“éªŒ',
                    'åˆ†æç”¨æˆ·åœ¨è¯¥æ­¥éª¤çš„å…·ä½“è¡Œä¸º',
                    'è¿›è¡ŒA/Bæµ‹è¯•éªŒè¯ä¼˜åŒ–æ•ˆæœ'
                ]
            })
    
    # è·¯å¾„ä¼˜åŒ–
    bounce_paths = [path for path in common_paths if len(path['events']) == 1]
    if len(bounce_paths) > len(common_paths) * 0.3:  # è·³å‡ºè·¯å¾„è¶…è¿‡30%
        insights.append({
            'type': 'engagement_optimization',
            'priority': 'medium',
            'message': 'ç”¨æˆ·è·³å‡ºç‡è¾ƒé«˜ï¼Œéœ€è¦æå‡é¡µé¢å¸å¼•åŠ›',
            'recommendations': [
                'ä¼˜åŒ–é¦–é¡µå†…å®¹å¸ƒå±€',
                'å¢åŠ ç›¸å…³å•†å“æ¨è',
                'æ”¹è¿›é¡µé¢åŠ è½½é€Ÿåº¦',
                'ä¼˜åŒ–ç§»åŠ¨ç«¯ä½“éªŒ'
            ]
        })
    
    return insights
```

### 2. SaaSäº§å“ç”¨æˆ·ç•™å­˜åˆ†æ

#### ä¸šåŠ¡èƒŒæ™¯
æŸSaaSäº§å“å¸Œæœ›åˆ†ææ–°ç”¨æˆ·çš„ç•™å­˜æƒ…å†µï¼Œè¯†åˆ«å½±å“ç”¨æˆ·ç•™å­˜çš„å…³é”®å› ç´ ã€‚

#### åˆ†æå®ç°
```python
from engines.retention_analysis_engine import RetentionAnalysisEngine
from engines.user_segmentation_engine import UserSegmentationEngine
from datetime import datetime, timedelta

# 1. æ„å»ºç”¨æˆ·é˜Ÿåˆ—
retention_engine = RetentionAnalysisEngine(storage_manager)

# æŒ‰å‘¨æ„å»ºæ–°ç”¨æˆ·é˜Ÿåˆ—
cohorts = retention_engine.build_user_cohorts(
    cohort_type='weekly',
    date_range=('2024-01-01', '2024-03-31')
)

# 2. è®¡ç®—ç•™å­˜ç‡
retention_periods = [1, 7, 14, 30, 60, 90]  # å¤©æ•°
retention_rates = retention_engine.calculate_retention_rates(
    cohorts, 
    periods=retention_periods
)

# 3. ç”¨æˆ·åˆ†ç¾¤åˆ†æ
segmentation_engine = UserSegmentationEngine(storage_manager)

# åŸºäºä½¿ç”¨è¡Œä¸ºè¿›è¡Œåˆ†ç¾¤
user_features = segmentation_engine.extract_user_features([
    'session_frequency',    # ä¼šè¯é¢‘ç‡
    'feature_usage_depth',  # åŠŸèƒ½ä½¿ç”¨æ·±åº¦
    'support_interactions', # å®¢æœäº¤äº’æ¬¡æ•°
    'onboarding_completion' # å…¥é—¨æµç¨‹å®Œæˆåº¦
])

clusters = segmentation_engine.perform_clustering(
    user_features,
    n_clusters=4,
    method='kmeans'
)

# 4. ç•™å­˜é©±åŠ¨å› ç´ åˆ†æ
def analyze_retention_drivers(retention_data, user_segments):
    """åˆ†æç•™å­˜é©±åŠ¨å› ç´ """
    drivers = {}
    
    for segment_id, segment_users in user_segments.items():
        segment_retention = retention_data[
            retention_data['user_id'].isin(segment_users)
        ]
        
        # è®¡ç®—å„æ—¶æœŸç•™å­˜ç‡
        segment_rates = {}
        for period in retention_periods:
            period_retention = segment_retention[
                segment_retention['period'] == period
            ]['retained'].mean()
            segment_rates[f'day_{period}'] = period_retention
        
        drivers[f'segment_{segment_id}'] = {
            'retention_rates': segment_rates,
            'user_count': len(segment_users),
            'characteristics': get_segment_characteristics(segment_id)
        }
    
    return drivers

def get_segment_characteristics(segment_id):
    """è·å–ç”¨æˆ·ç¾¤ä½“ç‰¹å¾"""
    characteristics = {
        0: {
            'name': 'é«˜æ´»è·ƒç”¨æˆ·',
            'traits': ['é¢‘ç¹ä½¿ç”¨æ ¸å¿ƒåŠŸèƒ½', 'å®Œæˆå…¥é—¨æµç¨‹', 'å¾ˆå°‘è”ç³»å®¢æœ'],
            'retention_pattern': 'é«˜ç•™å­˜ï¼Œç¨³å®šä½¿ç”¨'
        },
        1: {
            'name': 'æ¢ç´¢å‹ç”¨æˆ·', 
            'traits': ['å°è¯•å¤šç§åŠŸèƒ½', 'ä¸­ç­‰ä½¿ç”¨é¢‘ç‡', 'å¶å°”å¯»æ±‚å¸®åŠ©'],
            'retention_pattern': 'ä¸­ç­‰ç•™å­˜ï¼Œéœ€è¦å¼•å¯¼'
        },
        2: {
            'name': 'å›°éš¾ç”¨æˆ·',
            'traits': ['ä½¿ç”¨é¢‘ç‡ä½', 'ç»å¸¸è”ç³»å®¢æœ', 'å…¥é—¨æµç¨‹æœªå®Œæˆ'],
            'retention_pattern': 'ä½ç•™å­˜ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨'
        },
        3: {
            'name': 'è¯•ç”¨ç”¨æˆ·',
            'traits': ['çŸ­æœŸä½¿ç”¨', 'åŠŸèƒ½ä½¿ç”¨æµ…', 'å¿«é€Ÿæµå¤±'],
            'retention_pattern': 'æä½ç•™å­˜ï¼Œéœ€è¦æ¿€æ´»ç­–ç•¥'
        }
    }
    return characteristics.get(segment_id, {})
```

#### ç•™å­˜ä¼˜åŒ–ç­–ç•¥
```python
def generate_retention_strategies(retention_drivers):
    """ç”Ÿæˆç•™å­˜ä¼˜åŒ–ç­–ç•¥"""
    strategies = []
    
    for segment, data in retention_drivers.items():
        day_7_retention = data['retention_rates'].get('day_7', 0)
        day_30_retention = data['retention_rates'].get('day_30', 0)
        
        if day_7_retention < 0.4:  # 7å¤©ç•™å­˜ä½äº40%
            strategies.append({
                'segment': segment,
                'priority': 'urgent',
                'focus': 'æ—©æœŸæ¿€æ´»',
                'actions': [
                    'ä¼˜åŒ–ç”¨æˆ·å…¥é—¨æµç¨‹',
                    'å‘é€ä¸ªæ€§åŒ–å¼•å¯¼é‚®ä»¶',
                    'æä¾›ä¸€å¯¹ä¸€äº§å“æ¼”ç¤º',
                    'ç®€åŒ–æ ¸å¿ƒåŠŸèƒ½ä½¿ç”¨è·¯å¾„'
                ],
                'metrics': ['å…¥é—¨å®Œæˆç‡', 'é¦–æ¬¡ä»·å€¼å®ç°æ—¶é—´', '7å¤©æ´»è·ƒç‡']
            })
        
        if day_30_retention < 0.2:  # 30å¤©ç•™å­˜ä½äº20%
            strategies.append({
                'segment': segment,
                'priority': 'high',
                'focus': 'ä»·å€¼ä¼ é€’',
                'actions': [
                    'æ¨é€æˆåŠŸæ¡ˆä¾‹å’Œæœ€ä½³å®è·µ',
                    'æä¾›é«˜çº§åŠŸèƒ½åŸ¹è®­',
                    'å»ºç«‹ç”¨æˆ·ç¤¾åŒº',
                    'å®šæœŸå‘é€ä»·å€¼æŠ¥å‘Š'
                ],
                'metrics': ['åŠŸèƒ½é‡‡ç”¨ç‡', 'ç”¨æˆ·æ»¡æ„åº¦', '30å¤©ç•™å­˜ç‡']
            })
    
    return strategies
```

### 3. å†…å®¹å¹³å°ç”¨æˆ·å‚ä¸åº¦åˆ†æ

#### ä¸šåŠ¡èƒŒæ™¯
æŸå†…å®¹å¹³å°å¸Œæœ›åˆ†æç”¨æˆ·çš„å†…å®¹æ¶ˆè´¹è¡Œä¸ºï¼Œä¼˜åŒ–å†…å®¹æ¨èç®—æ³•å’Œç”¨æˆ·ä½“éªŒã€‚

#### åˆ†æå®ç°
```python
from engines.event_analysis_engine import EventAnalysisEngine
from engines.path_analysis_engine import PathAnalysisEngine

# 1. å†…å®¹æ¶ˆè´¹äº‹ä»¶åˆ†æ
event_engine = EventAnalysisEngine(storage_manager)

# å®šä¹‰å†…å®¹ç›¸å…³äº‹ä»¶
content_events = [
    'page_view',      # é¡µé¢æµè§ˆ
    'video_start',    # è§†é¢‘å¼€å§‹æ’­æ”¾
    'video_progress', # è§†é¢‘æ’­æ”¾è¿›åº¦
    'video_complete', # è§†é¢‘æ’­æ”¾å®Œæˆ
    'like',          # ç‚¹èµ
    'share',         # åˆ†äº«
    'comment',       # è¯„è®º
    'subscribe'      # è®¢é˜…
]

# åˆ†æäº‹ä»¶é¢‘æ¬¡å’Œè¶‹åŠ¿
event_frequency = event_engine.analyze_event_frequency(
    event_types=content_events,
    granularity='hour'  # æŒ‰å°æ—¶åˆ†æ
)

event_trends = event_engine.analyze_event_trends(
    event_types=content_events,
    window_size=24  # 24å°æ—¶æ»‘åŠ¨çª—å£
)

# 2. å†…å®¹æ¶ˆè´¹è·¯å¾„åˆ†æ
path_engine = PathAnalysisEngine(storage_manager)

# æå–ç”¨æˆ·å†…å®¹æ¶ˆè´¹è·¯å¾„
consumption_paths = path_engine.extract_user_paths(
    event_filter=content_events,
    session_timeout_minutes=60,  # 1å°æ—¶ä¼šè¯è¶…æ—¶
    min_path_length=3
)

# è¯†åˆ«é«˜ä»·å€¼è·¯å¾„
high_value_paths = path_engine.identify_high_value_paths(
    consumption_paths,
    value_events=['like', 'share', 'comment', 'subscribe'],
    min_value_score=2.0
)

# 3. ç”¨æˆ·å‚ä¸åº¦åˆ†ç¾¤
def calculate_engagement_metrics(user_events):
    """è®¡ç®—ç”¨æˆ·å‚ä¸åº¦æŒ‡æ ‡"""
    metrics = {}
    
    for user_id, events in user_events.groupby('user_pseudo_id'):
        user_metrics = {
            'session_count': len(events['session_id'].unique()),
            'total_events': len(events),
            'content_views': len(events[events['event_name'] == 'page_view']),
            'video_starts': len(events[events['event_name'] == 'video_start']),
            'video_completion_rate': calculate_video_completion_rate(events),
            'interaction_rate': calculate_interaction_rate(events),
            'avg_session_duration': calculate_avg_session_duration(events),
            'content_diversity': calculate_content_diversity(events)
        }
        metrics[user_id] = user_metrics
    
    return pd.DataFrame.from_dict(metrics, orient='index')

def calculate_video_completion_rate(user_events):
    """è®¡ç®—è§†é¢‘å®Œæˆç‡"""
    video_starts = len(user_events[user_events['event_name'] == 'video_start'])
    video_completes = len(user_events[user_events['event_name'] == 'video_complete'])
    
    if video_starts == 0:
        return 0
    return video_completes / video_starts

def calculate_interaction_rate(user_events):
    """è®¡ç®—äº’åŠ¨ç‡"""
    total_views = len(user_events[user_events['event_name'] == 'page_view'])
    interactions = len(user_events[
        user_events['event_name'].isin(['like', 'share', 'comment'])
    ])
    
    if total_views == 0:
        return 0
    return interactions / total_views
```

#### å†…å®¹æ¨èä¼˜åŒ–
```python
def optimize_content_recommendation(engagement_metrics, consumption_paths):
    """ä¼˜åŒ–å†…å®¹æ¨èç­–ç•¥"""
    recommendations = {}
    
    # åŸºäºç”¨æˆ·å‚ä¸åº¦åˆ†ç¾¤
    high_engagement = engagement_metrics[
        (engagement_metrics['interaction_rate'] > 0.1) &
        (engagement_metrics['video_completion_rate'] > 0.7)
    ]
    
    medium_engagement = engagement_metrics[
        (engagement_metrics['interaction_rate'] > 0.05) &
        (engagement_metrics['video_completion_rate'] > 0.4)
    ]
    
    low_engagement = engagement_metrics[
        (engagement_metrics['interaction_rate'] <= 0.05) |
        (engagement_metrics['video_completion_rate'] <= 0.4)
    ]
    
    # ä¸ºä¸åŒç¾¤ä½“åˆ¶å®šæ¨èç­–ç•¥
    recommendations['high_engagement'] = {
        'strategy': 'æ·±åº¦å†…å®¹æ¨è',
        'content_types': ['é•¿è§†é¢‘', 'ä¸“ä¸šå†…å®¹', 'ç³»åˆ—å†…å®¹'],
        'recommendation_count': 10,
        'personalization_weight': 0.8,
        'diversity_weight': 0.2
    }
    
    recommendations['medium_engagement'] = {
        'strategy': 'å…´è¶£æ¢ç´¢æ¨è',
        'content_types': ['ä¸­ç­‰é•¿åº¦è§†é¢‘', 'çƒ­é—¨å†…å®¹', 'ç›¸å…³æ¨è'],
        'recommendation_count': 8,
        'personalization_weight': 0.6,
        'diversity_weight': 0.4
    }
    
    recommendations['low_engagement'] = {
        'strategy': 'æ¿€æ´»å¼æ¨è',
        'content_types': ['çŸ­è§†é¢‘', 'çƒ­é—¨å†…å®¹', 'æ–°æ‰‹å‹å¥½å†…å®¹'],
        'recommendation_count': 6,
        'personalization_weight': 0.3,
        'diversity_weight': 0.7
    }
    
    return recommendations
```

## ğŸ† æœ€ä½³å®è·µ

### 1. æ•°æ®è´¨é‡ç®¡ç†

#### æ•°æ®éªŒè¯æœ€ä½³å®è·µ
```python
from tools.data_validator import DataValidator
import pandas as pd

class AdvancedDataValidator(DataValidator):
    """é«˜çº§æ•°æ®éªŒè¯å™¨"""
    
    def validate_business_logic(self, data: pd.DataFrame) -> dict:
        """éªŒè¯ä¸šåŠ¡é€»è¾‘"""
        issues = []
        
        # 1. æ—¶é—´é€»è¾‘éªŒè¯
        time_issues = self._validate_time_logic(data)
        issues.extend(time_issues)
        
        # 2. äº‹ä»¶åºåˆ—éªŒè¯
        sequence_issues = self._validate_event_sequences(data)
        issues.extend(sequence_issues)
        
        # 3. æ•°å€¼åˆç†æ€§éªŒè¯
        value_issues = self._validate_value_ranges(data)
        issues.extend(value_issues)
        
        return {
            'validation_passed': len(issues) == 0,
            'issues': issues,
            'recommendations': self._generate_fix_recommendations(issues)
        }
    
    def _validate_time_logic(self, data: pd.DataFrame) -> list:
        """éªŒè¯æ—¶é—´é€»è¾‘"""
        issues = []
        
        # æ£€æŸ¥æœªæ¥æ—¶é—´æˆ³
        current_timestamp = pd.Timestamp.now().timestamp() * 1000000
        future_events = data[data['event_timestamp'] > current_timestamp]
        if not future_events.empty:
            issues.append({
                'type': 'future_timestamp',
                'count': len(future_events),
                'severity': 'high',
                'message': f'å‘ç°{len(future_events)}ä¸ªæœªæ¥æ—¶é—´æˆ³äº‹ä»¶'
            })
        
        # æ£€æŸ¥æ—¶é—´æˆ³é¡ºåº
        for user_id in data['user_pseudo_id'].unique():
            user_events = data[data['user_pseudo_id'] == user_id].sort_values('event_timestamp')
            if not user_events['event_timestamp'].is_monotonic_increasing:
                issues.append({
                    'type': 'timestamp_order',
                    'user_id': user_id,
                    'severity': 'medium',
                    'message': f'ç”¨æˆ·{user_id}çš„äº‹ä»¶æ—¶é—´æˆ³é¡ºåºå¼‚å¸¸'
                })
        
        return issues
    
    def _validate_event_sequences(self, data: pd.DataFrame) -> list:
        """éªŒè¯äº‹ä»¶åºåˆ—"""
        issues = []
        
        # å®šä¹‰åˆç†çš„äº‹ä»¶åºåˆ—è§„åˆ™
        sequence_rules = {
            'purchase_without_view': {
                'condition': lambda df: (df['event_name'] == 'purchase') & 
                           (~df['user_pseudo_id'].isin(
                               df[df['event_name'] == 'page_view']['user_pseudo_id']
                           )),
                'message': 'å‘ç°æ²¡æœ‰é¡µé¢æµè§ˆå°±ç›´æ¥è´­ä¹°çš„å¼‚å¸¸è¡Œä¸º'
            },
            'add_to_cart_without_view': {
                'condition': lambda df: (df['event_name'] == 'add_to_cart') &
                           (~df['user_pseudo_id'].isin(
                               df[df['event_name'] == 'page_view']['user_pseudo_id']
                           )),
                'message': 'å‘ç°æ²¡æœ‰é¡µé¢æµè§ˆå°±åŠ è´­ç‰©è½¦çš„å¼‚å¸¸è¡Œä¸º'
            }
        }
        
        for rule_name, rule in sequence_rules.items():
            violations = data[rule['condition'](data)]
            if not violations.empty:
                issues.append({
                    'type': 'sequence_violation',
                    'rule': rule_name,
                    'count': len(violations),
                    'severity': 'medium',
                    'message': rule['message']
                })
        
        return issues
```

#### æ•°æ®æ¸…æ´—æœ€ä½³å®è·µ
```python
class SmartDataCleaner:
    """æ™ºèƒ½æ•°æ®æ¸…æ´—å™¨"""
    
    def __init__(self):
        self.cleaning_rules = self._load_cleaning_rules()
    
    def clean_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ™ºèƒ½æ•°æ®æ¸…æ´—"""
        cleaned_data = data.copy()
        
        # 1. å»é™¤é‡å¤äº‹ä»¶
        cleaned_data = self._remove_duplicate_events(cleaned_data)
        
        # 2. ä¿®å¤æ—¶é—´æˆ³æ ¼å¼
        cleaned_data = self._fix_timestamp_format(cleaned_data)
        
        # 3. æ ‡å‡†åŒ–äº‹ä»¶åç§°
        cleaned_data = self._standardize_event_names(cleaned_data)
        
        # 4. å¡«å……ç¼ºå¤±å€¼
        cleaned_data = self._fill_missing_values(cleaned_data)
        
        # 5. è¿‡æ»¤å¼‚å¸¸å€¼
        cleaned_data = self._filter_outliers(cleaned_data)
        
        return cleaned_data
    
    def _remove_duplicate_events(self, data: pd.DataFrame) -> pd.DataFrame:
        """å»é™¤é‡å¤äº‹ä»¶"""
        # åŸºäºç”¨æˆ·IDã€äº‹ä»¶åç§°å’Œæ—¶é—´æˆ³å»é‡
        duplicate_columns = ['user_pseudo_id', 'event_name', 'event_timestamp']
        
        # ä¿ç•™æœ€åä¸€ä¸ªé‡å¤äº‹ä»¶
        cleaned = data.drop_duplicates(
            subset=duplicate_columns,
            keep='last'
        )
        
        removed_count = len(data) - len(cleaned)
        if removed_count > 0:
            print(f"å·²ç§»é™¤ {removed_count} ä¸ªé‡å¤äº‹ä»¶")
        
        return cleaned
    
    def _standardize_event_names(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–äº‹ä»¶åç§°"""
        # äº‹ä»¶åç§°æ˜ å°„è§„åˆ™
        event_mapping = {
            'pageview': 'page_view',
            'page-view': 'page_view',
            'click': 'click_event',
            'btn_click': 'click_event',
            'purchase_complete': 'purchase',
            'buy': 'purchase',
            'signup': 'sign_up',
            'register': 'sign_up'
        }
        
        data['event_name'] = data['event_name'].replace(event_mapping)
        return data
```

### 2. æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

#### å¤§æ•°æ®å¤„ç†ä¼˜åŒ–
```python
import dask.dataframe as dd
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing as mp

class HighPerformanceAnalyzer:
    """é«˜æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self, max_workers=None):
        self.max_workers = max_workers or mp.cpu_count()
    
    def process_large_dataset(self, file_path: str, chunk_size: int = 100000):
        """å¤„ç†å¤§å‹æ•°æ®é›†"""
        # ä½¿ç”¨Daskè¿›è¡Œå¹¶è¡Œå¤„ç†
        ddf = dd.read_json(file_path, lines=True, blocksize="100MB")
        
        # å¹¶è¡Œæ‰§è¡ŒåŸºç¡€ç»Ÿè®¡
        results = {
            'total_events': len(ddf),
            'unique_users': ddf['user_pseudo_id'].nunique().compute(),
            'event_types': ddf['event_name'].value_counts().compute(),
            'date_range': {
                'start': ddf['event_date'].min().compute(),
                'end': ddf['event_date'].max().compute()
            }
        }
        
        return results
    
    def parallel_analysis(self, data_chunks: list, analysis_func):
        """å¹¶è¡Œåˆ†ææ‰§è¡Œ"""
        results = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_chunk = {
                executor.submit(analysis_func, chunk): chunk 
                for chunk in data_chunks
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_chunk):
                chunk = future_to_chunk[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as exc:
                    print(f'æ•°æ®å—åˆ†æå¤±è´¥: {exc}')
        
        return results
    
    def optimize_memory_usage(self, data: pd.DataFrame) -> pd.DataFrame:
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        # ä¼˜åŒ–æ•°æ®ç±»å‹
        for col in data.columns:
            if data[col].dtype == 'object':
                # å°è¯•è½¬æ¢ä¸ºcategoryç±»å‹
                if data[col].nunique() / len(data) < 0.5:
                    data[col] = data[col].astype('category')
            elif data[col].dtype == 'int64':
                # å°è¯•ä½¿ç”¨æ›´å°çš„æ•´æ•°ç±»å‹
                if data[col].min() >= 0:
                    if data[col].max() < 255:
                        data[col] = data[col].astype('uint8')
                    elif data[col].max() < 65535:
                        data[col] = data[col].astype('uint16')
                    elif data[col].max() < 4294967295:
                        data[col] = data[col].astype('uint32')
        
        return data
```

#### ç¼“å­˜ç­–ç•¥ä¼˜åŒ–
```python
import redis
import pickle
import hashlib
from functools import wraps

class AdvancedCache:
    """é«˜çº§ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, redis_url='redis://localhost:6379/0'):
        self.redis_client = redis.from_url(redis_url)
        self.default_ttl = 3600  # 1å°æ—¶
    
    def cache_result(self, ttl=None, key_prefix=''):
        """ç»“æœç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = self._generate_cache_key(
                    func.__name__, args, kwargs, key_prefix
                )
                
                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = self._get_from_cache(cache_key)
                if cached_result is not None:
                    return cached_result
                
                # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
                result = func(*args, **kwargs)
                self._set_to_cache(cache_key, result, ttl or self.default_ttl)
                
                return result
            return wrapper
        return decorator
    
    def _generate_cache_key(self, func_name, args, kwargs, prefix):
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åˆ›å»ºå‚æ•°çš„å“ˆå¸Œå€¼
        params_str = f"{args}_{kwargs}"
        params_hash = hashlib.md5(params_str.encode()).hexdigest()
        
        return f"{prefix}:{func_name}:{params_hash}"
    
    def _get_from_cache(self, key):
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        try:
            cached_data = self.redis_client.get(key)
            if cached_data:
                return pickle.loads(cached_data)
        except Exception as e:
            print(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        return None
    
    def _set_to_cache(self, key, value, ttl):
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        try:
            serialized_data = pickle.dumps(value)
            self.redis_client.setex(key, ttl, serialized_data)
        except Exception as e:
            print(f"ç¼“å­˜å†™å…¥å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
cache = AdvancedCache()

@cache.cache_result(ttl=7200, key_prefix='event_analysis')
def analyze_events_cached(data, event_types):
    """å¸¦ç¼“å­˜çš„äº‹ä»¶åˆ†æ"""
    # æ‰§è¡Œè€—æ—¶çš„äº‹ä»¶åˆ†æ
    return perform_event_analysis(data, event_types)
```

### 3. æ™ºèƒ½ä½“å¼€å‘æœ€ä½³å®è·µ

#### è‡ªå®šä¹‰æ™ºèƒ½ä½“å·¥å…·å¼€å‘
```python
from crewai.tools import BaseTool
from pydantic import BaseModel, Field
from typing import Dict, List, Any, Optional

class AdvancedAnalysisTool(BaseTool):
    """é«˜çº§åˆ†æå·¥å…·åŸºç±»"""
    
    name: str = "advanced_analysis_tool"
    description: str = "æ‰§è¡Œé«˜çº§æ•°æ®åˆ†æä»»åŠ¡"
    
    def __init__(self, storage_manager, config: Dict[str, Any] = None):
        super().__init__()
        self.storage_manager = storage_manager
        self.config = config or {}
        self.cache = {}
    
    def _run(self, **kwargs) -> Dict[str, Any]:
        """å·¥å…·æ‰§è¡Œå…¥å£"""
        try:
            # å‚æ•°éªŒè¯
            validated_params = self._validate_parameters(kwargs)
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(validated_params)
            if cache_key in self.cache:
                return self.cache[cache_key]
            
            # æ‰§è¡Œåˆ†æ
            result = self._execute_analysis(validated_params)
            
            # ç¼“å­˜ç»“æœ
            self.cache[cache_key] = result
            
            # åå¤„ç†
            processed_result = self._post_process_result(result)
            
            return processed_result
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'recommendations': self._get_error_recommendations(e)
            }
    
    def _validate_parameters(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """å‚æ•°éªŒè¯"""
        # å®ç°å…·ä½“çš„å‚æ•°éªŒè¯é€»è¾‘
        return params
    
    def _execute_analysis(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œåˆ†æé€»è¾‘"""
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°æ­¤æ–¹æ³•")
    
    def _post_process_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ç»“æœåå¤„ç†"""
        # æ·»åŠ å…ƒæ•°æ®
        result['metadata'] = {
            'tool_name': self.name,
            'execution_time': self._get_execution_time(),
            'data_quality_score': self._calculate_data_quality_score(result)
        }
        
        # ç”Ÿæˆæ´å¯Ÿ
        result['insights'] = self._generate_insights(result)
        
        return result
    
    def _generate_insights(self, result: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¸šåŠ¡æ´å¯Ÿ"""
        insights = []
        
        # åŸºäºç»“æœç”Ÿæˆæ´å¯Ÿ
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨è§„åˆ™å¼•æ“æˆ–æœºå™¨å­¦ä¹ æ¨¡å‹
        
        return insights

class CustomEventAnalysisTool(AdvancedAnalysisTool):
    """è‡ªå®šä¹‰äº‹ä»¶åˆ†æå·¥å…·"""
    
    name: str = "custom_event_analysis"
    description: str = "æ‰§è¡Œè‡ªå®šä¹‰äº‹ä»¶åˆ†æï¼ŒåŒ…æ‹¬é«˜çº§ç»Ÿè®¡å’Œæ¨¡å¼è¯†åˆ«"
    
    def _execute_analysis(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œäº‹ä»¶åˆ†æ"""
        event_types = params.get('event_types', [])
        date_range = params.get('date_range')
        
        # è·å–æ•°æ®
        events_data = self.storage_manager.get_events({
            'event_names': event_types,
            'date_range': date_range
        })
        
        # æ‰§è¡Œåˆ†æ
        results = {
            'basic_stats': self._calculate_basic_stats(events_data),
            'trend_analysis': self._analyze_trends(events_data),
            'pattern_detection': self._detect_patterns(events_data),
            'anomaly_detection': self._detect_anomalies(events_data)
        }
        
        return results
    
    def _detect_patterns(self, data: pd.DataFrame) -> Dict[str, Any]:
        """æ¨¡å¼æ£€æµ‹"""
        patterns = {}
        
        # æ—¶é—´æ¨¡å¼æ£€æµ‹
        data['hour'] = pd.to_datetime(data['event_timestamp'], unit='us').dt.hour
        hourly_pattern = data.groupby('hour')['event_name'].count()
        
        patterns['hourly_distribution'] = {
            'peak_hours': hourly_pattern.nlargest(3).index.tolist(),
            'low_hours': hourly_pattern.nsmallest(3).index.tolist(),
            'pattern_strength': self._calculate_pattern_strength(hourly_pattern)
        }
        
        # ç”¨æˆ·è¡Œä¸ºæ¨¡å¼
        user_patterns = data.groupby('user_pseudo_id')['event_name'].apply(list)
        common_sequences = self._find_common_sequences(user_patterns)
        
        patterns['behavior_sequences'] = common_sequences
        
        return patterns
```

#### æ™ºèƒ½ä½“åä½œä¼˜åŒ–
```python
from config.crew_config import CrewManager
from typing import Dict, List

class OptimizedCrewManager(CrewManager):
    """ä¼˜åŒ–çš„æ™ºèƒ½ä½“å›¢é˜Ÿç®¡ç†å™¨"""
    
    def __init__(self):
        super().__init__()
        self.execution_history = []
        self.performance_metrics = {}
    
    def execute_with_monitoring(self, inputs: dict = None):
        """å¸¦ç›‘æ§çš„æ‰§è¡Œ"""
        import time
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œå‰æ£€æŸ¥
            self._pre_execution_check()
            
            # æ‰§è¡Œä»»åŠ¡
            result = self.execute(inputs)
            
            # è®°å½•æ‰§è¡Œå†å²
            execution_time = time.time() - start_time
            self._record_execution(result, execution_time, success=True)
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            self._record_execution(str(e), execution_time, success=False)
            raise
    
    def _pre_execution_check(self):
        """æ‰§è¡Œå‰æ£€æŸ¥"""
        # æ£€æŸ¥æ™ºèƒ½ä½“çŠ¶æ€
        for agent_type, agent in self.agents.items():
            if not self._check_agent_health(agent):
                raise RuntimeError(f"æ™ºèƒ½ä½“ {agent_type} çŠ¶æ€å¼‚å¸¸")
        
        # æ£€æŸ¥èµ„æºå¯ç”¨æ€§
        if not self._check_resource_availability():
            raise RuntimeError("ç³»ç»Ÿèµ„æºä¸è¶³")
    
    def _check_agent_health(self, agent) -> bool:
        """æ£€æŸ¥æ™ºèƒ½ä½“å¥åº·çŠ¶æ€"""
        # å®ç°æ™ºèƒ½ä½“å¥åº·æ£€æŸ¥é€»è¾‘
        return True
    
    def _record_execution(self, result, execution_time, success):
        """è®°å½•æ‰§è¡Œå†å²"""
        record = {
            'timestamp': pd.Timestamp.now(),
            'execution_time': execution_time,
            'success': success,
            'result_summary': self._summarize_result(result) if success else result
        }
        
        self.execution_history.append(record)
        
        # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        self._update_performance_metrics(record)
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        if not self.execution_history:
            return {'message': 'æš‚æ— æ‰§è¡Œå†å²'}
        
        df = pd.DataFrame(self.execution_history)
        
        return {
            'total_executions': len(df),
            'success_rate': df['success'].mean(),
            'avg_execution_time': df['execution_time'].mean(),
            'recent_performance': df.tail(10)['success'].mean(),
            'performance_trend': self._calculate_performance_trend(df)
        }
```

### 4. å¯è§†åŒ–æœ€ä½³å®è·µ

#### å“åº”å¼å›¾è¡¨è®¾è®¡
```python
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

class ResponsiveChartGenerator:
    """å“åº”å¼å›¾è¡¨ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.default_config = {
            'displayModeBar': True,
            'displaylogo': False,
            'modeBarButtonsToRemove': ['pan2d', 'lasso2d']
        }
        
        self.theme_colors = {
            'primary': '#1f77b4',
            'secondary': '#ff7f0e', 
            'success': '#2ca02c',
            'warning': '#d62728',
            'info': '#9467bd'
        }
    
    def create_adaptive_dashboard(self, data: Dict[str, pd.DataFrame]) -> go.Figure:
        """åˆ›å»ºè‡ªé€‚åº”ä»ªè¡¨æ¿"""
        # æ ¹æ®æ•°æ®é‡è‡ªåŠ¨é€‰æ‹©å›¾è¡¨ç±»å‹
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('äº‹ä»¶è¶‹åŠ¿', 'ç”¨æˆ·åˆ†å¸ƒ', 'è½¬åŒ–æ¼æ–—', 'ç•™å­˜çƒ­åŠ›å›¾'),
            specs=[[{"secondary_y": True}, {"type": "pie"}],
                   [{"type": "funnel"}, {"type": "heatmap"}]]
        )
        
        # äº‹ä»¶è¶‹åŠ¿å›¾
        if 'events' in data and len(data['events']) > 0:
            trend_data = self._prepare_trend_data(data['events'])
            fig.add_trace(
                go.Scatter(
                    x=trend_data['date'],
                    y=trend_data['count'],
                    mode='lines+markers',
                    name='äº‹ä»¶æ•°é‡',
                    line=dict(color=self.theme_colors['primary'])
                ),
                row=1, col=1
            )
        
        # ç”¨æˆ·åˆ†å¸ƒé¥¼å›¾
        if 'users' in data:
            user_dist = self._prepare_user_distribution(data['users'])
            fig.add_trace(
                go.Pie(
                    labels=user_dist['category'],
                    values=user_dist['count'],
                    name="ç”¨æˆ·åˆ†å¸ƒ"
                ),
                row=1, col=2
            )
        
        # è½¬åŒ–æ¼æ–—
        if 'conversion' in data:
            funnel_data = data['conversion']
            fig.add_trace(
                go.Funnel(
                    y=funnel_data['step'],
                    x=funnel_data['users'],
                    name="è½¬åŒ–æ¼æ–—"
                ),
                row=2, col=1
            )
        
        # ç•™å­˜çƒ­åŠ›å›¾
        if 'retention' in data:
            retention_data = data['retention']
            fig.add_trace(
                go.Heatmap(
                    z=retention_data.values,
                    x=retention_data.columns,
                    y=retention_data.index,
                    colorscale='RdYlBu_r',
                    name="ç•™å­˜ç‡"
                ),
                row=2, col=2
            )
        
        # æ›´æ–°å¸ƒå±€
        fig.update_layout(
            height=800,
            showlegend=True,
            title_text="ç”¨æˆ·è¡Œä¸ºåˆ†æä»ªè¡¨æ¿",
            title_x=0.5
        )
        
        return fig
    
    def create_mobile_optimized_chart(self, data: pd.DataFrame, chart_type: str) -> go.Figure:
        """åˆ›å»ºç§»åŠ¨ç«¯ä¼˜åŒ–å›¾è¡¨"""
        fig = None
        
        if chart_type == 'line':
            fig = px.line(
                data, 
                x='date', 
                y='value',
                title='è¶‹åŠ¿åˆ†æ'
            )
        elif chart_type == 'bar':
            fig = px.bar(
                data,
                x='category',
                y='value', 
                title='åˆ†ç±»ç»Ÿè®¡'
            )
        
        if fig:
            # ç§»åŠ¨ç«¯ä¼˜åŒ–è®¾ç½®
            fig.update_layout(
                font_size=14,
                margin=dict(l=20, r=20, t=40, b=20),
                height=400,
                showlegend=False if len(data.columns) > 5 else True
            )
            
            # ç®€åŒ–å·¥å…·æ 
            fig.update_layout(
                config={
                    'displayModeBar': True,
                    'displaylogo': False,
                    'modeBarButtonsToRemove': [
                        'pan2d', 'lasso2d', 'zoom2d', 'zoomIn2d', 'zoomOut2d'
                    ]
                }
            )
        
        return fig
```

## ğŸ“Š ä¸šåŠ¡åœºæ™¯æ¨¡æ¿

### 1. ç”µå•†è½¬åŒ–ä¼˜åŒ–æ¨¡æ¿
```python
def ecommerce_conversion_analysis_template():
    """ç”µå•†è½¬åŒ–åˆ†ææ¨¡æ¿"""
    return {
        'analysis_steps': [
            {
                'step': 'data_preparation',
                'description': 'å‡†å¤‡ç”µå•†äº‹ä»¶æ•°æ®',
                'required_events': ['page_view', 'add_to_cart', 'begin_checkout', 'purchase'],
                'data_quality_checks': ['timestamp_validation', 'user_id_consistency', 'event_sequence_validation']
            },
            {
                'step': 'funnel_analysis', 
                'description': 'æ„å»ºè½¬åŒ–æ¼æ–—',
                'parameters': {
                    'conversion_window_hours': 24,
                    'attribution_model': 'first_touch',
                    'min_users_per_step': 100
                }
            },
            {
                'step': 'bottleneck_identification',
                'description': 'è¯†åˆ«è½¬åŒ–ç“¶é¢ˆ',
                'thresholds': {
                    'drop_rate_alert': 0.5,
                    'conversion_rate_warning': 0.02
                }
            },
            {
                'step': 'segment_analysis',
                'description': 'åˆ†ç¾¤è½¬åŒ–åˆ†æ',
                'segments': ['new_users', 'returning_users', 'mobile_users', 'desktop_users']
            }
        ],
        'kpis': [
            'overall_conversion_rate',
            'step_conversion_rates', 
            'average_time_to_convert',
            'revenue_per_visitor'
        ],
        'visualizations': [
            'conversion_funnel_chart',
            'conversion_rate_trend',
            'segment_comparison_chart',
            'bottleneck_heatmap'
        ]
    }
```

### 2. SaaSç”¨æˆ·æ¿€æ´»æ¨¡æ¿
```python
def saas_user_activation_template():
    """SaaSç”¨æˆ·æ¿€æ´»åˆ†ææ¨¡æ¿"""
    return {
        'activation_events': [
            'sign_up',
            'email_verification', 
            'profile_completion',
            'first_feature_use',
            'tutorial_completion',
            'first_value_achievement'
        ],
        'analysis_framework': {
            'time_to_activation': {
                'measurement': 'hours_from_signup_to_first_value',
                'target': '<24 hours',
                'segments': ['trial_users', 'paid_users']
            },
            'activation_funnel': {
                'steps': ['signup', 'verification', 'onboarding', 'first_use', 'value_realization'],
                'success_criteria': 'completion_of_all_steps_within_7_days'
            },
            'feature_adoption': {
                'core_features': ['feature_a', 'feature_b', 'feature_c'],
                'adoption_threshold': 'use_within_first_week'
            }
        },
        'optimization_strategies': [
            'onboarding_flow_optimization',
            'email_sequence_improvement', 
            'in_app_guidance_enhancement',
            'feature_discovery_improvement'
        ]
    }
```

## ğŸ“ å­¦ä¹ èµ„æº

### æ¨èé˜…è¯»
1. **æ•°æ®åˆ†æåŸºç¡€**
   - ã€ŠPythonæ•°æ®åˆ†æå®æˆ˜ã€‹
   - ã€Šç”¨æˆ·è¡Œä¸ºæ•°æ®åˆ†æã€‹
   - ã€Šå¢é•¿é»‘å®¢ã€‹

2. **æœºå™¨å­¦ä¹ åº”ç”¨**
   - ã€Šæœºå™¨å­¦ä¹ å®æˆ˜ã€‹
   - ã€Šç”¨æˆ·ç”»åƒï¼šæ–¹æ³•è®ºä¸å·¥ç¨‹åŒ–è§£å†³æ–¹æ¡ˆã€‹

3. **ä¸šåŠ¡åˆ†æ**
   - ã€Šç²¾ç›Šæ•°æ®åˆ†æã€‹
   - ã€Šæ•°æ®é©±åŠ¨å¢é•¿ã€‹

### åœ¨çº¿è¯¾ç¨‹
- Coursera: Data Analysis and Visualization
- edX: Introduction to Analytics Modeling
- Udacity: Data Analyst Nanodegree

### å®è·µé¡¹ç›®
1. **ä¸ªäººé¡¹ç›®**: åˆ†æè‡ªå·±çš„ç½‘ç«™æˆ–åº”ç”¨æ•°æ®
2. **å¼€æºè´¡çŒ®**: å‚ä¸ç›¸å…³å¼€æºé¡¹ç›®
3. **æ¡ˆä¾‹ç ”ç©¶**: å¤ç°çŸ¥åå…¬å¸çš„åˆ†ææ¡ˆä¾‹

---

*æœ¬ç¤ºä¾‹å’Œæœ€ä½³å®è·µæ–‡æ¡£å°†æŒç»­æ›´æ–°ï¼Œæ¬¢è¿ç¤¾åŒºè´¡çŒ®æ›´å¤šå®ç”¨æ¡ˆä¾‹å’Œç»éªŒåˆ†äº«ã€‚*