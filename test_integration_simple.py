"""
ç®€å•çš„ç³»ç»Ÿé›†æˆæµ‹è¯•

æµ‹è¯•ç³»ç»Ÿé›†æˆç®¡ç†å™¨çš„åŸºæœ¬åŠŸèƒ½ï¼Œä¸ä¾èµ–CrewAI
"""

import json
import tempfile
import time
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from system.standalone_integration_manager import StandaloneIntegrationManager, WorkflowConfig
from utils.logger import setup_logger

logger = setup_logger()


def create_test_data():
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    sample_events = []
    
    # ç”Ÿæˆ100ä¸ªç¤ºä¾‹äº‹ä»¶
    for i in range(100):
        event = {
            "event_date": "20241201",
            "event_timestamp": 1733097600000000 + i * 1000000,
            "event_name": ["page_view", "sign_up", "login", "purchase"][i % 4],
            "user_pseudo_id": f"user_{i % 20}",
            "user_id": f"user_{i % 20}" if i % 5 == 0 else None,
            "platform": "WEB",
            "device": {
                "category": "desktop",
                "operating_system": "Windows",
                "browser": "Chrome"
            },
            "geo": {
                "country": "China",
                "region": "Beijing",
                "city": "Beijing"
            },
            "traffic_source": {
                "name": "(direct)",
                "medium": "(none)",
                "source": "(direct)"
            },
            "event_params": [],
            "user_properties": [],
            "items": []
        }
        sample_events.append(event)
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.ndjson', delete=False)
    for event in sample_events:
        temp_file.write(json.dumps(event, ensure_ascii=False) + '\n')
    temp_file.close()
    
    return temp_file.name


def test_integration_manager():
    """æµ‹è¯•é›†æˆç®¡ç†å™¨"""
    logger.info("å¼€å§‹æµ‹è¯•ç³»ç»Ÿé›†æˆç®¡ç†å™¨")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_file = create_test_data()
    logger.info(f"åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶: {test_file}")
    
    try:
        # åˆå§‹åŒ–é›†æˆç®¡ç†å™¨
        config = WorkflowConfig(
            enable_parallel_processing=True,
            max_workers=2,
            memory_limit_gb=4.0,
            timeout_minutes=10,
            enable_caching=True,
            enable_monitoring=True,
            auto_cleanup=True
        )
        
        integration_manager = StandaloneIntegrationManager(config)
        logger.info("âœ… é›†æˆç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # æ‰§è¡Œå®Œæ•´å·¥ä½œæµç¨‹
        logger.info("å¼€å§‹æ‰§è¡Œå·¥ä½œæµç¨‹")
        start_time = time.time()
        
        result = integration_manager.execute_complete_workflow(
            file_path=test_file,
            analysis_types=['event_analysis', 'retention_analysis']
        )
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        logger.info(f"âœ… å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        
        # éªŒè¯ç»“æœ
        assert 'workflow_id' in result
        assert 'execution_summary' in result
        assert 'analysis_results' in result
        
        execution_summary = result['execution_summary']
        logger.info(f"æˆåŠŸåˆ†æ: {execution_summary['successful_analyses']}")
        logger.info(f"å¤±è´¥åˆ†æ: {execution_summary['failed_analyses']}")
        logger.info(f"æ•°æ®å¤§å°: {execution_summary['data_size']}")
        
        # æ£€æŸ¥åˆ†æç»“æœ
        analysis_results = result['analysis_results']
        for analysis_type, analysis_result in analysis_results.items():
            logger.info(f"{analysis_type}: {analysis_result['status']}")
            if analysis_result['status'] == 'completed':
                logger.info(f"  æ´å¯Ÿæ•°é‡: {len(analysis_result['insights'])}")
                logger.info(f"  å»ºè®®æ•°é‡: {len(analysis_result['recommendations'])}")
        
        # æµ‹è¯•ç³»ç»Ÿå¥åº·çŠ¶æ€
        health = integration_manager.get_system_health()
        logger.info(f"ç³»ç»ŸçŠ¶æ€: {health['overall_status']}")
        logger.info(f"ç¼“å­˜é¡¹æ•°é‡: {health['cache_size']}")
        
        # æµ‹è¯•å¯¼å‡ºåŠŸèƒ½
        try:
            workflow_id = result['workflow_id']
            export_file = integration_manager.export_workflow_results(
                workflow_id=workflow_id,
                export_format='json',
                include_raw_data=False
            )
            logger.info(f"âœ… ç»“æœå¯¼å‡ºæˆåŠŸ: {export_file}")
            
            # éªŒè¯å¯¼å‡ºæ–‡ä»¶
            with open(export_file, 'r', encoding='utf-8') as f:
                exported_data = json.load(f)
            assert 'workflow_id' in exported_data
            logger.info("âœ… å¯¼å‡ºæ–‡ä»¶éªŒè¯é€šè¿‡")
            
        except Exception as e:
            logger.warning(f"å¯¼å‡ºæµ‹è¯•å¤±è´¥: {e}")
        
        # å…³é—­é›†æˆç®¡ç†å™¨
        integration_manager.shutdown()
        logger.info("âœ… é›†æˆç®¡ç†å™¨å·²å…³é—­")
        
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        import os
        if os.path.exists(test_file):
            os.unlink(test_file)
            logger.info(f"æ¸…ç†æµ‹è¯•æ–‡ä»¶: {test_file}")


if __name__ == "__main__":
    test_integration_manager()