#!/usr/bin/env python3
"""
æµ‹è¯•ç”¨æˆ·è¡Œä¸ºåˆ†æå¹³å°åŸºç¡€åŠŸèƒ½
"""

import logging
from config.crew_config import get_llm, create_agent

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_basic_agent():
    """æµ‹è¯•åŸºç¡€æ™ºèƒ½ä½“åŠŸèƒ½"""
    try:
        print("=== æµ‹è¯•åŸºç¡€æ™ºèƒ½ä½“åŠŸèƒ½ ===")
        
        # åˆ›å»ºä¸€ä¸ªäº‹ä»¶åˆ†ææ™ºèƒ½ä½“
        agent = create_agent("event_analyst", provider="volcano")
        print(f"æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ: {agent.role}")
        print(f"æ™ºèƒ½ä½“ç›®æ ‡: {agent.goal}")
        
        # æµ‹è¯•ç®€å•çš„åˆ†æä»»åŠ¡
        test_prompt = "è¯·åˆ†æä¸€ä¸‹ç”¨æˆ·åœ¨ç”µå•†ç½‘ç«™ä¸Šçš„è´­ä¹°è¡Œä¸ºæ¨¡å¼ã€‚"
        
        # ä½¿ç”¨æ™ºèƒ½ä½“çš„ LLM è¿›è¡Œåˆ†æ
        if hasattr(agent.llm, 'invoke'):
            response = agent.llm.invoke(test_prompt)
        else:
            # ä½¿ç”¨æ—§ç‰ˆæœ¬çš„ predict æ–¹æ³•
            response = agent.llm.predict(test_prompt)
        
        print(f"\næ™ºèƒ½ä½“å“åº”:")
        print(f"å“åº”é•¿åº¦: {len(response)}")
        print(f"å“åº”å†…å®¹: {response[:200]}...")
        
        if response and len(response) > 50:
            print("âœ… æ™ºèƒ½ä½“åŸºç¡€åŠŸèƒ½æµ‹è¯•æˆåŠŸ")
            return True
        else:
            print("âŒ æ™ºèƒ½ä½“å“åº”è¿‡çŸ­")
            return False
            
    except Exception as e:
        print(f"âŒ æ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_llm_providers():
    """æµ‹è¯•ä¸åŒçš„ LLM æä¾›å•†"""
    try:
        print("\n=== æµ‹è¯• LLM æä¾›å•† ===")
        
        # æµ‹è¯• Volcano æä¾›å•†
        print("æµ‹è¯• Volcano æä¾›å•†...")
        volcano_llm = get_llm(provider="volcano")
        volcano_response = volcano_llm.invoke("ç®€å•ä»‹ç»ä¸€ä¸‹ç”¨æˆ·è¡Œä¸ºåˆ†æçš„é‡è¦æ€§ã€‚")
        print(f"Volcano å“åº”é•¿åº¦: {len(volcano_response)}")
        print(f"Volcano å“åº”: {volcano_response[:100]}...")
        
        # æµ‹è¯•é»˜è®¤æä¾›å•†
        print("\næµ‹è¯•é»˜è®¤æä¾›å•†...")
        default_llm = get_llm()
        default_response = default_llm.invoke("ä»€ä¹ˆæ˜¯ç”¨æˆ·ç•™å­˜åˆ†æï¼Ÿ")
        print(f"é»˜è®¤æä¾›å•†å“åº”é•¿åº¦: {len(default_response)}")
        print(f"é»˜è®¤æä¾›å•†å“åº”: {default_response[:100]}...")
        
        print("âœ… LLM æä¾›å•†æµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ LLM æä¾›å•†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ç”¨æˆ·è¡Œä¸ºåˆ†æå¹³å°åŸºç¡€åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    success_count = 0
    total_tests = 2
    
    # æµ‹è¯•åŸºç¡€æ™ºèƒ½ä½“åŠŸèƒ½
    if test_basic_agent():
        success_count += 1
    
    # æµ‹è¯• LLM æä¾›å•†
    if test_llm_providers():
        success_count += 1
    
    print(f"\næµ‹è¯•ç»“æœ: {success_count}/{total_tests} é€šè¿‡")
    
    if success_count == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¹³å°åŸºç¡€åŠŸèƒ½æ­£å¸¸")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")

if __name__ == "__main__":
    main()