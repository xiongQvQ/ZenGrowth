#!/usr/bin/env python3
"""
æœ€ç»ˆå®Œæ•´æµ‹è¯• - æ¨¡æ‹Ÿå®Œæ•´çš„ Streamlit å·¥ä½œæµç¨‹
"""

import pandas as pd
from pathlib import Path
from tools.ga4_data_parser import GA4DataParser
from tools.data_storage_manager import DataStorageManager
from system.integration_manager import IntegrationManager, WorkflowConfig

def simulate_streamlit_workflow():
    """æ¨¡æ‹Ÿå®Œæ•´çš„ Streamlit å·¥ä½œæµç¨‹"""
    try:
        print("ğŸ¯ æ¨¡æ‹Ÿå®Œæ•´çš„ Streamlit å·¥ä½œæµç¨‹...")
        print("=" * 50)
        
        # 1. æ¨¡æ‹Ÿæ•°æ®ä¸Šä¼ å’Œå¤„ç†ï¼ˆmain.py ä¸­çš„é€»è¾‘ï¼‰
        print("1. ğŸ“¤ æ¨¡æ‹Ÿæ•°æ®ä¸Šä¼ å’Œå¤„ç†...")
        
        data_file = Path("data/events_ga4.ndjson")
        parser = GA4DataParser()
        raw_data = parser.parse_ndjson(str(data_file))
        print(f"   âœ… è§£æäº† {len(raw_data)} ä¸ªåŸå§‹äº‹ä»¶")
        
        # æå–æ•°æ®
        events_data = parser.extract_events(raw_data)
        user_data = parser.extract_user_properties(raw_data)
        session_data = parser.extract_sessions(raw_data)
        
        # å¤„ç†äº‹ä»¶æ•°æ®ï¼ˆmain.py ä¸­çš„ä¿®å¤é€»è¾‘ï¼‰
        if isinstance(events_data, dict):
            all_events_list = []
            for event_type, event_df in events_data.items():
                if not event_df.empty:
                    all_events_list.append(event_df)
            
            if all_events_list:
                combined_events = pd.concat(all_events_list, ignore_index=True)
                print(f"   âœ… åˆå¹¶äº† {len(events_data)} ç§äº‹ä»¶ç±»å‹ï¼Œæ€»è®¡ {len(combined_events)} ä¸ªäº‹ä»¶")
            else:
                combined_events = pd.DataFrame()
        else:
            combined_events = events_data
        
        # 2. æ¨¡æ‹Ÿå­˜å‚¨åˆ°ä¼šè¯çŠ¶æ€
        print("2. ğŸ’¾ æ¨¡æ‹Ÿæ•°æ®å­˜å‚¨...")
        
        # åˆ›å»ºå­˜å‚¨ç®¡ç†å™¨ï¼ˆæ¨¡æ‹Ÿ st.session_state.storage_managerï¼‰
        storage_manager = DataStorageManager()
        storage_manager.store_events(combined_events)
        storage_manager.store_users(user_data)
        storage_manager.store_sessions(session_data)
        
        # éªŒè¯å­˜å‚¨
        stored_events = storage_manager.get_data('events')
        print(f"   âœ… å­˜å‚¨äº† {len(stored_events)} ä¸ªäº‹ä»¶")
        
        # 3. æ¨¡æ‹Ÿé›†æˆç®¡ç†å™¨åˆå§‹åŒ–
        print("3. ğŸ”— æ¨¡æ‹Ÿé›†æˆç®¡ç†å™¨åˆå§‹åŒ–...")
        
        config = WorkflowConfig(
            enable_parallel_processing=True,
            max_workers=4,
            timeout_minutes=30,
            enable_caching=True,
            auto_cleanup=True
        )
        
        # åˆ›å»ºé›†æˆç®¡ç†å™¨ï¼ˆæ¨¡æ‹Ÿ st.session_state.integration_managerï¼‰
        integration_manager = IntegrationManager(config)
        
        # åº”ç”¨ä¿®å¤ï¼šåˆ·æ–°å­˜å‚¨ç®¡ç†å™¨
        integration_manager.refresh_storage_manager(storage_manager)
        print("   âœ… é›†æˆç®¡ç†å™¨å·²åˆ·æ–°å­˜å‚¨ç®¡ç†å™¨")
        
        # 4. æµ‹è¯•æ™ºèƒ½åˆ†æåŠŸèƒ½
        print("4. ğŸš€ æµ‹è¯•æ™ºèƒ½åˆ†æåŠŸèƒ½...")
        
        # æµ‹è¯•å„ç§åˆ†æç±»å‹
        analysis_types = ['event_analysis', 'retention_analysis', 'conversion_analysis']
        
        for analysis_type in analysis_types:
            try:
                print(f"   ğŸ§ª æµ‹è¯• {analysis_type}...")
                result = integration_manager._execute_single_analysis(analysis_type)
                print(f"   âœ… {analysis_type}: {result.status}")
                
                if result.status == 'failed':
                    print(f"   âŒ å¤±è´¥åŸå› : {result.error_message}")
                    
            except Exception as e:
                print(f"   âŒ {analysis_type} å¼‚å¸¸: {e}")
        
        # 5. æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹
        print("5. ğŸ”„ æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹...")
        
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶æ¥æ¨¡æ‹Ÿæ–‡ä»¶ä¸Šä¼ 
            temp_file = Path("temp_test_data.ndjson")
            temp_file.write_text("\\n".join([str(event) for event in raw_data[:100]]))  # ä½¿ç”¨å‰100ä¸ªäº‹ä»¶
            
            workflow_result = integration_manager.execute_complete_workflow(
                file_path=str(temp_file),
                analysis_types=['event_analysis', 'retention_analysis']
            )
            
            print(f"   âœ… å®Œæ•´å·¥ä½œæµç¨‹çŠ¶æ€: {workflow_result.get('status', 'unknown')}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file.exists():
                temp_file.unlink()
                
        except Exception as e:
            print(f"   âš ï¸ å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•è·³è¿‡: {e}")
        
        print("\\nğŸ‰ å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•æˆåŠŸï¼")
        print("=" * 50)
        print("âœ… æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½éƒ½å·²éªŒè¯å·¥ä½œæ­£å¸¸")
        print("âœ… æ•°æ®åŠ è½½å’Œå­˜å‚¨æ­£å¸¸")
        print("âœ… åˆ†æå¼•æ“æ­£å¸¸å·¥ä½œ")
        print("âœ… é›†æˆç®¡ç†å™¨æ­£å¸¸å·¥ä½œ")
        print("\\nğŸš€ ç°åœ¨ Streamlit åº”ç”¨åº”è¯¥å¯ä»¥å®Œå…¨æ­£å¸¸å·¥ä½œäº†ï¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ å®Œæ•´æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = simulate_streamlit_workflow()
    if success:
        print("\\nğŸŠ æ­å–œï¼æ‰€æœ‰é—®é¢˜éƒ½å·²è§£å†³ï¼")
        print("ç°åœ¨å¯ä»¥å¯åŠ¨ Streamlit åº”ç”¨äº†:")
        print("   streamlit run main.py")
    else:
        print("\\nâŒ ä»æœ‰é—®é¢˜éœ€è¦è§£å†³")