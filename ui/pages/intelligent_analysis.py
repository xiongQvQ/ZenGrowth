"""
æ™ºèƒ½åˆ†æé¡µé¢
æä¾›å…¨é¢çš„å¤šç»´åº¦åˆ†æåŠŸèƒ½
"""

import streamlit as st
import pandas as pd
import json
import time
from pathlib import Path
from ui.components.common import render_data_status_check
from ui.state.state_manager import get_state_manager
from system.integration_manager_singleton import get_integration_manager
from utils.i18n import t

@render_data_status_check
def show_intelligent_analysis_page():
    """æ˜¾ç¤ºæ™ºèƒ½åˆ†æé¡µé¢"""
    st.header("ğŸ¤– " + t("pages.intelligent_analysis.title", "æ™ºèƒ½åˆ†æ"))
    st.markdown("---")
    
    st.markdown(t('analysis.intelligent_description', 'æ™ºèƒ½åˆ†ææä¾›å…¨é¢çš„å¤šç»´åº¦ç”¨æˆ·è¡Œä¸ºåˆ†æï¼Œå¸®åŠ©æ‚¨æ·±å…¥äº†è§£ç”¨æˆ·è¡Œä¸ºæ¨¡å¼ã€è¯†åˆ«ä¼˜åŒ–æœºä¼šå¹¶åˆ¶å®šæ•°æ®é©±åŠ¨çš„å†³ç­–ã€‚'))
    
    # åˆ†æé…ç½®
    st.subheader(t('analysis.analysis_config_header', 'åˆ†æé…ç½®'))
    
    col1, col2 = st.columns(2)
    
    with col1:
        analysis_types = st.multiselect(
            t('analysis.select_analysis_types', 'é€‰æ‹©åˆ†æç±»å‹'),
            options=[
                'event_analysis',
                'retention_analysis', 
                'conversion_analysis',
                'user_segmentation',
                'path_analysis'
            ],
            default=[
                'event_analysis',
                'retention_analysis', 
                'conversion_analysis'
            ],
            format_func=lambda x: {
                'event_analysis': t('analysis.event_analysis_option', 'äº‹ä»¶åˆ†æ'),
                'retention_analysis': t('analysis.retention_analysis_option', 'ç•™å­˜åˆ†æ'),
                'conversion_analysis': t('analysis.conversion_analysis_option', 'è½¬åŒ–åˆ†æ'),
                'user_segmentation': t('analysis.user_segmentation_option', 'ç”¨æˆ·åˆ†ç¾¤'),
                'path_analysis': t('analysis.path_analysis_option', 'è·¯å¾„åˆ†æ')
            }.get(x, x)
        )
        
        enable_parallel = st.checkbox(
            t('analysis.enable_parallel_processing', 'å¯ç”¨å¹¶è¡Œå¤„ç†'),
            value=True,
            help=t('analysis.parallel_processing_help', 'å¹¶è¡Œå¤„ç†å¯ä»¥åŠ é€Ÿåˆ†ææ‰§è¡Œï¼Œä½†ä¼šæ¶ˆè€—æ›´å¤šç³»ç»Ÿèµ„æº')
        )
    
    with col2:
        max_workers = st.slider(
            t('analysis.max_parallel_tasks', 'æœ€å¤§å¹¶è¡Œä»»åŠ¡æ•°'),
            min_value=1,
            max_value=8,
            value=4,
            disabled=not enable_parallel
        )
        
        enable_caching = st.checkbox(
            t('analysis.enable_intelligent_cache', 'å¯ç”¨æ™ºèƒ½ç¼“å­˜'),
            value=True,
            help=t('analysis.intelligent_cache_help', 'æ™ºèƒ½ç¼“å­˜å¯ä»¥åŠ é€Ÿé‡å¤åˆ†æï¼Œæé«˜å“åº”é€Ÿåº¦')
        )
    
    # æ‰§è¡Œåˆ†æ
    st.subheader(t('analysis.execute_analysis_header', 'æ‰§è¡Œåˆ†æ'))
    
    if not analysis_types:
        st.warning(t('analysis.select_analysis_type_warning', 'è¯·è‡³å°‘é€‰æ‹©ä¸€ç§åˆ†æç±»å‹'))
        return
    
    col1, col2, col3 = st.columns([1, 1, 2])
    
    with col1:
        if st.button(t('analysis.start_intelligent_analysis', 'å¼€å§‹æ™ºèƒ½åˆ†æ'), type="primary"):
            execute_intelligent_analysis(analysis_types, enable_parallel, max_workers, enable_caching)
    
    with col2:
        if st.button(t('analysis.view_system_status', 'æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€')):
            show_system_status()
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    state_manager = get_state_manager()
    workflow_results = state_manager.get_analysis_results('workflow')
    
    if workflow_results:
        show_workflow_results()


def execute_intelligent_analysis(analysis_types, enable_parallel, max_workers, enable_caching):
    """æ‰§è¡Œæ™ºèƒ½åˆ†æ"""
    state_manager = get_state_manager()
    
    # è·å–é›†æˆç®¡ç†å™¨
    integration_manager = get_integration_manager()
    if integration_manager is None:
        st.error("âŒ ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•")
        return
    
    # æ›´æ–°é›†æˆç®¡ç†å™¨é…ç½®
    if hasattr(integration_manager, 'config'):
        integration_manager.config.enable_parallel_processing = enable_parallel
        integration_manager.config.max_workers = max_workers
        integration_manager.config.enable_caching = enable_caching
    
    # è®¾ç½®åˆ†æçŠ¶æ€
    state_manager.set_analyzing(True)
    
    # åˆ›å»ºè¿›åº¦å®¹å™¨
    progress_container = st.container()
    
    with progress_container:
        st.subheader("âš™ï¸ æ™ºèƒ½åˆ†æè¿›åº¦")
        
        # åˆ›å»ºè¿›åº¦æ¡å’ŒçŠ¶æ€æ˜¾ç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()
        metrics_container = st.empty()
        
        try:
            # å‡†å¤‡ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            upload_dir = Path("data/uploads")
            upload_dir.mkdir(parents=True, exist_ok=True)
            
            # ä»çŠ¶æ€ç®¡ç†å™¨è·å–åŸå§‹æ•°æ®
            raw_data = state_manager.get_raw_data()
            
            if raw_data is not None:
                temp_file_path = upload_dir / f"temp_analysis_{int(time.time())}.ndjson"
                
                # å°†DataFrameè½¬æ¢å›NDJSONæ ¼å¼
                with open(temp_file_path, 'w', encoding='utf-8') as f:
                    for _, row in raw_data.iterrows():
                        # é‡æ„åŸå§‹äº‹ä»¶æ ¼å¼
                        event_data = {
                            'event_date': row.get('event_date', ''),
                            'event_timestamp': row.get('event_timestamp', 0),
                            'event_name': row.get('event_name', ''),
                            'user_pseudo_id': row.get('user_pseudo_id', ''),
                            'user_id': row.get('user_id'),
                            'platform': row.get('platform', 'WEB'),
                            'device': row.get('device', {}),
                            'geo': row.get('geo', {}),
                            'traffic_source': row.get('traffic_source', {}),
                            'event_params': row.get('event_params', []),
                            'user_properties': row.get('user_properties', []),
                            'items': row.get('items', [])
                        }
                        f.write(json.dumps(event_data, ensure_ascii=False) + '\n')
                
                status_text.text("ğŸš€ å¯åŠ¨æ™ºèƒ½åˆ†æå·¥ä½œæµç¨‹...")
                progress_bar.progress(10)
                
                # æ‰§è¡Œå®Œæ•´å·¥ä½œæµç¨‹
                start_time = time.time()
                
                result = integration_manager.execute_complete_workflow(
                    file_path=str(temp_file_path),
                    analysis_types=analysis_types
                )
                
                end_time = time.time()
                total_time = end_time - start_time
                
                # æ›´æ–°è¿›åº¦
                progress_bar.progress(100)
                status_text.text("âœ… æ™ºèƒ½åˆ†æå®Œæˆ!")
                
                # å­˜å‚¨å·¥ä½œæµç¨‹ç»“æœ
                state_manager.set_analysis_results('workflow', result)
                
                # æ˜¾ç¤ºæ‰§è¡ŒæŒ‡æ ‡
                if result and 'execution_summary' in result:
                    execution_summary = result['execution_summary']
                    
                    with metrics_container.container():
                        col1, col2, col3, col4 = st.columns(4)
                        
                        with col1:
                            st.metric(
                                "æ€»æ‰§è¡Œæ—¶é—´",
                                f"{total_time:.2f}ç§’",
                                help="å®Œæ•´å·¥ä½œæµç¨‹çš„æ‰§è¡Œæ—¶é—´"
                            )
                        
                        with col2:
                            st.metric(
                                "æˆåŠŸåˆ†æ",
                                execution_summary.get('successful_analyses', 0),
                                help="æˆåŠŸå®Œæˆçš„åˆ†æä»»åŠ¡æ•°é‡"
                            )
                        
                        with col3:
                            st.metric(
                                "å¤„ç†æ•°æ®é‡",
                                f"{execution_summary.get('data_size', 0):,}",
                                help="å¤„ç†çš„äº‹ä»¶è®°å½•æ•°é‡"
                            )
                        
                        with col4:
                            processing_rate = execution_summary.get('data_size', 0) / total_time if total_time > 0 else 0
                            st.metric(
                                "å¤„ç†é€Ÿç‡",
                                f"{processing_rate:.0f}/ç§’",
                                help="æ¯ç§’å¤„ç†çš„è®°å½•æ•°é‡"
                            )
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    temp_file_path.unlink()
                except:
                    pass
                    
                st.success("ğŸ‰ æ™ºèƒ½åˆ†æå®Œæˆï¼è¯·æŸ¥çœ‹ä¸‹æ–¹çš„è¯¦ç»†ç»“æœã€‚")
                st.rerun()
                
            else:
                st.error("âŒ æœªæ‰¾åˆ°åˆ†ææ•°æ®ï¼Œè¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶")
                
        except Exception as e:
            st.error(f"âŒ æ™ºèƒ½åˆ†ææ‰§è¡Œå¤±è´¥: {str(e)}")
            import traceback
            st.text("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            st.text(traceback.format_exc())
        finally:
            state_manager.set_analyzing(False)


def show_system_status():
    """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
    state_manager = get_state_manager()
    integration_manager = get_integration_manager()
    
    st.subheader("ğŸ”§ ç³»ç»ŸçŠ¶æ€")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ç³»ç»Ÿç»„ä»¶**")
        st.write(f"â€¢ é›†æˆç®¡ç†å™¨: {'âœ… å·²åˆå§‹åŒ–' if integration_manager else 'âŒ æœªåˆå§‹åŒ–'}")
        st.write(f"â€¢ çŠ¶æ€ç®¡ç†å™¨: {'âœ… è¿è¡Œä¸­' if state_manager else 'âŒ æœªè¿è¡Œ'}")
        st.write(f"â€¢ æ•°æ®çŠ¶æ€: {'âœ… å·²åŠ è½½' if state_manager.is_data_loaded() else 'âŒ æ— æ•°æ®'}")
    
    with col2:
        st.write("**åˆ†æçŠ¶æ€**")
        if state_manager.is_analyzing():
            progress, step = state_manager.get_analysis_progress()
            st.write(f"â€¢ æ­£åœ¨åˆ†æ: {step}")
            st.progress(progress)
        else:
            st.write("â€¢ åˆ†æçŠ¶æ€: ç©ºé—²")
        
        # APIå¥åº·çŠ¶æ€
        api_health = state_manager.get_api_health()
        if api_health:
            for provider, health_info in api_health.items():
                status = "âœ… å¥åº·" if health_info.get('healthy', False) else "âŒ å¼‚å¸¸"
                st.write(f"â€¢ {provider}: {status}")


def show_workflow_results():
    """æ˜¾ç¤ºå·¥ä½œæµç¨‹ç»“æœ"""
    st.subheader("ğŸ“‹ åˆ†æç»“æœ")
    
    state_manager = get_state_manager()
    result = state_manager.get_analysis_results('workflow')
    
    if not result:
        st.info("æš‚æ— åˆ†æç»“æœ")
        return
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š ç»“æœæ¦‚è§ˆ", "ğŸ” è¯¦ç»†åˆ†æ", "ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨", "ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š"])
    
    with tab1:
        show_results_overview(result)
    
    with tab2:
        show_detailed_analysis(result)
    
    with tab3:
        show_visualizations(result)
    
    with tab4:
        show_export_options(result)


def show_results_overview(result):
    """æ˜¾ç¤ºç»“æœæ¦‚è§ˆ"""
    st.write("### ğŸ“Š æ‰§è¡Œæ¦‚è§ˆ")
    
    execution_summary = result.get('execution_summary', {})
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        workflow_id = result.get('workflow_id', 'unknown')
        st.metric(
            "å·¥ä½œæµç¨‹ID",
            workflow_id[:12] + "..." if len(workflow_id) > 12 else workflow_id,
            help=f"å®Œæ•´ID: {workflow_id}"
        )
    
    with col2:
        st.metric(
            "æ€»æ‰§è¡Œæ—¶é—´",
            f"{execution_summary.get('total_execution_time', 0):.2f}ç§’"
        )
    
    with col3:
        successful = execution_summary.get('successful_analyses', 0)
        failed = execution_summary.get('failed_analyses', 0)
        st.metric(
            "æˆåŠŸåˆ†æ",
            f"{successful}/{successful + failed}"
        )
    
    with col4:
        st.metric(
            "æ•°æ®è§„æ¨¡",
            f"{execution_summary.get('data_size', 0):,} æ¡"
        )
    
    # åˆ†æç»“æœçŠ¶æ€
    st.write("### ğŸ“ˆ åˆ†ææ¨¡å—çŠ¶æ€")
    
    analysis_results = result.get('analysis_results', {})
    
    for analysis_type, analysis_result in analysis_results.items():
        status = getattr(analysis_result, 'status', 'unknown')
        status_icon = 'âœ…' if status == 'completed' else 'âŒ'
        
        with st.expander(f"{status_icon} {analysis_type.replace('_', ' ').title()}", 
                        expanded=(status == 'completed')):
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write(f"**çŠ¶æ€**: {status}")
                st.write(f"**æ‰§è¡Œæ—¶é—´**: {getattr(analysis_result, 'execution_time', 0):.2f}ç§’")
                insights_count = len(getattr(analysis_result, 'insights', []))
                recommendations_count = len(getattr(analysis_result, 'recommendations', []))
                st.write(f"**æ´å¯Ÿæ•°é‡**: {insights_count}")
                st.write(f"**å»ºè®®æ•°é‡**: {recommendations_count}")
            
            with col2:
                insights = getattr(analysis_result, 'insights', [])
                if insights:
                    st.write("**ä¸»è¦æ´å¯Ÿ**:")
                    for insight in insights[:3]:
                        st.write(f"â€¢ {insight}")

                    if len(insights) > 3:
                        st.write(f"... è¿˜æœ‰ {len(insights) - 3} ä¸ªæ´å¯Ÿ")


def show_detailed_analysis(result):
    """æ˜¾ç¤ºè¯¦ç»†åˆ†æ"""
    st.write("### ğŸ” è¯¦ç»†åˆ†æç»“æœ")
    
    analysis_results = result.get('analysis_results', {})
    
    if not analysis_results:
        st.info("æš‚æ— è¯¦ç»†åˆ†æç»“æœ")
        return
    
    # é€‰æ‹©è¦æŸ¥çœ‹çš„åˆ†æ
    selected_analysis = st.selectbox(
        "é€‰æ‹©åˆ†ææ¨¡å—",
        options=list(analysis_results.keys()),
        format_func=lambda x: x.replace('_', ' ').title()
    )
    
    if selected_analysis:
        analysis_result = analysis_results[selected_analysis]
        status = getattr(analysis_result, 'status', 'unknown')
        
        if status == 'completed':
            # æ˜¾ç¤ºæ´å¯Ÿ
            insights = getattr(analysis_result, 'insights', [])
            if insights:
                st.write("#### ğŸ’¡ å…³é”®æ´å¯Ÿ")
                for i, insight in enumerate(insights, 1):
                    st.write(f"{i}. {insight}")

            # æ˜¾ç¤ºå»ºè®®
            recommendations = getattr(analysis_result, 'recommendations', [])
            if recommendations:
                st.write("#### ğŸ¯ è¡ŒåŠ¨å»ºè®®")
                for i, recommendation in enumerate(recommendations, 1):
                    st.write(f"{i}. {recommendation}")

            # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
            if hasattr(analysis_result, 'data') and analysis_result.data:
                st.write("#### ğŸ“Š æ•°æ®æ‘˜è¦")
                data_summary = analysis_result.data
                
                summary_data = []
                for key, value in data_summary.items():
                    if isinstance(value, dict):
                        summary_data.append({
                            'å­—æ®µ': key,
                            'ç±»å‹': value.get('type', 'unknown'),
                            'æè¿°': str(value)[:100] + '...' if len(str(value)) > 100 else str(value)
                        })
                
                if summary_data:
                    df = pd.DataFrame(summary_data)
                    st.dataframe(df, use_container_width=True)
        
        else:
            error_message = getattr(analysis_result, 'error_message', 'æœªçŸ¥é”™è¯¯')
            st.error(f"âŒ åˆ†æå¤±è´¥: {error_message}")


def show_visualizations(result):
    """æ˜¾ç¤ºå¯è§†åŒ–å›¾è¡¨"""
    st.write("### ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨")
    
    visualizations = result.get('visualizations', {})
    
    if not visualizations:
        st.info("ğŸ“Š æš‚æ— å¯è§†åŒ–å›¾è¡¨æ•°æ®")
        return
    
    # é€‰æ‹©è¦æŸ¥çœ‹çš„å¯è§†åŒ–
    viz_options = []
    for analysis_type, viz_data in visualizations.items():
        if viz_data:
            for viz_name in viz_data.keys():
                viz_options.append(f"{analysis_type} - {viz_name}")
    
    if viz_options:
        selected_viz = st.selectbox(
            "é€‰æ‹©å¯è§†åŒ–å›¾è¡¨",
            options=viz_options
        )
        
        if selected_viz:
            analysis_type, viz_name = selected_viz.split(' - ', 1)
            viz_data = visualizations[analysis_type][viz_name]
            
            if isinstance(viz_data, dict) and 'chart' in viz_data:
                st.plotly_chart(viz_data['chart'], use_container_width=True)
            else:
                st.write("å›¾è¡¨æ•°æ®æ ¼å¼ä¸æ”¯æŒæ˜¾ç¤º")
                st.json(viz_data)
    else:
        st.info("ğŸ“Š æš‚æ— å¯ç”¨çš„å¯è§†åŒ–å›¾è¡¨")


def show_export_options(result):
    """æ˜¾ç¤ºå¯¼å‡ºé€‰é¡¹"""
    st.write("### ğŸ“¥ å¯¼å‡ºåˆ†ææŠ¥å‘Š")
    
    col1, col2 = st.columns(2)
    
    with col1:
        export_format = st.selectbox(
            "å¯¼å‡ºæ ¼å¼",
            options=['json', 'pdf', 'excel'],
            format_func=lambda x: {
                'json': 'JSON æ ¼å¼',
                'pdf': 'PDF æŠ¥å‘Š',
                'excel': 'Excel è¡¨æ ¼'
            }.get(x, x)
        )
        
        include_raw_data = st.checkbox(
            "åŒ…å«åŸå§‹æ•°æ®",
            value=False,
            help="åŒ…å«åŸå§‹æ•°æ®ä¼šå¢åŠ æ–‡ä»¶å¤§å°"
        )
    
    with col2:
        st.write("**å¯¼å‡ºå†…å®¹é¢„è§ˆ**:")
        st.write("â€¢ æ‰§è¡Œæ‘˜è¦")
        st.write("â€¢ åˆ†æç»“æœå’Œæ´å¯Ÿ")
        st.write("â€¢ è¡ŒåŠ¨å»ºè®®")
        st.write("â€¢ ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡")
        if include_raw_data:
            st.write("â€¢ åŸå§‹æ•°æ®")
    
    if st.button("ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š", type="primary"):
        try:
            integration_manager = get_integration_manager()
            workflow_id = result.get('workflow_id', 'unknown')
            
            # ç”ŸæˆæŠ¥å‘Šæ•°æ®
            report_data = {
                'workflow_id': workflow_id,
                'export_time': time.time(),
                'export_format': export_format,
                'analysis_results': result.get('analysis_results', {}),
                'execution_summary': result.get('execution_summary', {}),
                'visualizations': result.get('visualizations', {}) if not include_raw_data else {}
            }
            
            # åˆ›å»ºæŠ¥å‘Šæ–‡ä»¶
            timestamp = int(time.time())
            filename = f"intelligent_analysis_report_{timestamp}.{export_format}"
            
            if export_format == 'json':
                file_content = json.dumps(report_data, ensure_ascii=False, indent=2)
                mime_type = 'application/json'
            else:
                # å…¶ä»–æ ¼å¼çš„ç®€åŒ–å®ç°
                file_content = json.dumps(report_data, ensure_ascii=False, indent=2)
                mime_type = 'application/octet-stream'
            
            # æä¾›ä¸‹è½½é“¾æ¥
            st.download_button(
                label=f"â¬‡ï¸ ä¸‹è½½ {export_format.upper()} æŠ¥å‘Š",
                data=file_content.encode('utf-8'),
                file_name=filename,
                mime=mime_type
            )
            
            st.success(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
            
        except Exception as e:
            st.error(f"âŒ å¯¼å‡ºå¤±è´¥: {str(e)}")


def get_mime_type(export_format):
    """è·å–MIMEç±»å‹"""
    mime_types = {
        'json': 'application/json',
        'pdf': 'application/pdf',
        'excel': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    }
    return mime_types.get(export_format, 'application/octet-stream')