"""
æ•°æ®ä¸Šä¼ é¡µé¢ç»„ä»¶
å¤„ç†GA4æ•°æ®æ–‡ä»¶çš„ä¸Šä¼ ã€éªŒè¯å’Œå¤„ç†
"""

import streamlit as st
import pandas as pd
import json
import time
from pathlib import Path
from typing import Optional

from config.settings import settings
from tools.ga4_data_parser import GA4DataParser
from tools.data_validator import DataValidator
from utils.i18n import t
from utils.cached_components import get_cached_ga4_parser, get_cached_data_validator


class DataUploadPage:
    """æ•°æ®ä¸Šä¼ é¡µé¢ç±»"""
    
    def __init__(self):
        self.parser = get_cached_ga4_parser()
        self.validator = get_cached_data_validator()
    
    def render(self):
        """æ¸²æŸ“æ•°æ®ä¸Šä¼ é¡µé¢"""
        from ui.state import get_state_manager
        
        st.header(t("data_upload.title"))
        
        # æ–‡ä»¶ä¸Šä¼ è¯´æ˜
        self._render_instructions()
        
        # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
        self._render_file_upload()
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœ
        state_manager = get_state_manager()
        if state_manager.is_data_loaded():
            self._render_processing_results()
    
    def _render_instructions(self):
        """æ¸²æŸ“ä½¿ç”¨è¯´æ˜"""
        with st.expander(t("data_upload.usage_instructions"), expanded=False):
            st.markdown(t("data_upload.supported_file_formats_title"))
            st.markdown(t("data_upload.ga4_ndjson_format"))
            st.markdown(t("data_upload.file_size_limit_desc").format(max_size=settings.max_file_size_mb))
            
            st.markdown(t("data_upload.data_requirements_title"))
            st.markdown(t("data_upload.data_structure_requirement"))
            st.markdown(t("data_upload.supported_event_types"))
            st.markdown(t("data_upload.required_fields"))
            
            st.markdown(t("data_upload.processing_flow_title"))
            st.markdown(t("data_upload.processing_step_1"))
            st.markdown(t("data_upload.processing_step_2"))
            st.markdown(t("data_upload.processing_step_3"))
            st.markdown(t("data_upload.processing_step_4"))
    
    def _render_file_upload(self):
        """æ¸²æŸ“æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ"""
        st.subheader(t("data_upload.file_upload"))
        
        uploaded_file = st.file_uploader(
            t("data_upload.upload_ga4_file"),
            type=['ndjson', 'json', 'jsonl'],
            help=f"{t('data_upload.supported_formats')} (æœ€å¤§{settings.max_file_size_mb}MB)"
        )
        
        if uploaded_file is not None:
            self._handle_uploaded_file(uploaded_file)
    
    def _handle_uploaded_file(self, uploaded_file):
        """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶"""
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size_mb = uploaded_file.size / (1024 * 1024)
        
        if file_size_mb > settings.max_file_size_mb:
            st.error(t("data_upload.file_size_exceeded").format(
                size=f"{file_size_mb:.1f}", 
                limit=settings.max_file_size_mb
            ))
            return
        
        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        st.info(f"{t('data_upload.file_name')}: {uploaded_file.name}")
        st.info(f"{t('data_upload.file_size')}: {file_size_mb:.2f}MB")
        
        # å¤„ç†æŒ‰é’®
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            if st.button(t("data_upload.start_processing"), type="primary"):
                self._process_file(uploaded_file)
        
        with col2:
            if st.button(t("data_upload.preview_file")):
                self._preview_file(uploaded_file)
    
    def _preview_file(self, uploaded_file):
        """é¢„è§ˆä¸Šä¼ çš„æ–‡ä»¶"""
        st.subheader(t("data_upload.file_preview"))
        
        try:
            # è¯»å–å‰å‡ è¡Œè¿›è¡Œé¢„è§ˆ
            content = uploaded_file.read().decode('utf-8')
            lines = content.split('\n')[:5]  # åªæ˜¾ç¤ºå‰5è¡Œ
            
            st.write(f"**{t('data_upload.file_lines')}**: ~{len(content.split())}")
            st.write(f"**{t('data_upload.first_lines')}**:")
            
            for i, line in enumerate(lines, 1):
                if line.strip():
                    try:
                        # å°è¯•è§£æJSONå¹¶æ ¼å¼åŒ–æ˜¾ç¤º
                        json_data = json.loads(line)
                        st.code(f"{t('data_upload.line_number').format(number=i)}: {json.dumps(json_data, indent=2, ensure_ascii=False)[:500]}...")
                    except json.JSONDecodeError:
                        st.code(f"{t('data_upload.line_number').format(number=i)}: {line[:500]}...")
            
            # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            uploaded_file.seek(0)
            
        except Exception as e:
            st.error(f"{t('data_upload.file_preview_failed')}: {str(e)}")
    
    def _process_file(self, uploaded_file):
        """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶"""
        progress_container = st.container()
        
        with progress_container:
            st.subheader(t("data_upload.processing_progress"))
            
            # åˆ›å»ºè¿›åº¦æ¡
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                # Step 1: Save file
                status_text.text(t("data_upload.step_saving_file"))
                progress_bar.progress(10)
                
                file_path = self._save_uploaded_file(uploaded_file)
                
                # Step 2: Parse data
                status_text.text(t("data_upload.step_parsing_data"))
                progress_bar.progress(30)
                
                raw_data = self.parser.parse_ndjson(str(file_path))
                
                # Step 3: Data validation
                status_text.text(t("data_upload.step_validating_data"))
                progress_bar.progress(50)
                
                validation_report = self.validator.validate_dataframe(raw_data)
                
                # Step 4: Data processing
                status_text.text(t("data_upload.step_processing_data"))
                progress_bar.progress(70)
                
                processed_data = self._process_data(raw_data)
                
                # Step 5: Store data
                status_text.text(t("data_upload.step_storing_data"))
                progress_bar.progress(90)
                
                self._store_data(raw_data, processed_data, validation_report)
                
                # Complete
                status_text.text(t("data_upload.step_completed"))
                progress_bar.progress(100)
                
                st.success(t("data_upload.processing_success"))
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if file_path.exists():
                    file_path.unlink()
                
                time.sleep(1)
                st.rerun()
                
            except Exception as e:
                st.error(f"{t('data_upload.processing_failed')}: {str(e)}")
                progress_bar.progress(0)
                status_text.text(t("data_upload.step_failed"))
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if 'file_path' in locals() and file_path.exists():
                    file_path.unlink()
    
    def _save_uploaded_file(self, uploaded_file) -> Path:
        """ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶"""
        # ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
        upload_dir = Path("data/uploads")
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        file_path = upload_dir / uploaded_file.name
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        return file_path
    
    def _process_data(self, raw_data):
        """å¤„ç†æ•°æ®"""
        # æå–äº‹ä»¶æ•°æ®
        events_data = self.parser.extract_events(raw_data)
        user_data = self.parser.extract_user_properties(raw_data)
        session_data = self.parser.extract_sessions(raw_data)
        
        # å¤„ç†äº‹ä»¶æ•°æ® - å¦‚æœæ˜¯å­—å…¸ï¼Œåˆå¹¶æ‰€æœ‰äº‹ä»¶ç±»å‹
        if isinstance(events_data, dict):
            # åˆå¹¶æ‰€æœ‰äº‹ä»¶ç±»å‹çš„æ•°æ®
            all_events_list = []
            for event_type, event_df in events_data.items():
                if not event_df.empty:
                    all_events_list.append(event_df)
            
            if all_events_list:
                combined_events = pd.concat(all_events_list, ignore_index=True)
                st.success(t("data_upload.merged_events").format(
                    types=len(events_data), 
                    events=len(combined_events)
                ))
            else:
                combined_events = pd.DataFrame()
                st.warning(t("data_upload.no_valid_events"))
        else:
            combined_events = events_data
        
        return {
            'events': combined_events,
            'users': user_data,
            'sessions': session_data
        }
    
    def _store_data(self, raw_data, processed_data, validation_report):
        """å­˜å‚¨æ•°æ®åˆ°ä¼šè¯çŠ¶æ€å’Œå­˜å‚¨ç®¡ç†å™¨"""
        from ui.state import get_state_manager
        
        # è·å–çŠ¶æ€ç®¡ç†å™¨
        state_manager = get_state_manager()
        
        # é€šè¿‡çŠ¶æ€ç®¡ç†å™¨æ­£ç¡®è®¾ç½®æ•°æ®çŠ¶æ€
        state_manager.set_data_loaded(True, raw_data)
        state_manager.set_processed_data(processed_data)
        
        # å­˜å‚¨åˆ°æ•°æ®ç®¡ç†å™¨
        if hasattr(st.session_state, 'storage_manager'):
            storage_manager = st.session_state.storage_manager
            storage_manager.store_events(processed_data['events'])
            storage_manager.store_users(processed_data['users'])
            storage_manager.store_sessions(processed_data['sessions'])
            
            # åˆ·æ–°é›†æˆç®¡ç†å™¨çš„å­˜å‚¨ç®¡ç†å™¨
            if 'integration_manager' in st.session_state:
                st.session_state.integration_manager.refresh_storage_manager(storage_manager)
        
        # ç”Ÿæˆæ•°æ®æ‘˜è¦å¹¶é€šè¿‡çŠ¶æ€ç®¡ç†å™¨å­˜å‚¨
        data_summary = self.parser.validate_data_quality(raw_data)
        state_manager.update_data_summary(data_summary)
        st.session_state.validation_report = validation_report
    
    def _render_processing_results(self):
        """æ˜¾ç¤ºå¤„ç†ç»“æœ"""
        from ui.state import get_state_manager
        state_manager = get_state_manager()
        
        st.markdown("---")
        st.subheader(t("data_upload.processing_results"))
        
        data_summary = state_manager.get_data_summary()
        if data_summary:
            self._render_data_summary()
        
        # éªŒè¯æŠ¥å‘Š
        if st.session_state.validation_report:
            self._render_validation_report()
    
    def _render_data_summary(self):
        """æ¸²æŸ“æ•°æ®æ‘˜è¦"""
        from ui.state import get_state_manager
        state_manager = get_state_manager()
        summary = state_manager.get_data_summary()
        
        # åŸºç¡€ç»Ÿè®¡
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                t("metrics.total_events"),
                f"{summary.get('total_events', 0):,}",
                help=t("metrics.total_events_help")
            )
        
        with col2:
            st.metric(
                t("metrics.unique_users"),
                f"{summary.get('unique_users', 0):,}",
                help=t("metrics.unique_users_help")
            )
        
        with col3:
            date_range = summary.get('date_range', {})
            days = (pd.to_datetime(date_range.get('end', '')) - pd.to_datetime(date_range.get('start', ''))).days
            st.metric(
                t("metrics.data_days"),
                f"{days}{t('common.day')}",
                help=t("metrics.time_range_help")
            )
        
        with col4:
            st.metric(
                t("metrics.event_types_data"),
                len(summary.get('event_types', {})),
                help=t("metrics.event_types_help")
            )
        
        # è¯¦ç»†ä¿¡æ¯
        col1, col2 = st.columns(2)
        
        with col1:
            st.write(f"**ğŸ“… {t('metrics.time_range_data')}**")
            date_range = summary.get('date_range', {})
            st.write(f"- {t('metrics.start_time')}: {date_range.get('start', 'N/A')}")
            st.write(f"- {t('metrics.end_time')}: {date_range.get('end', 'N/A')}")
            
            st.write(f"**ğŸ¯ {t('metrics.event_types_data')}**")
            event_types = summary.get('event_types', {})
            for event_type, count in list(event_types.items())[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                st.write(f"- {event_type}: {count:,}")
            if len(event_types) > 10:
                st.write(f"- ... è¿˜æœ‰ {len(event_types) - 10} ç§äº‹ä»¶ç±»å‹")
        
        with col2:
            st.write(f"**ğŸ“± {t('metrics.platform_distribution')}**")
            platforms = summary.get('platforms', {})
            for platform, count in platforms.items():
                st.write(f"- {platform}: {count:,}")
            
            # æ•°æ®è´¨é‡é—®é¢˜
            if summary.get('data_issues'):
                st.write(f"**{t('metrics.data_quality_issues')}**")
                for issue in summary.get('data_issues', []):
                    st.warning(f"- {issue}")
    
    def _render_validation_report(self):
        """æ¸²æŸ“éªŒè¯æŠ¥å‘Š"""
        with st.expander("ğŸ” è¯¦ç»†éªŒè¯æŠ¥å‘Š", expanded=False):
            validation_report = st.session_state.validation_report
            
            if validation_report.get('validation_passed'):
                st.success(t("metrics.data_validation_passed"))
            else:
                st.error(t("metrics.data_validation_failed"))
            
            # é”™è¯¯ä¿¡æ¯
            if validation_report.get('errors'):
                st.write(f"**{t('metrics.error_messages')}**")
                for error in validation_report['errors']:
                    st.error(f"- {error}")
            
            # è­¦å‘Šä¿¡æ¯
            if validation_report.get('warnings'):
                st.write(f"**{t('metrics.warning_messages')}**")
                for warning in validation_report['warnings']:
                    st.warning(f"- {warning}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = validation_report.get('statistics', {})
            if stats:
                st.write(f"**{t('metrics.statistics')}**")
                st.json(stats)


def show_data_upload_page():
    """æ•°æ®ä¸Šä¼ é¡µé¢å…¥å£å‡½æ•° - ä¿æŒå‘åå…¼å®¹"""
    page = DataUploadPage()
    page.render()