#!/usr/bin/env python3
"""
ç®€åŒ–çš„ç›‘æ§å’Œæ—¥å¿—å¢å¼ºåŠŸèƒ½æµ‹è¯•
ä¸“æ³¨äºæµ‹è¯•ç›‘æ§ç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½
"""

import os
import sys
import time
import json
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.abspath('.'))

from config.monitoring_system import (
    get_performance_monitor, 
    reset_performance_monitor,
    RequestType, 
    ResponseStatus,
    RequestMetrics
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_provider_specific_metrics():
    """æµ‹è¯•æä¾›å•†ç‰¹å®šæŒ‡æ ‡æ”¶é›†"""
    print("\n=== æµ‹è¯•æä¾›å•†ç‰¹å®šæŒ‡æ ‡æ”¶é›† ===")
    
    # é‡ç½®ç›‘æ§å™¨
    reset_performance_monitor()
    monitor = get_performance_monitor()
    
    # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„è¯·æ±‚
    test_scenarios = [
        {
            "provider": "volcano",
            "request_type": RequestType.TEXT_ONLY,
            "model": "doubao-seed-1-6-250615",
            "response_time": 1.2,
            "tokens_used": 150,
            "status": ResponseStatus.SUCCESS
        },
        {
            "provider": "volcano",
            "request_type": RequestType.MULTIMODAL,
            "model": "doubao-seed-1-6-250615",
            "response_time": 2.5,
            "tokens_used": 200,
            "image_count": 2,
            "status": ResponseStatus.SUCCESS
        },
        {
            "provider": "google",
            "request_type": RequestType.TEXT_ONLY,
            "model": "gemini-2.5-pro",
            "response_time": 0.8,
            "tokens_used": 120,
            "status": ResponseStatus.SUCCESS
        },
        {
            "provider": "volcano",
            "request_type": RequestType.TEXT_ONLY,
            "model": "doubao-seed-1-6-250615",
            "status": ResponseStatus.RATE_LIMITED,
            "error_message": "Rate limit exceeded"
        }
    ]
    
    # è®°å½•æµ‹è¯•è¯·æ±‚
    for i, scenario in enumerate(test_scenarios):
        request_id = f"test_request_{i+1}"
        
        # å¼€å§‹è¯·æ±‚è®°å½•
        metrics = monitor.record_request_start(
            request_id=request_id,
            provider=scenario["provider"],
            prompt=f"Test prompt {i+1}",
            request_type=scenario["request_type"],
            image_count=scenario.get("image_count", 0),
            model=scenario["model"]
        )
        
        # ç»“æŸè¯·æ±‚è®°å½•
        monitor.record_request_end(
            metrics=metrics,
            response=f"Test response {i+1}" if scenario["status"] == ResponseStatus.SUCCESS else None,
            status=scenario["status"],
            error_message=scenario.get("error_message"),
            tokens_used=scenario.get("tokens_used"),
            retry_count=scenario.get("retry_count", 0)
        )
        
        print(f"âœ“ è®°å½•äº†è¯·æ±‚ {request_id}: {scenario['provider']} - {scenario['request_type'].value}")
    
    # éªŒè¯æä¾›å•†ç‰¹å®šæŒ‡æ ‡
    volcano_metrics = monitor.get_provider_specific_metrics("volcano")
    google_metrics = monitor.get_provider_specific_metrics("google")
    
    print(f"\n--- Volcano æä¾›å•†ç‰¹å®šæŒ‡æ ‡ ---")
    if volcano_metrics:
        print(f"æ¨¡å‹æ€§èƒ½: {json.dumps(volcano_metrics.get('model_performance', {}), indent=2, ensure_ascii=False)}")
        print(f"å¤šæ¨¡æ€ç»Ÿè®¡: {json.dumps(volcano_metrics.get('multimodal_stats', {}), indent=2, ensure_ascii=False)}")
        print(f"é”™è¯¯æ¨¡å¼: {json.dumps(volcano_metrics.get('error_patterns', {}), indent=2, ensure_ascii=False)}")
        print(f"å»¶è¿Ÿåˆ†å¸ƒè®°å½•æ•°: {len(volcano_metrics.get('latency_distribution', []))}")
    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ° Volcano æä¾›å•†ç‰¹å®šæŒ‡æ ‡")
    
    print(f"\n--- Google æä¾›å•†ç‰¹å®šæŒ‡æ ‡ ---")
    if google_metrics:
        print(f"æ¨¡å‹æ€§èƒ½: {json.dumps(google_metrics.get('model_performance', {}), indent=2, ensure_ascii=False)}")
        print(f"å»¶è¿Ÿåˆ†å¸ƒè®°å½•æ•°: {len(google_metrics.get('latency_distribution', []))}")
    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ° Google æä¾›å•†ç‰¹å®šæŒ‡æ ‡")
    
    return volcano_metrics is not None and google_metrics is not None


def test_detailed_logging():
    """æµ‹è¯•è¯¦ç»†çš„è¯·æ±‚/å“åº”æ—¥å¿—è®°å½•"""
    print("\n=== æµ‹è¯•è¯¦ç»†çš„è¯·æ±‚/å“åº”æ—¥å¿—è®°å½• ===")
    
    # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦åˆ›å»º
    log_dir = Path("logs/monitoring")
    expected_log_files = [
        "requests.log",
        "performance.log",
        "errors.log",
        "provider_comparison.log",
        "provider_metrics.log"
    ]
    
    created_files = []
    for log_file in expected_log_files:
        log_path = log_dir / log_file
        if log_path.exists():
            created_files.append(log_file)
            print(f"âœ“ æ—¥å¿—æ–‡ä»¶å·²åˆ›å»º: {log_path}")
            
            # æ£€æŸ¥æ–‡ä»¶å†…å®¹
            try:
                with open(log_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if content.strip():
                        print(f"  - æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
                        # æ˜¾ç¤ºæœ€åå‡ è¡Œ
                        lines = content.strip().split('\n')
                        if lines:
                            print(f"  - æœ€æ–°æ—¥å¿—: {lines[-1][:100]}...")
                    else:
                        print(f"  - æ–‡ä»¶ä¸ºç©º")
            except Exception as e:
                print(f"  - è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        else:
            print(f"âŒ æ—¥å¿—æ–‡ä»¶æœªåˆ›å»º: {log_path}")
    
    print(f"\nåˆ›å»ºçš„æ—¥å¿—æ–‡ä»¶: {len(created_files)}/{len(expected_log_files)}")
    return len(created_files) >= 3  # è‡³å°‘è¦æœ‰3ä¸ªä¸»è¦æ—¥å¿—æ–‡ä»¶


def test_performance_comparison():
    """æµ‹è¯•æ€§èƒ½ç›‘æ§å’Œæä¾›å•†æ¯”è¾ƒ"""
    print("\n=== æµ‹è¯•æ€§èƒ½ç›‘æ§å’Œæä¾›å•†æ¯”è¾ƒ ===")
    
    monitor = get_performance_monitor()
    
    # ç”Ÿæˆæä¾›å•†æ¯”è¾ƒæŠ¥å‘Š
    comparison_report = monitor.get_provider_comparison_report(time_window_hours=1)
    
    print(f"æ¯”è¾ƒæŠ¥å‘ŠçŠ¶æ€: {comparison_report.get('status', 'success')}")
    
    if comparison_report.get('status') != 'no_data':
        print(f"æ€»è¯·æ±‚æ•°: {comparison_report.get('total_requests', 0)}")
        print(f"æä¾›å•†æ•°é‡: {len(comparison_report.get('providers', {}))}")
        
        # æ˜¾ç¤ºæ¯ä¸ªæä¾›å•†çš„æ€§èƒ½æ•°æ®
        for provider, data in comparison_report.get('providers', {}).items():
            print(f"\n--- {provider} æä¾›å•†æ€§èƒ½ ---")
            print(f"  æ€»è¯·æ±‚: {data.get('total_requests', 0)}")
            print(f"  æˆåŠŸç‡: {data.get('success_rate', 0):.1%}")
            print(f"  å¹³å‡å“åº”æ—¶é—´: {data['performance'].get('avg_response_time', 0):.2f}ç§’")
            print(f"  å¤šæ¨¡æ€è¯·æ±‚: {data['capabilities'].get('multimodal_requests', 0)}")
            print(f"  å›é€€ä½¿ç”¨: {data['reliability'].get('fallback_usage', 0)}")
        
        # æ˜¾ç¤ºæ•´ä½“æ¯”è¾ƒ
        overall = comparison_report.get('overall_comparison', {})
        if overall:
            print(f"\n--- æ•´ä½“æ¯”è¾ƒ ---")
            print(f"æ€§èƒ½æ’å: {overall.get('performance_ranking', [])}")
            print(f"å¯é æ€§æ’å: {overall.get('reliability_ranking', [])}")
            print(f"æˆæœ¬æ’å: {overall.get('cost_ranking', [])}")
            print(f"æœ€ä½³æ•´ä½“: {overall.get('best_overall', 'N/A')}")
        
        # æ˜¾ç¤ºå»ºè®®
        recommendations = comparison_report.get('recommendations', [])
        if recommendations:
            print(f"\n--- å»ºè®® ---")
            for rec in recommendations:
                print(f"  â€¢ {rec}")
        
        return True
    else:
        print(f"âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š: {comparison_report.get('message', '')}")
        return False


def test_detailed_performance_metrics():
    """æµ‹è¯•è¯¦ç»†æ€§èƒ½æŒ‡æ ‡"""
    print("\n=== æµ‹è¯•è¯¦ç»†æ€§èƒ½æŒ‡æ ‡ ===")
    
    monitor = get_performance_monitor()
    
    # æµ‹è¯•æ¯ä¸ªæä¾›å•†çš„è¯¦ç»†æŒ‡æ ‡
    providers = ["volcano", "google"]
    
    success_count = 0
    for provider in providers:
        print(f"\n--- {provider} è¯¦ç»†æ€§èƒ½æŒ‡æ ‡ ---")
        
        detailed_metrics = monitor.get_detailed_performance_metrics(provider, time_window_hours=1)
        
        if detailed_metrics.get('status') != 'no_data':
            print(f"âœ“ è·å–åˆ° {provider} çš„è¯¦ç»†æŒ‡æ ‡")
            success_count += 1
            
            # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
            latency_analysis = detailed_metrics.get('latency_analysis', {})
            if latency_analysis.get('status') != 'no_data':
                print(f"  å»¶è¿Ÿåˆ†æ:")
                print(f"    æ ·æœ¬æ•°: {latency_analysis.get('total_samples', 0)}")
                print(f"    å¹³å‡å€¼: {latency_analysis.get('mean', 0):.2f}ç§’")
                print(f"    ä¸­ä½æ•°: {latency_analysis.get('median', 0):.2f}ç§’")
                print(f"    P95: {latency_analysis.get('percentiles', {}).get('p95', 0):.2f}ç§’")
            
            throughput_analysis = detailed_metrics.get('throughput_analysis', {})
            if throughput_analysis.get('status') != 'no_data':
                print(f"  ååé‡åˆ†æ:")
                print(f"    æ€»è¯·æ±‚: {throughput_analysis.get('total_requests', 0)}")
                print(f"    æˆåŠŸç‡: {throughput_analysis.get('success_rate', 0):.1%}")
                print(f"    å¹³å‡æ¯åˆ†é’Ÿè¯·æ±‚: {throughput_analysis.get('avg_requests_per_minute', 0):.1f}")
            
            multimodal_analysis = detailed_metrics.get('multimodal_analysis', {})
            if multimodal_analysis.get('status') != 'no_data':
                print(f"  å¤šæ¨¡æ€åˆ†æ:")
                print(f"    å¤šæ¨¡æ€è¯·æ±‚: {multimodal_analysis.get('total_multimodal_requests', 0)}")
                print(f"    å¤šæ¨¡æ€æˆåŠŸç‡: {multimodal_analysis.get('multimodal_success_rate', 0):.1%}")
                print(f"    å¤„ç†å›¾ç‰‡æ€»æ•°: {multimodal_analysis.get('total_images_processed', 0)}")
        else:
            print(f"âŒ æ²¡æœ‰ {provider} çš„è¯¦ç»†æŒ‡æ ‡æ•°æ®")
    
    return success_count > 0


def test_export_functionality():
    """æµ‹è¯•ç›‘æ§æ•°æ®å¯¼å‡ºåŠŸèƒ½"""
    print("\n=== æµ‹è¯•ç›‘æ§æ•°æ®å¯¼å‡ºåŠŸèƒ½ ===")
    
    monitor = get_performance_monitor()
    
    try:
        # å¯¼å‡ºç›‘æ§æŒ‡æ ‡
        exported_data = monitor.export_metrics()
        
        print(f"âœ“ æˆåŠŸå¯¼å‡ºç›‘æ§æ•°æ®")
        print(f"å¯¼å‡ºæ•°æ®å¤§å°: {len(exported_data)} å­—ç¬¦")
        
        # éªŒè¯JSONæ ¼å¼
        parsed_data = json.loads(exported_data)
        print(f"âœ“ å¯¼å‡ºæ•°æ®ä¸ºæœ‰æ•ˆJSONæ ¼å¼")
        print(f"åŒ…å«çš„é”®: {list(parsed_data.keys())}")
        
        # æ£€æŸ¥å…³é”®æ•°æ®
        if 'provider_stats' in parsed_data:
            provider_count = len(parsed_data['provider_stats'])
            print(f"âœ“ åŒ…å« {provider_count} ä¸ªæä¾›å•†çš„ç»Ÿè®¡æ•°æ®")
        
        if 'performance_comparison' in parsed_data:
            comparison_data = parsed_data['performance_comparison']
            if comparison_data.get('status') != 'no_data':
                print(f"âœ“ åŒ…å«æ€§èƒ½æ¯”è¾ƒæ•°æ®")
            else:
                print(f"âš  æ€§èƒ½æ¯”è¾ƒæ•°æ®ä¸ºç©º")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç›‘æ§æ•°æ®å¯¼å‡ºæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_all_provider_metrics():
    """æµ‹è¯•è·å–æ‰€æœ‰æä¾›å•†æŒ‡æ ‡"""
    print("\n=== æµ‹è¯•è·å–æ‰€æœ‰æä¾›å•†æŒ‡æ ‡ ===")
    
    monitor = get_performance_monitor()
    
    # è·å–æ‰€æœ‰æä¾›å•†çš„ç‰¹å®šæŒ‡æ ‡
    all_metrics = monitor.get_all_provider_specific_metrics()
    
    print(f"ç›‘æ§çš„æä¾›å•†æ•°é‡: {len(all_metrics)}")
    
    for provider, metrics in all_metrics.items():
        print(f"\n--- {provider} æä¾›å•†æŒ‡æ ‡æ¦‚è§ˆ ---")
        print(f"  æ¨¡å‹æ•°é‡: {len(metrics.get('model_performance', {}))}")
        print(f"  å»¶è¿Ÿè®°å½•æ•°: {len(metrics.get('latency_distribution', []))}")
        print(f"  é”™è¯¯æ¨¡å¼æ•°: {len(metrics.get('error_patterns', {}))}")
        print(f"  æˆæœ¬åˆ†æé¡¹: {len(metrics.get('cost_analysis', {}))}")
        
        # æ˜¾ç¤ºå¤šæ¨¡æ€ç»Ÿè®¡
        multimodal_stats = metrics.get('multimodal_stats', {})
        if multimodal_stats:
            print(f"  å¤šæ¨¡æ€è¯·æ±‚: {multimodal_stats.get('total_requests', 0)}")
            print(f"  å¤„ç†å›¾ç‰‡æ•°: {multimodal_stats.get('total_images', 0)}")
    
    return len(all_metrics) > 0


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•ç›‘æ§å’Œæ—¥å¿—å¢å¼ºåŠŸèƒ½...")
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    log_dir = Path("logs/monitoring")
    log_dir.mkdir(parents=True, exist_ok=True)
    
    test_results = []
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    test_functions = [
        ("æä¾›å•†ç‰¹å®šæŒ‡æ ‡æ”¶é›†", test_provider_specific_metrics),
        ("è¯¦ç»†æ—¥å¿—è®°å½•", test_detailed_logging),
        ("æ€§èƒ½æ¯”è¾ƒ", test_performance_comparison),
        ("è¯¦ç»†æ€§èƒ½æŒ‡æ ‡", test_detailed_performance_metrics),
        ("æ•°æ®å¯¼å‡ºåŠŸèƒ½", test_export_functionality),
        ("æ‰€æœ‰æä¾›å•†æŒ‡æ ‡", test_all_provider_metrics)
    ]
    
    for test_name, test_func in test_functions:
        print(f"\n{'='*60}")
        print(f"è¿è¡Œæµ‹è¯•: {test_name}")
        print(f"{'='*60}")
        
        try:
            result = test_func()
            test_results.append((test_name, result))
            
            if result:
                print(f"âœ… {test_name} - é€šè¿‡")
            else:
                print(f"âŒ {test_name} - å¤±è´¥")
                
        except Exception as e:
            print(f"âŒ {test_name} - å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            test_results.append((test_name, False))
    
    # æ±‡æ€»æµ‹è¯•ç»“æœ
    print(f"\n{'='*60}")
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print(f"{'='*60}")
    
    passed_tests = sum(1 for _, result in test_results if result)
    total_tests = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
    
    print(f"\næ€»ä½“ç»“æœ: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
    
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰ç›‘æ§å’Œæ—¥å¿—å¢å¼ºåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)